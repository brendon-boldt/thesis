@inproceedings{Papadimitriou2020LearningMH,
  title={Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models},
  author={Isabel Papadimitriou and Dan Jurafsky},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:221891676}
}

@inproceedings{
yao2022linking,
title={Linking Emergent and Natural Languages via Corpus Transfer},
author={Shunyu Yao and Mo Yu and Yang Zhang and Karthik R Narasimhan and Joshua B. Tenenbaum and Chuang Gan},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=49A1Y6tRhaq}
}

@inproceedings{ortiz-suarez-etal-2020-monolingual,
    title = "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages",
    author = "Ortiz Su{\'a}rez, Pedro Javier  and
      Romary, Laurent  and
      Sagot, Beno{\^\i}t",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.156",
    doi = "10.18653/v1/2020.acl-main.156",
    pages = "1703--1714",
    abstract = "We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.",
}

@inproceedings{kharitonov-etal-2019-egg,
    title = "{EGG}: a toolkit for research on Emergence of lan{G}uage in Games",
    author = "Kharitonov, Eugene  and
      Chaabouni, Rahma  and
      Bouchacourt, Diane  and
      Baroni, Marco",
    editor = "Pad{\'o}, Sebastian  and
      Huang, Ruihong",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-3010",
    doi = "10.18653/v1/D19-3010",
    pages = "55--60",
    abstract = "There is renewed interest in simulating language emergence among deep neural agents that communicate to jointly solve a task, spurred by the practical aim to develop language-enabled interactive AIs, as well as by theoretical questions about the evolution of human language. However, optimizing deep architectures connected by a discrete communication channel (such as that in which language emerges) is technically challenging. We introduce EGG, a toolkit that greatly simplifies the implementation of emergent-language communication games. EGG{'}s modular design provides a set of building blocks that the user can combine to create new games, easily navigating the optimization and architecture space. We hope that the tool will lower the technical barrier, and encourage researchers from various backgrounds to do original work in this exciting area.",
}

@article{mandelbrot1953informational,
  title={An informational theory of the statistical structure of language},
  author={Mandelbrot, Benoit and others},
  journal={Communication theory},
  volume={84},
  pages={486--502},
  year={1953},
  publisher={London}
}

ARTICLE{Piantadosi2014-ru,
  title     = "Zipf's word frequency law in natural language: a critical review
               and future directions",
  author    = "Piantadosi, Steven T",
  abstract  = "The frequency distribution of words has been a key object of
               study in statistical linguistics for the past 70 years. This
               distribution approximately follows a simple mathematical form
               known as Zipf's law. This article first shows that human
               language has a highly complex, reliable structure in the
               frequency distribution over and above this classic law, although
               prior data visualization methods have obscured this fact. A
               number of empirical phenomena related to word frequencies are
               then reviewed. These facts are chosen to be informative about
               the mechanisms giving rise to Zipf's law and are then used to
               evaluate many of the theoretical explanations of Zipf's law in
               language. No prior account straightforwardly explains all the
               basic facts or is supported with independent evaluation of its
               underlying assumptions. To make progress at understanding why
               language obeys Zipf's law, studies must seek evidence beyond the
               law itself, testing assumptions and evaluating novel predictions
               with new, independent data.",
  journal   = "Psychon. Bull. Rev.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  21,
  number    =  5,
  pages     = "1112--1130",
  month     =  oct,
  year      =  2014,
  language  = "en"
}

@inproceedings{
evtimova2018emergent,
title={Emergent Communication in a Multi-Modal, Multi-Step Referential Game},
author={Katrina Evtimova and Andrew Drozdov and Douwe Kiela and Kyunghyun Cho},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJGZq6g0-},
}

@inproceedings{mu2021generalizations,
 author = {Mu, Jesse and Goodman, Noah},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {17994--18007},
 publisher = {Curran Associates, Inc.},
 title = {Emergent Communication of Generalizations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/9597353e41e6957b5e7aa79214fcb256-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{zoph-etal-2016-transfer,
    title = "Transfer Learning for Low-Resource Neural Machine Translation",
    author = "Zoph, Barret  and
      Yuret, Deniz  and
      May, Jonathan  and
      Knight, Kevin",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1163",
    doi = "10.18653/v1/D16-1163",
    pages = "1568--1575",
}

article{Lazaridou2020EmergentMC,
  title={Emergent Multi-Agent Communication in the Deep Learning Era},
  author={Angeliki Lazaridou and Marco Baroni},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.02419},
  url={https://api.semanticscholar.org/CorpusID:219260403}
}

article{Lazaridou2018EmergenceOL,
  title={Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input},
  author={Angeliki Lazaridou and Karl Moritz Hermann and Karl Tuyls and Stephen Clark},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.03984},
  url={https://api.semanticscholar.org/CorpusID:4737664}
}

@article{Korbak2020MeasuringNC,
  title={Measuring non-trivial compositionality in emergent communication},
  author={Tomasz Korbak and Julian Zubek and Joanna Rkaczaszek-Leonardi},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.15058},
  url={https://api.semanticscholar.org/CorpusID:225094094}
}

@misc{
perkins2022icy,
title={Icy: A benchmark for measuring compositional inductive bias of emergent communication models},
author={Hugh Perkins},
year={2022},
url={https://openreview.net/forum?id=S352vriz3G}
}

@inproceedings{
  guo2023emergent,
  title={Emergent Communication for Rules Reasoning},
  author={Yuxuan Guo and Yifan Hao and Rui Zhang and Enshuai Zhou and Zidong Du and Xishan Zhang and Xinkai Song and Yuanbo Wen and Yongwei Zhao and Xuehai Zhou and Jiaming Guo and Qi Yi and Shaohui Peng and Di Huang and Ruizhi Chen and Qi Guo and Yunji Chen},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=gx20B4ItIw}
}

@inproceedings{
guo2022expressivity,
title={Expressivity of Emergent Languages is a Trade-off between Contextual Complexity and Unpredictability},
author={Shangmin Guo and Yi Ren and Kory Wallace Mathewson and Simon Kirby and Stefano V Albrecht and Kenny Smith},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=WxuE_JWxjkW}
}

@article{Bullard2020ExploringZE,
  title={Exploring Zero-Shot Emergent Communication in Embodied Multi-Agent Populations},
  author={Kalesha Bullard and Franziska Meier and Douwe Kiela and Joelle Pineau and Jakob N. Foerster},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.15896},
  url={https://api.semanticscholar.org/CorpusID:226222037}
}

@inproceedings{li2019ease,
author = {Li, Fushan and Bowling, Michael},
title = {Ease-of-Teaching and Language Structure from Emergent Communication},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Artificial agents have been shown to learn to communicate when needed to complete a cooperative task. Some level of language structure (e.g., compositionality) has been found in the learned communication protocols. This observed structure is often the result of specific environmental pressures during training. By introducing new agents periodically to replace old ones, sequentially and within a population, we explore such a new pressure — ease of teaching — and show its impact on the structure of the resulting language.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1420},
numpages = {11}
}

@article{Russakovsky2014ImageNetLS,
  title={ImageNet Large Scale Visual Recognition Challenge},
  author={Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael S. Bernstein and Alexander C. Berg and Li Fei-Fei},
  journal={International Journal of Computer Vision},
  year={2014},
  volume={115},
  pages={211 - 252},
  url={https://api.semanticscholar.org/CorpusID:2930547}
}

@inproceedings{Wang2018GLUEAM,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  booktitle={BlackboxNLP@EMNLP},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:5034059}
}

@article{Marcus1993BuildingAL,
  title={Building a Large Annotated Corpus of English: The Penn Treebank},
  author={Mitchell P. Marcus and Beatrice Santorini and Mary Ann Marcinkiewicz},
  journal={Comput. Linguistics},
  year={1993},
  volume={19},
  pages={313-330},
  url={https://api.semanticscholar.org/CorpusID:252796}
}

@InProceedings{bojar2014wmt,
  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale
{s}},
  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {12--58},
  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}
}

@inproceedings{downey-etal-2023-learning,
    title = "Learning to translate by learning to communicate",
    author = "Downey, C.m.  and
      Zhou, Xuhui  and
      Liu, Zeyu  and
      Steinert-Threlkeld, Shane",
    editor = "Ataman, Duygu",
    booktitle = "Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.mrl-1.17",
    pages = "218--238",
}

@article{Galke2022EmergentCF,
  title={Emergent Communication for Understanding Human Language Evolution: What's Missing?},
  author={Lukas Galke and Yoav Ram and Limor Raviv},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.10590}
}

@article{MoulinFrier2020MultiAgentRL,
  title={Multi-Agent Reinforcement Learning as a Computational Tool for Language Evolution Research: Historical Context and Future Challenges},
  author={Cl{\'e}ment Moulin-Frier and Pierre-Yves Oudeyer},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.08878}
}

@article{lacroix2019biology,
  author       = {Travis LaCroix},
  title        = {Biology and Compositionality: Empirical Considerations for Emergent-Communication
                  Protocols},
  journal      = {CoRR},
  volume       = {abs/1911.11668},
  year         = {2019},
  url          = {http://arxiv.org/abs/1911.11668},
  eprinttype    = {arXiv},
  eprint       = {1911.11668},
  timestamp    = {Tue, 03 Dec 2019 20:41:07 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1911-11668.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li-etal-2020-emergent,
    title = "Emergent Communication Pretraining for Few-Shot Machine Translation",
    author = "Li, Yaoyiran  and
      Ponti, Edoardo Maria  and
      Vuli{\'c}, Ivan  and
      Korhonen, Anna",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.416",
    doi = "10.18653/v1/2020.coling-main.416",
    pages = "4716--4731",
    abstract = "While state-of-the-art models that rely upon massively multilingual pretrained encoders achieve sample efficiency in downstream applications, they still require abundant amounts of unlabelled text. Nevertheless, most of the world{'}s languages lack such resources. Hence, we investigate a more radical form of unsupervised knowledge transfer in the absence of linguistic data. In particular, for the first time we pretrain neural networks via emergent communication from referential games. Our key assumption is that grounding communication on images{---}as a crude approximation of real-world environments{---}inductively biases the model towards learning natural languages. On the one hand, we show that this substantially benefits machine translation in few-shot settings. On the other hand, this also provides an extrinsic evaluation protocol to probe the properties of emergent languages ex vitro. Intuitively, the closer they are to natural languages, the higher the gains from pretraining on them should be. For instance, in this work we measure the influence of communication success and maximum sequence length on downstream performances. Finally, we introduce a customised adapter layer and annealing strategies for the regulariser of maximum-a-posteriori inference during fine-tuning. These turn out to be crucial to facilitate knowledge transfer and prevent catastrophic forgetting. Compared to a recurrent baseline, our method yields gains of 59.0{\%} 147.6{\%} in BLEU score with only 500 NMT training instances and 65.1{\%} 196.7{\%} with 1,000 NMT training instances across four language pairs. These proof-of-concept results reveal the potential of emergent communication pretraining for both natural language processing tasks in resource-poor settings and extrinsic evaluation of artificial languages.",
}

@INPROCEEDINGS{mu2023ec2,
  author={Mu, Yao and Yao, Shunyu and Ding, Mingyu and Luo, Ping and Gan, Chuang},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={EC2: Emergent Communication for Embodied Control}, 
  year={2023},
  volume={},
  number={},
  pages={6704-6714},
  doi={10.1109/CVPR52729.2023.00648}}

@inproceedings{heusel2017fid,
 author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@inproceedings{artetxe-etal-2020-cross,
    title = "On the Cross-lingual Transferability of Monolingual Representations",
    author = "Artetxe, Mikel  and
      Ruder, Sebastian  and
      Yogatama, Dani",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.421",
    doi = "10.18653/v1/2020.acl-main.421",
    pages = "4623--4637",
    abstract = "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
}

inproceedings{Mihai2021LearningTD,
  title={Learning to Draw: Emergent Communication through Sketching},
  author={Daniela Mihai and Jonathon S. Hare},
  booktitle={Neural Information Processing Systems},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235352746}
}

@article{Eloff2021TowardsLT,
  title={Towards Learning to Speak and Hear Through Multi-Agent Communication over a Continuous Acoustic Channel},
  author={Kevin Eloff and Arnu Pretorius and Okko Johannes R{\"a}s{\"a}nen and Herman Arnold Engelbrecht and Herman Kamper},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.02827},
  url={https://api.semanticscholar.org/CorpusID:242757488}
}

@ONLINE{wikidump,
    author = "{Wikimedia Foundation}",
    title  = "Wikimedia Downloads",
    url    = "https://dumps.wikimedia.org"
}

@article{Gage1994ANA,
  title={A new algorithm for data compression},
  author={Philip Gage},
  journal={The C Users Journal archive},
  year={1994},
  volume={12},
  pages={23-38},
  url={https://api.semanticscholar.org/CorpusID:59804030}
}

@inproceedings{
ueda2023on,
title={On the Word Boundaries of Emergent Languages Based on Harris's Articulation Scheme},
author={Ryo Ueda and Taiga Ishii and Yusuke Miyao},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=b4t9_XASt6G}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'e}mi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771},
  url={https://api.semanticscholar.org/CorpusID:208117506}
}


@inproceedings{pytorch,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{adam,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@techreport{WahCUB_200_2011,
	Title = {The Caltech-{UCSD} Birds-200-2011 Dataset},
	Author = {Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.},
	Year = {2011},
	Institution = {California Institute of Technology},
	Number = {CNS-TR-2011-001}
}

@article{Kuhnle2017ShapeWorldA,
  title={ShapeWorld - A new test methodology for multimodal language understanding},
  author={Alexander Kuhnle and Ann A. Copestake},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.04517},
  url={https://api.semanticscholar.org/CorpusID:16515835}
}

@inproceedings{sharma-etal-2018-conceptual,
    title = "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
    author = "Sharma, Piyush  and
      Ding, Nan  and
      Goodman, Sebastian  and
      Soricut, Radu",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1238",
    doi = "10.18653/v1/P18-1238",
    pages = "2556--2565",
    abstract = "We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",
}

@inproceedings{bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{chrf,
    title = "chr{F}: character n-gram {F}-score for automatic {MT} evaluation",
    author = "Popovi{\'c}, Maja",
    booktitle = "Proceedings of the Tenth Workshop on Statistical Machine Translation",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-3049",
    doi = "10.18653/v1/W15-3049",
    pages = "392--395",
}

@inproceedings{post-2018-call,
  title = "A Call for Clarity in Reporting {BLEU} Scores",
  author = "Post, Matt",
  booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
  month = oct,
  year = "2018",
  address = "Belgium, Brussels",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/W18-6319",
  pages = "186--191",
}
