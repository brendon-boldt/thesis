\section{Results}

\begin{figure*}
  \centering
  \inputsrc{figures/clm}
  \caption{%
    Average cross-entropy on target language datasets for each source language.
    Lower is better.
    Error bars represent $95\%$ confidence intervals.
  }
  \unskip\label{fig:ces}
\end{figure*}

\subsection{XferBench}

In \Cref{fig:ces} we show the results of the benchmark (i.e., causal language modeling) on the various baselines.
Each mean is displayed with error bars showing the $95\%$ confidence interval of mean as calculated with bootstrapping (details in \Cref{sec:bootstrapping}).
For reference, the cross-entropies range from about $5.2$ to $5.5$ corresponding to perplexities of $180$ to $240$.

The human languages show the best score (lowest cross-entropy) on the benchmark with \emph{Chinese}, \emph{Korean}, and \emph{Arabic} performing the best in one cluster and \emph{French}, \emph{Spanish}, \emph{Russian}, and \emph{Hindi} performing slightly worse in their own cluster (based on confidence intervals).
The synthetic and emergent languages all show similar performance with only small variations with the exception of the \emph{Disc, large} language which is better than the rest of the emergent languages but still worse than the human languages.
Finally, the random baselines perform worse than the rest of the tested languages.
\emph{No pretrain}'s performance is worse than the cluster of synthetic and emergent languages but better than the fully random language (\emph{Random}).


\subsection{Machine Translation}

\begin{table}[t]
  \centering
  \inputsrc{figures/mt-chrf}
  \caption{%
    chrF scores across three English-to-French machine translation settings.
    Correlation measured with the Pearson correlation coefficient.
    Colors normalized by column.
  }
  \unskip\label{fig:mt}
\end{table}

The chrF scores of the machine translation experiment are given in \Cref{fig:mt} (BLEU scores in \Cref{sec:mt-bleu}).
Additionally, we give Pearson correlation coefficients between each setting and the scores generated by XferBench (scatter plots shown in \Cref{sec:scatter}).
In all settings, we see that XferBench is strongly correlated with the results of the machine translation experiment.

For the \emph{Full} setting, the results are somewhat inconclusive.
Human languages perform the best and similarly to each other.
\emph{Paren, real},
  \emph{Paren, syn},
  \emph{Disc, large},
  and \emph{Mu+, CUB}
  all match the performance of human languages as well.
The rest of the language perform significantly worse than the aforementioned languages, especially \emph{Yao+} and \emph{Mu+, SW} (see \Cref{sec:error-analysis} for sample outputs).
In the case of \emph{Random}, the training loss did not decrease during training likely due to the high learning rate.

In \emph{Frozen}, we see the best correlation with the hypothesis regarding human languages (as well as with XferBench itself).
\emph{Disc, large} performs comparably to the worst human languages and better than the rest of the languages.
The remainder of the synthetic and emergent languages perform worse than the human languages but better than the random baselines.

Finally, \emph{Reduced} (i.e., lower learning rate and tuning data) displays better separation than \emph{Full}, but not as significant as \emph{Frozen}.
Human languages still perform the best, although they are matched by the \emph{Paren} languages.
\emph{Disc, large} underperforms the human languages but still outperforms all other emergent languages.
All emergent languages, apart from \emph{Disc., large} underperform the \emph{No pretrain} baseline.
The better half of languages performed better (compared to themselves) with a higher learning rate while the lower half performed better with a reduced learning rate.


\section{Discussion}

\subsection{Experiments}

The basic ordering of the language by XferBench follows basic \emph{a priori} assumptions:
  random baselines perform the worst,
  human languages perform the best,
  and emergent and synthetic languages are bounded above and below by these
  (supporting Hypothesis 1).
Human languages cluster together in XferBench although there is still variation with non-overlapping confidence intervals (partially supporting Hypothesis 2).

\paragraph{Intra-EL differences}
Generally speaking, there is very little variation shown by XferBench on the emergent languages; nevertheless, we can still draw a handful of conclusions.
First, \emph{Disc, large} outperforms \emph{Disc, small} while sharing the same codebase, task, etc.\@ and differing only in message length, vocabulary size, observation space, and corpus size (supporting Hypothesis 3).
This result matches the trend seen in \citet{yao2022linking} that larger vocabularies and message lengths in an emergent language lead to better performance on downstream data.
On the other hand, \emph{Disc, small} performs similarly to other languages with larger vocabularies and longer message lengths (contradicting Hypothesis 3).

Second, it seems that the underlying complexity of the emergent communication game does not directly correlate with XferBench score: the abstract visual reasoning of \emph{Mu+, SW} and \emph{Mu+, CUB} does not lead to it outperform \emph{Disc, small}.
Additionally, the richer observations (i.e., image embeddings) of \emph{Mu+, CUB} and \emph{Yao+} also do not, by their mere presence, confer an advantage to the emergent language with respect to XferBench.

Finally, \emph{Disc, large} and \emph{Recon, large} both share hyperparameters in terms of the vocabulary size, message length, and corpus size, yet \emph{Disc, large} shows significantly better performance on XferBench.
This indicates that XferBench is not \emph{solely} concerned with surface-level features as we see that the nature of the game (e.g., discrimination versus reconstruction, success rate) is relevant as well.


\paragraph{Correlation with MT}
The results from the machine translation experiment show strong, though not perfect, (negative) correlation with XferBench (supporting Hypothesis 4).
For example, in all cases, \emph{Disc, large} outperforms all other emergent languages.
This strongly supports the notion that XferBench performance is predictive of downstream performance on more concrete NLP tasks.

The results from the \emph{Full} setting of the MT experiment do show some correlation with XferBench but fail to show expected trends in other ways.
For example, there is no clear ordering among the human languages (e.g., \emph{French} does \emph{not} outperform \emph{Arabic}).
Additionally \emph{Yao+} and \emph{Mu+, SW} drastically underperform the other emergent languages and the \emph{No pretrain} baseline.
We suspect that these aberrations from expected results come in part due to the high learning rate which cause unstable training or generation.
On the other hand, the \emph{Frozen} setting gives us the clearest ordering of human languages that matches with \emph{a priori} expectations; this setting also has the strongest correlation with XferBench scores.
The \emph{Reduced} setting shows better correlation than \emph{Full} but is not as clear as \emph{Frozen}.


\paragraph{Random baselines}
In all of our experiments, the pretraining on random tokens (\emph{Random}) performed notably worse than not pretraining at all (\emph{No pretrain}), suggesting that ill-conditioning the neural network can be a significant hindrance to performing well on XferBench.
This is important to note in light of the fact that a perfectly one-to-one compositional language describing uniformly sampled attribute--value vectors would yield a corpus with a uniformly random unigram distribution.
This is to say, a fully compositional language, which is often seen as desirable in emergent communication research, could make for a very poor source of pretraining data as shown by \emph{Random}'s performance on XferBench.

This fact along with the observations about sensitivity to learning rate indicates that performance on XferBench is not simply a function of the particular features of the emergent language in relation to the downstream human languages but also a function of the dynamics of optimization (i.e., priming the model to adapt well).
Although this increases the difficulty of developing and interpreting a tool like XferBench, it is almost an unavoidable part of deep learning methods.

\subsection{Future work}
We identify three main directions for future work with XferBench.
The first direction is determining what XferBench is measuring and how its scores correlate with the different factors of emergent languages.
\citet[app.\@ B.4]{yao2022linking} pursued this on a small scale with factors like vocabulary size and message length, but there exist a host of other factors worth exploring: speaker model size, game design, language entropy, observation modality, etc.

The second direction is more extensively investigating the correlation of XferBench with downstream tasks.
We would expect that tasks that rely heavily on a language model---such as automatic speech recognition, abstractive summarization, and generative question-answering---to correlate well with XferBench.
On the other hand, tasks that are more focused on classification---such as named entity recognition, sentiment analysis, and multiple choice question-answering---might not correlate as well.

Finally, XferBench would benefit greatly from improved compute efficiency.
For example, if the results of XferBench could be replicated with a fraction of the training steps, it could (1) allow for a larger number of downstream languages to be tested which would reduce the size of the confidence intervals, allowing more more precise scoring.
And (2), it would open the door to using larger models which would better capture the deeper structures of language and likely correlate better with realistic downstream tasks.
