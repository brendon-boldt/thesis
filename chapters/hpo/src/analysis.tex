\section{Analysis}
\unskip\label{hpo:sec:analysis}

\subsection{Importance of hyperparameters}
% Performing large scale exploration on the hyperparameters of an emergent communication system reveals a number of trends about they impact \cmt{Revise} transfer learning suitability.
% Since transfer learning suitability closely relates to aspects human language-likeness, these trends are directly relevant to emergent communication research which is aiming to to apply its methods to machine learning and natural language processing.
% \cmt{wordy}
% Indirectly speaking, though, these trends are also relevant to applications of emergent language like linguistics which use it as a proxy for human language; in these cases it is critical that the proxy is as close to the target as possible.
% \cmt{Remove this paragraph?  It sounds like it belongs or is already presented in the discussion section.}

\paragraph{Vocabulary size}
The most notable hyperparameter trend we found was with vocabulary size, where the best-performing languages 
had unique token counts of on the order of $1000$ and vocabulary sizes closer to $10\,000$ (see \Cref{hpo:fig:slice-3}); that is, the model could use up to $10\,000$ unique words but only uses $1000$ after training.
For reference, it is common practice in emergent communication research to use vocabulary sizes well under $100$ (e.g., only $1$ out of the $8$ systems in ELCC produce corpora with ${>}70$ unique tokens).

\paragraph{Scaling up}
Similarly to vocabulary size, we observe indications to scale up message length, neural network layer size, and task information (i.e., number of attributes, values, and distractors):
  the most human like emergent languages require longer training, larger networks, and higher-information tasks than are often used in the emergent communication literature.
Along with vocabulary size, these hyperparameter are most often trivial to adjust, meaning there is little reason not to adjust standard practice in emergent communication research to using hyperparameters in these ranges.

\paragraph{Learning rate}
Finally, in terms of raw importance with respect to XferBench score, learning rate was most significant; this result is not surprising as learning rate is significant in any deep learning algorithm.
Nevertheless, part of the difficulty with learning rate is that there is no one best learning rate, and so performing at least some hyperparameter tuning with learning rate will be necessary for optimal performance.

\paragraph{Summary of recommendations}
We recommend the following hyperparameters as a rule of thumb:
vocabulary size: $10\,000$,
hidden layer size: $256$,
embedding layer size: $128$,
message length: $20$,
observation diversity: the higher the better (e.g., $6^{12}\approx 2\,\text{trillion}$ unique observations),
epochs: train until task success plateau (not just until arbitrary threshold),
learning rate: tune on final setting.


\subsection{Entropy and XferBench}
\unskip\label{hpo:sec:ent-xb}

The most striking correlation we observe in our experiments is between XferBench score and unigram token entropy, which is illustrated in \Cref{hpo:fig:ent-xb} (Pearson's $r=-0.57$ for Search 5r only).
The emergent languages pictured are all those generated by Searches 4 and 5r, while the human languages are taken from \citet{xferbench}. 
We see that low entropy languages tend to score poorly on XferBench while high scoring languages have higher entropy; this aligns with the observed correlation between XferBench and entropy in \citet{elcc}.
Furthermore, this correlation follows the same trend we see in human languages with respect to entropy.

\paragraph{Entropy's lower bound}
In particular, we have illustrated a lower bound of low entropy--low XferBench score that describes both emergent and human languages (the gray dashed line in \Cref{hpo:fig:ent-xb}).
This suggests that given a certain entropy, there is a hard limit on the performance XferBench that can be achieved.
While further theoretical and empirical analysis would be required to verify that this a true lower bound, this aligns with the notion of language models as entropy-minimizers:
Language models, in order to reduce the entropy on a target language, require a certain degree of entropy (i.e., information) in the pretraining data.
Hence, low-entropy, low-information pretraining data leads to low entropy reduction (higher cross-entropy) language models.

\paragraph{Entropy minimization}
Looking again at \Cref{hpo:fig:ent-xb}, we also see that the high-entropy, high-XferBench quadrant (upper right) is also sparsely inhabited.
In fact, emergent and human languages seem to lie primarily near the Pareto frontier of low-entropy, low-XferBench score mentioned above.
This comes in contrast to the XferBench scores of a variety of synthetic languages (descriptions of which are given in \Cref{hpo:sec:synth}) which often do not demonstrate this Pareto efficiency, even for synthetic languages performing well on XferBench.

This result is concordant with the related claim that entropy is ``minimized'' inside of emergent communication systems \citep{kharitonov2020entmin,chaabouni2021color}.
Such work has shown that emergent communication systems tend to find Pareto efficient solutions in terms of maximizing task success and minimizing entropy (this correlation in the hyperparameter search is discussed briefly in \Cref{hpo:sec:ent-vs-acc}).
% What is notable, though, about entropy minimization with respect to XferBench is that it is a \emph{full emergent} phenomenon, as neither entropy nor transfer learning performance are directly optimized for (cf.\@ task success).


\paragraph{Optimizing on entropy directly}
The correlation between entropy and XferBench naturally leads to a potential performance improvement: Why not use entropy as the hyperparameter objective instead of XferBench?
Entropy takes seconds to compute instead of close to an hour.
This is the experiment performed in Search 6e which was successful in producing languages with good XferBench scores but which still performed significantly worse than optimizing on XferBench directly (see \Cref{hpo:fig:bar}).

Given that the lower bound of entropy versus XferBench score is tighter than the upper bound, it is roughly the case that low entropy implies poor XferBench performance, but high entropy does not necessarily imply good XferBench performance.
Thus, the fact that the entropy-based search finds good but not optimal emergent languages fits with the earlier observation about bounds of entropy and XferBench score.
With these observations in mind, a refinement to the hyperparameter search algorithm would be to prune low-entropy trials before running XferBench while fully evaluating the trial on XferBench if has a high entropy.


\paragraph{Task success}
\begin{figure}
  \centering
  \inputHpo{assets/acc-vs-xb.pgf}
  \caption{%
    Accuracy versus XferBench for Search 5r.
    Accuracy is measured as proportion of rounds for which the correct observation is ranked in the top-$1$ percentile among all distractors.}
  \unskip\label{hpo:fig:acc-vs-xb}
\end{figure}
The correlation between task success and XferBench score (\Cref{hpo:fig:acc-vs-xb}, Pearson's $r=-0.40$) is not as dramatic as with entropy.
Nevertheless, the negative correlation (better task success, better XferBench score) matches the expectation that the realism of emergent language is positively correlated with the efficacy of the language.
This relationship is a foundational assumption of emergent communication techniques generally: the realism of simulation-derived language comes, in part, from its development out of the functional pressures to communicate.



% \begin{enumerate}
%   \item Entropy and XferBench scores are correlated; confirms what we see in ELCC; look at this graph.
%   \item The lower left part of the plot shows that a minimum entropy is required to achieve a certain XferBench score.
%   \item The fact that emergent languages hug this Pareto frontier seems to suggest that there is some factor ``minimizing entropy'' given XferBench score.
%   \item Success does seems to be related to XferBench score but only loosely, by comparison; direction of causality is not clear.
%   \item Success and entropy have a similar relationship, as we seen in previous literature.
% \end{enumerate}


% user_attrs_entropy     vs top1pct_acc           : +0.57
% user_attrs_entropy     vs user_attrs_xferbench  : -0.57
% top1pct_acc            vs user_attrs_xferbench  : -0.40


% \subsection{Distributions of emergent languages}
% In this section, we look at the distributions of emergent languages to see if they at all resemble the Zipfian distribution present in natural language.
