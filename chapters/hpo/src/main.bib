@article{boldt2023mathmodel,
      title={Mathematically Modeling the Lexicon Entropy of Emergent Language}, 
      author={Brendon Boldt and David Mortensen},
      year={2023},
      eprint={2211.15783},
      volume={2211.15783},
      archivePrefix={arXiv},
      journal={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.15783}, 
}

@inproceedings{boldt-mortensen-2024-xferbench,
    title = "{X}fer{B}ench: a Data-Driven Benchmark for Emergent Language",
    author = "Boldt, Brendon  and
      Mortensen, David",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.82",
    doi = "10.18653/v1/2024.naacl-long.82",
    pages = "1475--1489",
    abstract = "In this paper, we introduce a benchmark for evaluating the overall quality of emergent languages using data-driven methods. Specifically, we interpret the notion of the {``}quality{''} of an emergent language as its similarity to human language within a deep learning framework. We measure this by using the emergent language as pretraining data for a downstream NLP tasks in human language{---}the better the downstream performance, the better the emergent language. We implement this benchmark as an easy-to-use Python package that only requires a text file of utterances from the emergent language to be evaluated. Finally, we empirically test the benchmark{'}s validity using human, synthetic, and emergent language baselines.",
}

@inproceedings{optuna,
  title={{O}ptuna: A Next-Generation Hyperparameter Optimization Framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle={The 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2623--2631},
  year={2019}
}

misc{egg,
  author = "Kharitonov, Eugene  and Dess{\`i}, Roberto and Chaabouni, Rahma  and Bouchacourt, Diane  and Baroni, Marco",
  title = "{EGG}: a toolkit for research on {E}mergence of lan{G}uage in {G}ames",
  howpublished = {\url{https://github.com/facebookresearch/EGG}},
  year = {2021}
}

@misc{elcc,
      title={{ELCC}: the {E}mergent {L}anguage {C}orpus {C}ollection}, 
      author={Brendon Boldt and David Mortensen},
      year={2024},
      eprint={2407.04158},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.04158}, 
}

@inproceedings{ozaki2020tpe-mo,
author = {Ozaki, Yoshihiko and Tanigaki, Yuki and Watanabe, Shuhei and Onishi, Masaki},
title = {Multiobjective tree-structured parzen estimator for computationally expensive optimization problems},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3389817},
doi = {10.1145/3377930.3389817},
abstract = {Practitioners often encounter computationally expensive multiobjective optimization problems to be solved in a variety of real-world applications. On the purpose of challenging these problems, we propose a new surrogate-based multiobjective optimization algorithm that does not require a large evaluation budget. It is called Multiobjective Tree-structured Parzen Estimator (MOTPE) and is an extension of the tree-structured Parzen estimator widely used to solve expensive single-objective optimization problems. Our empirical evidences reveal that MOTPE can approximate Pareto fronts of many benchmark problems better than existing methods with a limited budget. In this paper, we discuss furthermore the influence of MOTPE configurations to understand its behavior.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {533–541},
numpages = {9},
keywords = {bayesian optimization, computationally expensive optimization, infill criteria, machine learning, multiobjective optimization, surrogate modeling, tree-structured parzen estimator},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{watanabe2023tpe-tutorial,
      title={Tree-Structured Parzen Estimator: Understanding Its Algorithm Components and Their Roles for Better Empirical Performance}, 
      author={Shuhei Watanabe},
      year={2023},
      eprint={2304.11127},
      volume={2304.11127},
      archivePrefix={arXiv},
      journal={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.11127}, 
}

@inproceedings{bergstra2011tpe,
 author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Algorithms for Hyper-Parameter Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
 volume = {24},
 year = {2011}
}

article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{
maddison2017the,
title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
author={Chris J. Maddison and Andriy Mnih and Yee Whye Teh},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=S1jE5L5gl}
}

@inproceedings{
jang2017categorical,
title={Categorical Reparameterization with Gumbel-Softmax},
author={Eric Jang and Shixiang Gu and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rkE3y85ee}
}

article{
boldt2024review,
title={A Review of the Applications of Deep Learning-Based Emergent Communication},
author={Brendon Boldt and David R Mortensen},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=jesKcQxQ7j},
note={}
}

inproceedings{
chaabouni2022emergent,
title={Emergent Communication at Scale},
author={Rahma Chaabouni and Florian Strub and Florent Altch{\'e} and Eugene Tarassov and Corentin Tallec and Elnaz Davoodi and Kory Wallace Mathewson and Olivier Tieleman and Angeliki Lazaridou and Bilal Piot},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=AUGBfDIV9rL}
}

inproceedings{
yao2022linking,
title={Linking Emergent and Natural Languages via Corpus Transfer},
author={Shunyu Yao and Mo Yu and Yang Zhang and Karthik R Narasimhan and Joshua B. Tenenbaum and Chuang Gan},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=49A1Y6tRhaq}
}

misc{lazaridou2020emergentmultiagentcommunicationdeep,
      title={Emergent Multi-Agent Communication in the Deep Learning Era}, 
      author={Angeliki Lazaridou and Marco Baroni},
      year={2020},
      eprint={2006.02419},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.02419}, 
}

@InCollection{sep-linguistics,
	author       =	{Scholz, Barbara C. and Pelletier, Francis Jeffry and Pullum, Geoffrey K. and Nefdt, Ryan},
	title        =	{{Philosophy of Linguistics}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta and Uri Nodelman},
	howpublished =	{\url{https://plato.stanford.edu/archives/spr2024/entries/linguistics/}},
	year         =	{2024},
	edition      =	{{S}pring 2024},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@inproceedings{
mu2021emergent,
title={Emergent Communication of Generalizations},
author={Jesse Mu and Noah Goodman},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=yq5MYHVaClG}
}

@article{piantadosi2014zipf,
  title={Zipf’s word frequency law in natural language: A critical review and future directions},
  author={Piantadosi, S.T.},
  year=2014,
  journal={Psychon Bull Rev},
  volume={21},
  pages={1112-–1130},
  url={https://doi.org/10.3758/s13423-014-0585-6},
}

@article{schutzenberger1963,
title = {On context-free languages and push-down automata},
journal = {Information and Control},
volume = {6},
number = {3},
pages = {246-264},
year = {1963},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(63)90306-1},
url = {https://www.sciencedirect.com/science/article/pii/S0019995863903061},
author = {M.P. Schützenberger},
abstract = {This note describes a special type of one-way, one-tape automata in the sense of Rabin and Scott that idealizes some of the elementary formal features used in the so-called “push-down store” programming techniques. It is verified that the sets of words accepted by these automata form a proper subset of the family of the unambiguous context-free languages of Chomsky's and that this property admits a weak converse.}
}

@inproceedings{gupta-etal-2020-compositionality,
    title = "Compositionality and Capacity in Emergent Languages",
    author = "Gupta, Abhinav  and
      Resnick, Cinjon  and
      Foerster, Jakob  and
      Dai, Andrew  and
      Cho, Kyunghyun",
    editor = "Gella, Spandana  and
      Welbl, Johannes  and
      Rei, Marek  and
      Petroni, Fabio  and
      Lewis, Patrick  and
      Strubell, Emma  and
      Seo, Minjoon  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 5th Workshop on Representation Learning for NLP",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.repl4nlp-1.5",
    doi = "10.18653/v1/2020.repl4nlp-1.5",
    pages = "34--38",
    abstract = "Recent works have discussed the extent to which emergent languages can exhibit properties of natural languages particularly learning compositionality. In this paper, we investigate the learning biases that affect the efficacy and compositionality in multi-agent communication in addition to the communicative bandwidth. Our foremost contribution is to explore how the capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.",
}

@InProceedings{kharitonov2020entmin,
  title = 	 {Entropy Minimization In Emergent Languages},
  author =       {Kharitonov, Eugene and Chaabouni, Rahma and Bouchacourt, Diane and Baroni, Marco},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5220--5230},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/kharitonov20a/kharitonov20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/kharitonov20a.html},
  abstract = 	 {There is growing interest in studying the languages that emerge when neural agents are jointly trained to solve tasks requiring communication through a discrete channel. We investigate here the information-theoretic complexity of such languages, focusing on the basic two-agent, one-exchange setup. We find that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the communicating agent’s inputs and the messages is minimized, within the range afforded by the need for successful communication. That is, emergent languages are (nearly) as simple as the task they are developed for allow them to be. This pressure is amplified as we increase communication channel discreteness. Further, we observe that stronger discrete-channel-driven entropy minimization leads to representations with increased robustness to overfitting and adversarial attacks. We conclude by discussing the implications of our findings for the study of natural and artificial communication systems.}
}

@article{
chaabouni2021color,
author = {Rahma Chaabouni  and Eugene Kharitonov  and Emmanuel Dupoux  and Marco Baroni },
title = {Communicating artificial neural networks develop efficient color-naming systems},
journal = {Proceedings of the National Academy of Sciences},
volume = {118},
number = {12},
pages = {e2016569118},
year = {2021},
doi = {10.1073/pnas.2016569118},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2016569118},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2016569118},
abstract = {Color names in human languages are organized into efficient systems optimizing an accuracy/complexity trade-off. We show that artificial neural networks trained with generic deep-learning methods to play a color-discrimination game develop color-naming systems whose distribution on the accuracy/complexity plane is strikingly similar to that of human languages. We proceed to show that efficiency and narrow complexity crucially depend on the discrete nature of communication, acting as an information bottleneck on the emergent code. This suggests that efficient categorization of colors (and possibly other semantic domains) in natural languages does not depend on specific biological constraints of humans, but it is instead a general property of discrete communication systems. Words categorize the semantic fields they refer to in ways that maximize communication accuracy while minimizing complexity. Focusing on the well-studied color domain, we show that artificial neural networks trained with deep-learning techniques to play a discrimination game develop communication systems whose distribution on the accuracy/complexity plane closely matches that of human languages. The observed variation among emergent color-naming systems is explained by different degrees of discriminative need, of the sort that might also characterize different human communities. Like human languages, emergent systems show a preference for relatively low-complexity solutions, even at the cost of imperfect communication. We demonstrate next that the nature of the emergent systems crucially depends on communication being discrete (as is human word usage). When continuous message passing is allowed, emergent systems become more complex and eventually less efficient. Our study suggests that efficient semantic categorization is a general property of discrete communication systems, not limited to human language. It suggests moreover that it is exactly the discrete nature of such systems that, acting as a bottleneck, pushes them toward low complexity and optimal efficiency.}}

book{zipf1949least,
  author = {GK Zipf},
  year = 1949,
  title = "Human behavior and the principle of least effort",
  publisher = "Addison-Wesley",
  address = "Cambridge, MA",
}

@inproceedings{suzgun-etal-2019-lstm,
    title = "{LSTM} Networks Can Perform Dynamic Counting",
    author = "Suzgun, Mirac  and
      Belinkov, Yonatan  and
      Shieber, Stuart  and
      Gehrmann, Sebastian",
    editor = "Eisner, Jason  and
      Gall{\'e}, Matthias  and
      Heinz, Jeffrey  and
      Quattoni, Ariadna  and
      Rabusseau, Guillaume",
    booktitle = "Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges",
    month = aug,
    year = "2019",
    address = "Florence",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3905",
    doi = "10.18653/v1/W19-3905",
    pages = "44--54",
    abstract = "In this paper, we systematically assess the ability of standard recurrent networks to perform dynamic counting and to encode hierarchical representations. All the neural models in our experiments are designed to be small-sized networks both to prevent them from memorizing the training sets and to visualize and interpret their behaviour at test time. Our results demonstrate that the Long Short-Term Memory (LSTM) networks can learn to recognize the well-balanced parenthesis language (Dyck-1) and the shuffles of multiple Dyck-1 languages, each defined over different parenthesis-pairs, by emulating simple real-time k-counter machines. To the best of our knowledge, this work is the first study to introduce the shuffle languages to analyze the computational power of neural networks. We also show that a single-layer LSTM with only one hidden unit is practically sufficient for recognizing the Dyck-1 language. However, none of our recurrent networks was able to yield a good performance on the Dyck-2 language learning task, which requires a model to have a stack-like mechanism for recognition.",
}
