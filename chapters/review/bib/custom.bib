@article{baker1985mirror,
 ISSN = {00243892, 15309150},
 URL = {http://www.jstor.org/stable/4178442},
 author = {Mark Baker},
 journal = {Linguistic Inquiry},
 number = {3},
 pages = {373--415},
 publisher = {The MIT Press},
 title = {The Mirror Principle and Morphosyntactic Explanation},
 urldate = {2023-08-21},
 volume = {16},
 year = {1985}
}

@inproceedings{Bybee1985MorphologyAS,
  title={Morphology: A study of the relation between meaning and form},
  author={Joan L. Bybee},
  year={1985},
  url={https://api.semanticscholar.org/CorpusID:120471918}
}
@book{flemming2013auditory,
  title={Auditory representations in phonology},
  author={Flemming, Edward S},
  year={2013},
  publisher={Routledge}
}

@book{blevins2004evolutionary,
  title={Evolutionary phonology: The emergence of sound patterns},
  author={Blevins, Juliette},
  year={2004},
  publisher={Cambridge University Press}
}

@Article{roberts2017linguists,
  author = 	 {Roberts, Gareth},
  title = 	 {The linguist's {Drosophila}: Experiments in language change},
  journal = 	 {Linguistics Vanguard},
  year = 	 {2017},
  volume =	 {2017},
  pages =	 {1--13}
}

@InCollection{weinreich1968empirical,
  author = 	 {Weinreich, Uriel and Labov, William and Herzog, Marvin},
  title = 	 {Empirical foundations for a theory of language change},
  booktitle = 	 {Directions for Historical Linguistics},
  publisher =	 {University of Texas Press},
  year =	 {1968},
  editor =	 {Lehman, Winfred},
  pages =	 {95--88},
  address =	 {Austin}
}

@article{Smith_Kirby_Brighton_2003, title={Iterated Learning: A Framework for the Emergence of Language}, volume={9}, ISSN={1064-5462}, DOI={10.1162/106454603322694825}, abstractNote={Language is culturally transmitted. Iterated learning, the process by which the output of one individual’s learning becomes the input to other individuals’ learning, provides a framework for investigating the cultural evolution of linguistic structure. We present two models, based upon the iterated learning framework, which show that the poverty of the stimulus available to language learners leads to the emergence of linguistic structure. Compositionality is language’s adaptation to stimulus poverty.}, number={4}, journal={Artificial Life}, publisher={MIT Press}, author={Smith, Kenny and Kirby, Simon and Brighton, Henry}, year={2003}, month={Oct}, pages={371–386} }


@article{resnick_capacity_2020,
	title = {Capacity, {Bandwidth}, and {Compositionality} in {Emergent} {Language} {Learning}},
	url = {http://arxiv.org/abs/1910.11424},
	urldate = {2020-06-01},
	journal = {International Conference on Autonomous Agents and Multi-Agent Systems},
	author = {Resnick, Cinjon and Gupta, Abhinav and Foerster, Jakob and Dai, Andrew M. and Cho, Kyunghyun},
	month = apr,
	year = {2020},
}

@article{goldwater2011pitmanyor,
  title={Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models},
  author={Sharon Goldwater and Thomas L. Griffiths and Mark Johnson},
  journal={J. Mach. Learn. Res.},
  year={2011},
  volume={12},
  pages={2335-2382}
}

@ARTICLE{shannon,  author={Shannon, C. E.},  journal={The Bell System Technical Journal},   title={A mathematical theory of communication},   year={1948},  volume={27},  number={3},  pages={379-423},  doi={10.1002/j.1538-7305.1948.tb01338.x}}

 @inproceedings{jaques2019social, title={Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning}, ISSN={2640-3498}, url={<a href="http://proceedings.mlr.press/v97/jaques19a.html">http://proceedings.mlr.press/v97/jaques19a.html</a>}, booktitle={International Conference on Machine Learning}, publisher={PMLR}, author={Jaques, Natasha and Lazaridou, Angeliki and Hughes, Edward and Gulcehre, Caglar and Ortega, Pedro and Strouse, Dj and Leibo, Joel Z. and Freitas, Nando De}, year={2019}, month={May}, pages={3040–3049} }

@misc{hazra2021zeroshot,
  title = {Zero-Shot Generalization using Intrinsically Motivated Compositional Emergent Protocols},
  author = {Rishi Hazra and Sonu Dixit and Sayambhu Sen},
  howpublished = {Visually Grounded Interaction and Language Workshop, NAACL},
  year         = 2021,
  note = {},
}

@misc{brandizzi2021rlupus,
  IDS={'2021rlupus,Brandizzi2021RLupusCT},
    title={RLupus: Cooperation through emergent communication in The Werewolf social deduction game},
    author={Nicolo' Brandizzi and Davide Grossi and Luca Iocchi},
    year={2021},
    eprint={2106.05018},
    archivePrefix={arXiv},
    primaryClass={cs.MA}
}

@misc{bullard2021quasiequivalence,
    title={Quasi-Equivalence Discovery for Zero-Shot Emergent Communication},
    author={Kalesha Bullard and Douwe Kiela and Franziska Meier and Joelle Pineau and Jakob Foerster},
    year={2021},
    eprint={2103.08067},
    archivePrefix={arXiv},
    primaryClass={cs.MA}
}

@Inbook{Wiewiora2010,
author="Wiewiora, Eric",
editor="Sammut, Claude
and Webb, Geoffrey I.",
title="Reward Shaping",
bookTitle="Encyclopedia of Machine Learning",
year="2010",
publisher="Springer US",
address="Boston, MA",
pages="863--865",
isbn="978-0-387-30164-8",
doi="10.1007/978-0-387-30164-8_731",
url="https://doi.org/10.1007/978-0-387-30164-8_731"
}

@article{schulman2017ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@comment{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{blei2007crp,
    author = {Blei, David},
    title = {The Chinese Restaurant Process},
    url = {https://www.cs.princeton.edu/courses/archive/fall07/cos597C/scribe/20070921.pdf},
    year = {2007},
}

@InProceedings{aldous1985exchangeability,
author="Aldous, David J.",
editor="Hennequin, P. L.",
title="Exchangeability and related topics",
booktitle="{\'E}cole d'{\'E}t{\'e} de Probabilit{\'e}s de Saint-Flour XIII --- 1983",
year="1985",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--198",
isbn="978-3-540-39316-0"
}


@inproceedings{tomlin2018incremental,
  title={Incremental pragmatics and emergent communication},
  author={Tomlin, Nicholas and Pavlick, Ellie},
  booktitle={Neural Information Processing Systems Workshop on Emergent Communication},
  year={2018}
}


 @inproceedings{bouchacourt20219tool, title={Miss Tools and Mr Fruit: Emergent communication in agents learning about object affordances}, DOI={10.18653/v1/p19-1380}, booktitle={ACL}, author={Bouchacourt, Diane and Baroni, Marco}, year={2019} }


@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@inproceedings{kingma2015adam,
  author={Diederik P. Kingma and Jimmy Ba},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  cdate={1420070400000},
  url={http://arxiv.org/abs/1412.6980},
  booktitle={ICLR (Poster)},
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@misc{stable-baselines3,
  author = {Raffin, Antonin and Hill, Ashley and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Dormann, Noah},
  title = {Stable Baselines3},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/stable-baselines3}},
}

@article{vogel1979sunflower,
title = {A better way to construct the sunflower head},
journal = {Mathematical Biosciences},
volume = {44},
number = {3},
pages = {179-189},
year = {1979},
issn = {0025-5564},
doi = {https://doi.org/10.1016/0025-5564(79)90080-4},
url = {https://www.sciencedirect.com/science/article/pii/0025556479900804},
author = {Helmut Vogel},
}

@misc{arumugam2020information,
  title = {An Information-Theoretic Perspective on Credit Assignment in Reinforcement Learning},
  author = {Arumugam, Dilip and Henderson, Peter and Bacon, Pierre-Luc},
  howpublished = {Biological and Artificial Reinforcement Learning Workshop, NeurIPS},
  year         = 2020,
  note = {},
}

@inproceedings{trott2019keeping,
 author = {Trott, Alexander and Zheng, Stephan and Xiong, Caiming and Socher, Richard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards},
 url = {https://proceedings.neurips.cc/paper/2019/file/64c26b2a2dcf068c49894bd07e0e6389-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{
    amiranashvili2018analyzing,
    title={Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning},
    author={Artemij Amiranashvili and Alexey Dosovitskiy and Vladlen Koltun and Thomas Brox},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=HyiAuyb0b},
}

@inproceedings{lu2020seedediterated,
  title={Countering language drift with seeded iterated learning},
  author={Lu, Yuchen and Singhal, Soumye and Strub, Florian and Courville, Aaron and Pietquin, Olivier},
  booktitle={International Conference on Machine Learning},
  pages={6437--6447},
  year={2020},
  organization={PMLR}
}

@comment{chaabouni2021color, title={Communicating artificial neural networks develop efficient color-naming systems}, volume={118}, ISSN={0027-8424, 1091-6490}, url={https://www.pnas.org/content/118/12/e2016569118}, DOI={10.1073/pnas.2016569118}, number={12}, journal={Proceedings of the National Academy of Sciences}, publisher={National Academy of Sciences}, author={Chaabouni, Rahma and Kharitonov, Eugene and Dupoux, Emmanuel and Baroni, Marco}, year={2021}, month={Mar} }

@InProceedings{fujimoto2019offpolicy, title = {Off-Policy Deep Reinforcement Learning without Exploration}, author = {Fujimoto, Scott and Meger, David and Precup, Doina}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {2052--2062}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/fujimoto19a/fujimoto19a.pdf}, url = { http://proceedings.mlr.press/v97/fujimoto19a.html }}
@inproceedings{kimdambi2020morel,
 author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {21810--21823},
 publisher = {Curran Associates, Inc.},
 title = {MOReL: Model-Based Offline Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf},
 volume = {33},
 year = {2020}
}
@inproceedings{agarwal2020optimistic,
  title={An optimistic perspective on offline reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@comment{jang2017categorical,
    title = "Categorical Reparameterization with Gumbel-Softmax",
    author = "Jang, Eric and Gu, Shixian and Poole, Ben",
    booktitle = "Proceedings of the 2017 International Conference on Learning Representations (ICLR)",
    year = "2017",
    url = "https://openreview.net/forum?id=rkE3y85ee",
}

@comment{maddison2017concrete,
    title = "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
    author = "Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye",
    booktitle = {Proceedings of the 2017 International Conference on Learning Representations (ICLR)},
    year = "2017",
    url = "https://openreview.net/forum?id=S1jE5L5gl",
}


@comment{bisk-etal-2020-experience,
    title = "Experience Grounds Language",
    author = "Bisk, Yonatan  and
      Holtzman, Ari  and
      Thomason, Jesse  and
      Andreas, Jacob  and
      Bengio, Yoshua  and
      Chai, Joyce  and
      Lapata, Mirella  and
      Lazaridou, Angeliki  and
      May, Jonathan  and
      Nisnevich, Aleksandr  and
      Pinto, Nicolas  and
      Turian, Joseph",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.703",
    doi = "10.18653/v1/2020.emnlp-main.703",
    pages = "8718--8735",
}

@InProceedings{lu2020countering,
    title = {Countering Language Drift with Seeded Iterated Learning}, author = {Lu, Yuchen and Singhal, Soumye and Strub, Florian and Courville, Aaron and Pietquin, Olivier}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {6437--6447}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, address = {Virtual}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/lu20c/lu20c.pdf}, url = {http://proceedings.mlr.press/v119/lu20c.html}
}

@comment {gibson2017color,
	author = {Gibson, Edward and Futrell, Richard and Jara-Ettinger, Julian and Mahowald, Kyle and Bergen, Leon and Ratnasingam, Sivalogeswaran and Gibson, Mitchell and Piantadosi, Steven T. and Conway, Bevil R.},
	title = {Color naming across languages reflects color use},
	volume = {114},
	number = {40},
	pages = {10785--10790},
	year = {2017},
	doi = {10.1073/pnas.1619666114},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/114/40/10785},
	eprint = {https://www.pnas.org/content/114/40/10785.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@Book{berlin1969basic,
 author = {Berlin, Brent and Kay, Paul},
 title = {Basic Color Terms : Their Universality and Evolution},
 publisher = {University of California Press},
 year = {1969},
 address = {Berkeley, Calif},
 isbn = {0520014421}
}

@inproceedings{havrylov2017advances,
 author = {Havrylov, Serhii and Titov, Ivan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {2149--2159},
 publisher = {Curran Associates, Inc.},
 title = {Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols},
 url = {https://proceedings.neurips.cc/paper/2017/file/70222949cc0db89ab32c9969754d4758-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{kajic2020learning,
    title={Learning to cooperate: Emergent communication in multi-agent navigation},
    author={Ivana Kaji\'c and Eser Ayg\"un and Doina Precup},
    booktitle={42nd Annual Meeting of the Cognitive Science Society},
    year={2020},
    pages={1993--1999},
    publisher={Cognitive Science Society},
    address={Toronto, ON},
    pdf={http://compneuro.uwaterloo.ca/files/publications/kajic.2020.pdf},
    abstract={
              Emergent communication in artificial agents has been studied to
              understand language evolution, as well as to develop artificial
              systems that learn to communicate with humans. We show that
              agents performing a cooperative navigation task in various
              gridworld environments learn an interpretable communication
              protocol that enables them to efficiently, and in many cases,
              optimally, solve the task. An analysis of the agents' policies
              reveals that emergent signals spatially cluster the state space,
              with signals referring to specific locations and spatial
              directions such as left, up, or upper left
              room. Using populations of agents, we show that the emergent
    protocol has basic compositional structure, thus exhibiting a core property
    of natural language.}
}

 @InProceedings{kharitonov2020entropy, title = {Entropy Minimization In Emergent Languages}, author = {Kharitonov, Eugene and Chaabouni, Rahma and Bouchacourt, Diane and Baroni, Marco}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {5220--5230}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/kharitonov20a/kharitonov20a.pdf}, url = { http://proceedings.mlr.press/v119/kharitonov20a.html }, abstract = {There is growing interest in studying the languages that emerge when neural agents are jointly trained to solve tasks requiring communication through a discrete channel. We investigate here the information-theoretic complexity of such languages, focusing on the basic two-agent, one-exchange setup. We find that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the communicating agent’s inputs and the messages is minimized, within the range afforded by the need for successful communication. That is, emergent languages are (nearly) as simple as the task they are developed for allow them to be. This pressure is amplified as we increase communication channel discreteness. Further, we observe that stronger discrete-channel-driven entropy minimization leads to representations with increased robustness to overfitting and adversarial attacks. We conclude by discussing the implications of our findings for the study of natural and artificial communication systems.} }

@misc{mordatch2018emergence,
      title={Emergence of Grounded Compositional Language in Multi-Agent Populations},
      author={Igor Mordatch and Pieter Abbeel},
      year={2018},
      eprint={1703.04908},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{Francis2021,
    author = {Francis, David and Rabinovich, Ella and Samir, Farhan and Mortensen, David R. and Stevenson, Suzanne},
    title = "{Quantifying Cognitive Factors in Lexical Decline}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {1529-1545},
    year = {2021},
    month = {12},
    abstract = "{We adopt an evolutionary view on language change in which cognitive factors (in addition to social ones) affect the fitness of words and their success in the linguistic ecosystem. Specifically, we propose a variety of psycholinguistic factors—semantic, distributional, and phonological—that we hypothesize are predictive of lexical decline, in which words greatly decrease in frequency over time. Using historical data across three languages (English, French, and German), we find that most of our proposed factors show a significant difference in the expected direction between each curated set of declining words and their matched stable words. Moreover, logistic regression analyses show that semantic and distributional factors are significant in predicting declining words. Further diachronic analysis reveals that declining words tend to decrease in the diversity of their lexical contexts over time, gradually narrowing their ‘ecological niches’.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00441},
    url = {https://doi.org/10.1162/tacl\_a\_00441},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00441/1979747/tacl\_a\_00441.pdf},
}

@article{andreas_measuring_2019,
	title = {Measuring {Compositionality} in {Representation} {Learning}},
	url = {http://arxiv.org/abs/1902.07181},
	urldate = {2021-12-23},
	journal = {arXiv:1902.07181 [cs, stat]},
	author = {Andreas, Jacob},
	month = apr,
	year = {2019},
	note = {arXiv: 1902.07181},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/2KHXETYI/Andreas - 2019 - Measuring Compositionality in Representation Learn.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/AIY9DBY2/1902.html:text/html},
}

@comment{brighton2006UnderstandingLE,
  title={Understanding Linguistic Evolution by Visualizing the Emergence of Topographic Mappings},
  author={Henry Brighton and Simon Kirby},
  journal={Artificial Life},
  year={2006},
  volume={12},
  pages={229-242}
}

@inproceedings{
baker2020emergent,
title={Emergent Tool Use From Multi-Agent Autocurricula},
author={Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkxpxJBKwS}
}\cmg{cite OpenAI hide and seek}

@misc{silver2017mastering,
    title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
    author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
    year={2017},
    eprint={1712.01815},
    url={https://arxiv.org/abs/1712.01815},
    note={arXiv:1712.01815},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

 @article{Baroni_2020, title={Linguistic generalization and compositionality in modern artificial neural networks}, volume={375}, ISSN={0962-8436, 1471-2970}, DOI={10.1098/rstb.2019.0307}, note={arXiv: 1904.00157}, number={1791}, journal={Philosophical Transactions of the Royal Society B: Biological Sciences}, author={Baroni, Marco}, year={2020}, month={Feb}, pages={20190307} }

@misc{devaux_list_2021,
	title = {The list of synthetic data companies — 2021},
	url = {https://elise-deux.medium.com/the-list-of-synthetic-data-companies-2021-5aa246265b42},
	abstract = {Are you looking for a synthetic data company? Check out this list of synthetic data vendors.},
	language = {en},
	urldate = {2022-07-25},
	journal = {Medium},
	author = {Devaux, Elise},
	month = oct,
	year = {2021},
	file = {Snapshot:/home/brendon/academic/misc/Zotero/storage/RVFZKP97/the-list-of-synthetic-data-companies-2021-5aa246265b42.html:text/html},
}

@inproceedings{Andreas2018LearningWL,
  title={Learning with Latent Language},
  author={Jacob Andreas and Dan Klein and Sergey Levine},
  booktitle={NAACL},
  year={2018}
}

@article{Singh2019LearningWT,
  title={Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks},
  author={Amanpreet Singh and Tushar Jain and Sainbayar Sukhbaatar},
  journal={ArXiv},
  year={2019},
  volume={abs/1812.09755}
}

@article{Das2019TarMACTM,
  title={TarMAC: Targeted Multi-Agent Communication},
  author={Abhishek Das and Th{\'e}ophile Gervet and Joshua Romoff and Dhruv Batra and Devi Parikh and Michael G. Rabbat and Joelle Pineau},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.11187}
}

 @inproceedings{narayanchen2019collaborative, address={Florence, Italy}, title={Collaborative Dialogue in Minecraft}, url={https://aclanthology.org/P19-1537}, DOI={10.18653/v1/P19-1537}, abstractNote={We wish to develop interactive agents that can communicate with humans to collaboratively solve tasks in grounded scenarios. Since computer games allow us to simulate such tasks without the need for physical robots, we define a Minecraft-based collaborative building task in which one player (A, the Architect) is shown a target structure and needs to instruct the other player (B, the Builder) to build this structure. Both players interact via a chat interface. A can observe B but cannot place blocks. We present the Minecraft Dialogue Corpus, a collection of 509 conversations and game logs. As a first step towards our goal of developing fully interactive agents for this task, we consider the subtask of Architect utterance generation, and show how challenging it is.}, booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, publisher={Association for Computational Linguistics}, author={Narayan-Chen, Anjali and Jayannavar, Prashant and Hockenmaier, Julia}, year={2019}, month={Jul}, pages={5405–5415} }

@conference{dekker2020contact,
  title = "Neural Agent-based Models To Study Language Contact Using Linguistic Data",
  keywords = "agent-based models, neural networks, emergent communication, language change, language contact, deep learning",
  author = "Peter Dekker and {De Boer}, Bart",
  year = "2020",
  month = "12",
  day = "12",
  language = "English",
  pages = "1--6",
  note = "4th NeurIPS Workshop on Emergent Communication : Talking to Strangers: Zero-Shot Emergent Communication ; Conference date: 12-12-2020 Through 12-12-2020",
  url = "https://sites.google.com/view/emecom2020/home",
}

@inproceedings{
choi2018multiagent,
title={Multi-Agent Compositional Communication Learning from Raw Visual Input},
author={Edward Choi and Angeliki Lazaridou and Nando de Freitas},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rknt2Be0-},
}

@inproceedings{
herrmann2022sifting,
title={Sifting the Signal from the Noise},
author={Daniel Alexander Herrmann and Jacob VanDrunen},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=rcUeWQbRQb5}
}

@inproceedings{
ossenkopf2022which,
title={Which Language Evolves Between Heterogeneous Agents? - Communicating Movement Instructions With Widely Different Time Scopes},
author={Marie Ossenkopf and Kevin Sebastian Luck and Kory Wallace Mathewson},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=BnfgM7-0mW5}
}

@article{ha2018worldmodels,
  doi = {10.5281/ZENODO.1207631},
  url = {https://zenodo.org/record/1207631},
  author = {Ha, David and Schmidhuber, Jürgen},
  title = {World Models},
  publisher = {Zenodo},
  year = {2018},
  copyright = {Creative Commons Attribution 4.0}
}

@inproceedings{
cope2022joining,
title={Joining the Conversation: Towards Language Acquisition for Ad Hoc Team Play},
author={Dylan Cope and Peter McBurney},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=SLqgf7ZCQbq}
}

@inproceedings{
lo2022learning,
title={Learning To Ground Decentralized Multi-Agent Communication with Contrastive Learning},
author={Yat Long Lo and Biswa Sengupta},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=rLceWXWCmZc}
}

@article{Kirby2008CumulativeCE,
  title={Cumulative cultural evolution in the laboratory: An experimental approach to the origins of structure in human language},
  author={Simon Kirby and Hannah Cornish and Kenny Smith},
  journal={Proceedings of the National Academy of Sciences},
  year={2008},
  volume={105},
  pages={10681 - 10686}
}

@comment{Werner_Dyer_1991,
    title={Evolution of Communication in Artificial Organisms},
    author={Werner, Gregory M. and Dyer, Michael G.},
    year={1991},
    pages={659 - 687},
    journal={Artificial Life II},
    editor={Langton, C. G. and Taylor, C. and Farmer, J. D., and Rasmussen, S.},
    publisher={Addison-Wesley}
}

@inbook{kirby_2000, place={Cambridge}, title={Syntax Without Natural Selection: How Compositionality Emerges from Vocabulary in a Population of Learners}, DOI={10.1017/CBO9780511606441.019}, booktitle={The Evolutionary Emergence of Language: Social Function and the Origins of Linguistic Form}, publisher={Cambridge University Press}, author={Kirby, Simon}, editor={Knight, Chris and Studdert-Kennedy, Michael and Hurford, JamesEditors}, year={2000}, pages={303–323}}


@article{rogers2021bertology,
    author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
    title = "{A Primer in BERTology: What We Know About How BERT Works}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {842-866},
    year = {2021},
    month = {01},
    abstract = "{Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00349},
    url = {https://doi.org/10.1162/tacl\_a\_00349},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00349/1923281/tacl\_a\_00349.pdf},
}

@inproceedings{kucinski2020emergence,
  title={Emergence of compositional language in communication through noisy channel},
  author={{\L}ukasz Kuci{\'n}ski and Pawe{\l} Ko{\l}odziej and Piotr Mi{\l}o{\'s}},
  booktitle={Language in Reinforcement Learning Workshop at ICML 2020},
  year={2020},
  url={https://openreview.net/forum?id=ZbXlSL_xwtA}
}


@article{kriegeskorte2008rsa,
  author={Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter},
  title={Representational similarity analysis - connecting the branches of systems neuroscience},
  journal={Frontiers in Systems Neuroscience},
  volume={2},
  year={2008},
  url={https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008},
  doi={10.3389/neuro.06.004.2008},
  issn={1662-5137},
}

@article{Silver2017MasteringTG,
  title={Mastering the game of Go without human knowledge},
  author={David Silver and Julian Schrittwieser and Karen Simonyan and Ioannis Antonoglou and Aja Huang and Arthur Guez and Thomas Hubert and Lucas baker and Matthew Lai and Adrian Bolton and Yutian Chen and Timothy P. Lillicrap and Fan Hui and L. Sifre and George van den Driessche and Thore Graepel and Demis Hassabis},
  journal={Nature},
  year={2017},
  volume={550},
  pages={354-359}
}

@article{parry_2001, title={Triangle centers and central triangles, by Clark Kimberling (Congress Numerantium Vol. 129) Pp. 295. $42.50 1998. ISSN 0316-1282 (Utilitas Mathematica Publishing, Inc., Winnipeg).}, volume={85}, DOI={10.2307/3620531}, number={502}, journal={The Mathematical Gazette}, publisher={Cambridge University Press}, author={Parry, C. F.}, year={2001}, pages={172–173}}

@misc{openaigym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
  note = {arXiv:1606.01540},
  url = {https://arxiv.org/abs/1606.01540},
}

@misc{lipowski2008computational,
  doi = {10.48550/ARXIV.0801.1658},
  url = {https://arxiv.org/abs/0801.1658},
  author = {Lipowski, Adam and Lipowska, Dorota},
  keywords = {Physics and Society (physics.soc-ph), Computation and Language (cs.CL), Multiagent Systems (cs.MA), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Computational approach to the emergence and evolution of language - evolutionary naming game model},
  publisher = {arXiv},
  year = {2008},
  copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004}
}

@Inbook{kirby2002lingstruct,
  author="Kirby, Simon
  and Hurford, James R.",
  editor="Cangelosi, Angelo
  and Parisi, Domenico",
  title="The Emergence of Linguistic Structure: An Overview of the Iterated Learning Model",
  bookTitle="Simulating the Evolution of Language",
  year="2002",
  publisher="Springer London",
  address="London",
  pages="121--147",
  abstract="As language users humans possess a culturally transmitted system of unparalleled complexity in the natural world. Linguistics has revealed over the past 40 years the degree to which the syntactic structure of language in particular is strikingly complex. Furthermore, as Pinker and Bloom point out in their agenda-setting paper Natural Language and Natural Selection ``grammar is a complex mechanism tailored to the transmission of propositional structures through a serial interface'' (Pinker and Bloom, 1990: 707). These sorts of observations, along with influential arguments from linguistics and psychology about the innateness of language (see Chomsky, 1986; Pinker, 1994), have led many authors to the conclusion that an explanation for the origin of syntax must invoke neo-Darwinian natural selection.",
  isbn="978-1-4471-0663-0",
  doi="10.1007/978-1-4471-0663-0_6",
  url="https://doi.org/10.1007/978-1-4471-0663-0_6"
}

@article{kegl1999creation,
  title={Creation through contact: Sign language emergence and sign language change in Nicaragua},
  author={Kegl, Judy and Senghas, Ann and others},
  journal={Language creation and language change: Creolization, diachrony, and development},
  pages={179--237},
  year={1999}
}

@article{kirby2008cumulative,
  title={Cumulative cultural evolution in the laboratory: An experimental approach to the origins of structure in human language},
  author={Kirby, Simon and Cornish, Hannah and Smith, Kenny},
  journal={Proceedings of the National Academy of Sciences},
  volume={105},
  number={31},
  pages={10681--10686},
  year={2008},
  publisher={National Acad Sciences}
}

@article{
kirby2007innateness,
author = {Simon Kirby  and Mike Dowman  and Thomas L. Griffiths },
title = {Innateness and culture in the evolution of language},
journal = {Proceedings of the National Academy of Sciences},
volume = {104},
number = {12},
pages = {5241-5245},
year = {2007},
doi = {10.1073/pnas.0608222104},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0608222104},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0608222104},
abstract = {Human language arises from biological evolution, individual learning, and cultural transmission, but the interaction of these three processes has not been widely studied. We set out a formal framework for analyzing cultural transmission, which allows us to investigate how innate learning biases are related to universal properties of language. We show that cultural transmission can magnify weak biases into strong linguistic universals, undermining one of the arguments for strong innate constraints on language learning. As a consequence, the strength of innate biases can be shielded from natural selection, allowing these genes to drift. Furthermore, even when there is no natural selection, cultural transmission can produce apparent adaptations. Cultural transmission thus provides an alternative to traditional nativist and adaptationist explanations for the properties of human languages.}}

@article{Kgebck2018DeepColorRL,
  title={DeepColor: Reinforcement Learning optimizes information efficiency and well-formedness in color name partitioning},
  author={Mikael K{\aa}geb{\"a}ck and Devdatt P. Dubhashi and Asad B. Sayeed},
  journal={Cognitive Science},
  year={2018}
}

@article{Chaabouni2021CommunicatingAN,
  title={Communicating artificial neural networks develop efficient color-naming systems},
  author={Rahma Chaabouni and Eugene Kharitonov and Emmanuel Dupoux and Marco Baroni},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={2021},
  volume={118}
}

@article{Zipf1949HumanBA,

title = {Human behavior and the principle of least effort. Cambridge, (Mass.): Addison-Wesley, 1949, pp. 573},
journal = {Journal of Clinical Psychology},
author = {Zipf, George K.},
volume = {6},
number = {3},
pages = {306-306},
doi = {https://doi.org/10.1002/1097-4679(195007)6:3<306::AID-JCLP2270060331>3.0.CO;2-7},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1097-4679%28195007%296%3A3%3C306%3A%3AAID-JCLP2270060331%3E3.0.CO%3B2-7},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/1097-4679%28195007%296%3A3%3C306%3A%3AAID-JCLP2270060331%3E3.0.CO%3B2-7},
year = {1950}
}


@misc{openai2023gpt4,
      title={GPT-4 Technical Report},
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      note={arXiv:2303.08774},
      url={https://arxiv.org/abs/2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@phdthesis{naikthesis,
  title = {Adapting to the Long Tail in Language Understanding},
  school = {Carnegie Mellon University},
  author = {Aakanksha Naik},
  year = {2022},
  url = {https://www.lti.cs.cmu.edu/sites/default/files/naik%2C%20aakanksha%20-%20Thesis_1.pdf},
}

@comment{Lewis1970ConventionAP,
	address = {Cambridge, MA, USA},
	author = {David Kellogg Lewis},
	editor = {},
	publisher = {Wiley-Blackwell},
	title = {Convention: A Philosophical Study},
	year = {1969}
}

@article{Misra2021DoLM,
  title={Do language models learn typicality judgments from text?},
  author={Kanishka Misra and Allyson Ettinger and Julia Taylor Rayz},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.02987}
}

@article {Schrimpf2020ArtificialNN,
	author = {Martin Schrimpf and Idan Blank and Greta Tuckute and Carina Kauf and Eghbal A. Hosseini and Nancy Kanwisher and Joshua Tenenbaum and Evelina Fedorenko},
	title = {Artificial Neural Networks Accurately Predict Language Processing in the Brain},
	elocation-id = {2020.06.26.174482},
	year = {2020},
	doi = {10.1101/2020.06.26.174482},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {The ability to share ideas through language is our species{\textquoteright} signature cognitive skill, but how this feat is achieved by the brain remains unknown. Inspired by the success of artificial neural networks (ANNs) in explaining neural responses in perceptual tasks (Kell et al., 2018; Khaligh-Razavi \&amp; Kriegeskorte, 2014; Schrimpf et al., 2018; Yamins et al., 2014; Zhuang et al., 2017), we here investigated whether state-of-the-art ANN language models (e.g. Devlin et al., 2018; Pennington et al., 2014; Radford et al., 2019) capture human brain activity elicited during language comprehension. We tested 43 language models spanning major current model classes on three neural datasets (including neuroimaging and intracranial recordings) and found that the most powerful generative transformer models (Radford et al., 2019) accurately predict neural responses, in some cases achieving near-perfect predictivity relative to the noise ceiling. In contrast, simpler word-based embedding models (e.g. Pennington et al., 2014) only poorly predict neural responses (\&lt;10\% predictivity). Models{\textquoteright} predictivities are consistent across neural datasets, and also correlate with their success on a next-word-prediction task (but not other language tasks) and ability to explain human comprehension difficulty in an independent behavioral dataset. Intriguingly, model architecture alone drives a large portion of brain predictivity, with each model{\textquoteright}s untrained score predictive of its trained score. These results support the hypothesis that a drive to predict future inputs may shape human language processing, and perhaps the way knowledge of language is learned and organized in the brain. In addition, the finding of strong correspondences between ANNs and human representations opens the door to using the growing suite of tools for neural network interpretation to test hypotheses about the human mind.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2020/06/27/2020.06.26.174482},
	eprint = {https://www.biorxiv.org/content/early/2020/06/27/2020.06.26.174482.full.pdf},
	journal = {bioRxiv}
}

@article{mahowald2023dissociating,
  title={Dissociating language and thought in large language models: a cognitive perspective},
  author={Mahowald, Kyle and Ivanova, Anna A and Blank, Idan A and Kanwisher, Nancy and Tenenbaum, Joshua B and Fedorenko, Evelina},
  journal={arXiv preprint arXiv:2301.06627},
  year={2023}
}

@comment{artetxe-etal-2020-cross,
    title = "On the Cross-lingual Transferability of Monolingual Representations",
    author = "Artetxe, Mikel  and
      Ruder, Sebastian  and
      Yogatama, Dani",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.421",
    doi = "10.18653/v1/2020.acl-main.421",
    pages = "4623--4637",
    abstract = "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
}

@book{pollard1994head,
  title={Head-driven phrase structure grammar},
  author={Pollard, Carl and Sag, Ivan A},
  year={1994},
  publisher={University of Chicago Press}
}

@Inbook{Lindblom1990,
author="Lindblom, B.",
editor="Hardcastle, William J.
and Marchal, Alain",
title="Explaining Phonetic Variation: A Sketch of the H{\&}H Theory",
bookTitle="Speech Production and Speech Modelling",
year="1990",
publisher="Springer Netherlands",
address="Dordrecht",
pages="403--439",
abstract="The H{\&}H theory is developed from evidence showing that speaking and listening are shaped by biologically general processes. Speech production is adaptive. Speakers can, and typically do, tune their performance according to communicative and situational demands, controlling the interplay between production-oriented factors on the one hand, and output-oriented constraints on the other. For the ideal speaker, H{\&}H claims that such adaptations reflect his tacit awareness of the listener's access to sources of information independent of the signal and his judgement of the short-term demands for explicit signal information. Hence speakers are expected to vary their output along a continuum of hyper- and hypospeech. The theory suggests that the lack of invariance that speech signals commonly exhibit (Perkell and Klatt 1986) is a direct consequence of this adaptive organization (cf MacNeilage 1970). Accordingly, in the H{\&}H program the quest for phonetic invariance is replaced by another research task: Explicating the notion of sufficient discriminability and defining the class of speech signals that meet that criterion.",
isbn="978-94-009-2037-8",
doi="10.1007/978-94-009-2037-8_16",
url="https://doi.org/10.1007/978-94-009-2037-8_16"
}

@article{ullman_2001, title={The neural basis of lexicon and grammar in first and second language: the declarative/procedural model}, volume={4}, DOI={10.1017/S1366728901000220}, number={2}, journal={Bilingualism: Language and Cognition}, publisher={Cambridge University Press}, author={Ullman, Michael T.}, year={2001}, pages={105–122}}

@article{Ullman2001ANP,
  title={A neurocognitive perspective on language: The declarative/procedural model},
  author={Michael T. Ullman},
  journal={Nature Reviews Neuroscience},
  year={2001},
  volume={2},
  pages={717-726}
}

@misc{qiu2023pragmatic,
 title={Pragmatic Implicature Processing in ChatGPT},
 url={osf.io/preprints/psyarxiv/qtbh9},
 DOI={10.31234/osf.io/qtbh9},
 publisher={PsyArXiv},
 author={Qiu, Zhuang and Duan, Xufeng and Cai, Zhenguang G},
 year={2023},
 month={May}
}

@misc{guo2023close,
    title={How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection},
    author={Biyang Guo and Xin Zhang and Ziyuan Wang and Minqi Jiang and Jinran Nie and Yuxuan Ding and Jianwei Yue and Yupeng Wu},
    year={2023},
    eprint={2301.07597},
    url={https://arxiv.org/abs/2301.07597},
    note={arXiv:2301.07597},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{instructgpt,
    title={Training language models to follow instructions with human feedback},
    author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
    year={2022},
    eprint={2203.02155},
    url={https://arxiv.org/abs/2203.02155},
    note={arXiv:2203.02155},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{warstadt2022artificial,
  title={What artificial neural networks can tell us about human language acquisition},
  author={Warstadt, Alex and Bowman, Samuel R},
  journal={Algebraic Structures in Natural Language},
  pages={17--60},
  year={2022},
  publisher={CRC Press}
}

@article{chang2022word,
    author = {Chang, Tyler A. and Bergen, Benjamin K.},
    title = "{Word Acquisition in Neural Language Models}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {1-16},
    year = {2022},
    month = {01},
    abstract = "{We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words’ ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00444},
    url = {https://doi.org/10.1162/tacl\_a\_00444},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00444/1986589/tacl\_a\_00444.pdf},
}

@misc{warstadt2020neural,
      title={Can neural networks acquire a structural bias from raw linguistic data?},
      author={Alex Warstadt and Samuel R. Bowman},
      year={2020},
      eprint={2007.06761},
      note={arXiv:2007.06761},
      url={https://arxiv.org/abs/2007.06761},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
bosc2022varying,
title={Varying meaning complexity to explain and measure compositionality},
author={Tom Bosc},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=BnGzfmZ07bq}
}

@inproceedings{
steinert-threlkeld2022emergent,
title={Emergent Communication Fine-tuning ({EC}-{FT}) for Pretrained Language Models},
author={Shane Steinert-Threlkeld and Xuhui Zhou and Zeyu Liu and C. M. Downey},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=SUqrM7WR7W5}
}

@inproceedings{
kalinowska2022situated,
title={Situated Communication: A Solution to Over-communication between Artificial Agents},
author={Aleksandra Kalinowska and Elnaz Davoodi and Florian Strub and Kory Mathewson and Todd Murphey and Patrick Pilarski},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=HLqzzQWA7Z9}
}

@comment{
ueda2022categorial,
title={Categorial Grammar Induction as a Compositionality Measure for Emergent Languages in Signaling Games},
author={Ryo Ueda and Taiga Ishii and Koki Washio and Yusuke Miyao},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=Sbgb7b0Q-5}
}

@inproceedings{
raviv2022what,
title={What makes a language easy to learn? A preregistered study on how systematic structure and community size affect language learnability},
author={Limor Raviv and Marianne de Heer Kloots and Antje Meyer},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=BdbexQ-0XW9}
}

@inproceedings{
ossenkopf2020comaze,
title={CoMaze: A cooperative game for zero-shot coordination},
author={Marie Ossenkopf},
booktitle={Emergent Communication Workshop at NeurIPS 2020},
year={2020},
url={https://drive.google.com/file/d/1GnPBV3Q-w65TZSaLD4YASt6nIuTZq7bM/view}
}

@inproceedings{
ossenkopf2019enhancing,
title={Enhancing Communication Learning through Empathic Prediction},
author={Marie Ossenkopf},
booktitle={Emergent Communication Workshop at NeurIPS 2019},
year={2019},
url={https://drive.google.com/file/d/19VWGANQUBWBBqEgXi1zbRgQ9uv0zh8jJ/view}
}

@inproceedings{
fitzgerald2019populate,
title={To Populate Is To Regulate},
author={Nicole Fitzgerald},
booktitle={Emergent Communication Workshop at NeurIPS 2019},
year={2019},
url={https://drive.google.com/file/d/1mINOlJvyWxGtqW_qNadMEhWNNJ4ZPIIa/view}
}

@misc{leni2018seq2seq,
      title={Seq2Seq Mimic Games: A Signaling Perspective}, 
      author={Juan Leni and John Levine and John Quigley},
      year={2018},
      eprint={1811.06564},
      url={https://arxiv.org/abs/1811.06564},
      note={arXiv:1811.06564},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{Foerster2016LearningTC,
  title={Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
  author={Jakob N. Foerster and Yannis Assael and Nando de Freitas and Shimon Whiteson},
  journal={ArXiv},
  year={2016},
  volume={abs/1605.06676}
}

@inproceedings{
lipinski2022emergent,
title={Emergent Password Signalling in the Game of Werewolf},
author={Olaf Lipinski and Adam Sobey and Federico Cerutti and Timothy J Norman},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=B4xM-Qb0mbq}
}

@inproceedings{lu2020sil,
author = {Lu, Yuchen and Singhal, Soumye and Strub, Florian and Pietquin, Olivier and Courville, Aaron},
title = {Countering Language Drift with Seeded Iterated Learning},
year = {2020},
publisher = {JMLR.org},
abstract = {Pretraining on human corpus and then finetuning in a simulator has become a standard pipeline for training a goal-oriented dialogue agent. Nevertheless, as soon as the agents are finetuned to maximize task completion, they suffer from the so-called language drift phenomenon: they slowly lose syntactic and semantic properties of language as they only focus on solving the task. In this paper, we propose a generic approach to counter language drift called Seeded iterated learning (SIL). We periodically refine a pretrained student agent by imitating data sampled from a newly generated teacher agent. At each time step, the teacher is created by copying the student agent, before being finetuned to maximize task completion. SIL does not require external syntactic constraint nor semantic knowledge, making it a valuable taskagnostic finetuning protocol. We evaluate SIL in a toy-setting Lewis Game, and then scale it up to the translation game with natural language. In both settings, SIL helps counter language drift as well as it improves the task completion compared to baselines.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {597},
numpages = {11},
series = {ICML'20}
}

@inproceedings{cogswell2020dialog,
 author = {Cogswell, Michael and Lu, Jiasen and Jain, Rishabh and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {19988--19999},
 publisher = {Curran Associates, Inc.},
 title = {Dialog without Dialog Data: Learning Visual Dialog Agents from VQA Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/e7023ba77a45f7e84c5ee8a28dd63585-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{lee-etal-2019-countering,
    title = "Countering Language Drift via Visual Grounding",
    author = "Lee, Jason  and
      Cho, Kyunghyun  and
      Kiela, Douwe",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1447",
    doi = "10.18653/v1/D19-1447",
    pages = "4385--4395",
    abstract = "Emergent multi-agent communication protocols are very different from natural language and not easily interpretable by humans. We find that agents that were initially pretrained to produce natural language can also experience detrimental language drift: when a non-linguistic reward is used in a goal-based task, e.g. some scalar success metric, the communication protocol may easily and radically diverge from natural language. We recast translation as a multi-agent communication game and examine auxiliary training constraints for their effectiveness in mitigating language drift. We show that a combination of syntactic (language model likelihood) and semantic (visual grounding) constraints gives the best communication performance, allowing pre-trained agents to retain English syntax while learning to accurately convey the intended meaning.",
}

@comment{Papadimitriou2020LearningMH,
  title={Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models},
  author={Isabel Papadimitriou and Dan Jurafsky},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:221891676}
}


@InProceedings{lake2018generalization,
  title = 	 {Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks},
  author =       {Lake, Brenden and Baroni, Marco},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2873--2882},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/lake18a/lake18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/lake18a.html},
  abstract = 	 {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks’ notorious training data thirst.}
}

@misc{zhang2022unveiling,
    title={Unveiling Transformers with LEGO: a synthetic reasoning task},
    author={Yi Zhang and Arturs Backurs and Sébastien Bubeck and Ronen Eldan and Suriya Gunasekar and Tal Wagner},
    year={2022},
    eprint={2206.04301},
    note={arXiv:2206.04301},
    url={https://arxiv.org/abs/2206.04301},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{mirzaee-kordjamshidi-2022-transfer,
    title = "Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning",
    author = "Mirzaee, Roshanak  and
      Kordjamshidi, Parisa",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.413",
    doi = "10.18653/v1/2022.emnlp-main.413",
    pages = "6148--6165",
    abstract = "Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is less explored for spatial language. We provide two new data resources on multiple spatial language processing tasks. The first dataset is synthesized for transfer learning on spatial question answering (SQA) and spatial role labeling (SpRL). Compared to previous SQA datasets, we include a larger variety of spatial relation types and spatial expressions. Our data generation process is easily extendable with new spatial expression lexicons. The second one is a real-world SQA dataset with human-generated questions built on an existing corpus with SPRL annotations. This dataset can be used to evaluate spatial language processing models in realistic situations. We show pretraining with automatically generated data significantly improves the SOTA results on several SQA and SPRL benchmarks, particularly when the training data in the target domain is small.",
}

@article{bar2002general,
  title={General features of complex systems},
  author={Bar-Yam, Yaneer},
  journal={Encyclopedia of Life Support Systems (EOLSS), UNESCO, EOLSS Publishers, Oxford, UK},
  volume={1},
  year={2002}
}

@ARTICLE{li2023metadrive,
  author={Li, Quanyi and Peng, Zhenghao and Feng, Lan and Zhang, Qihang and Xue, Zhenghai and Zhou, Bolei},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning}, 

  year={2023},

  volume={45},

  number={3},

  pages={3461-3475},

  doi={10.1109/TPAMI.2022.3190471}}

@comment{
maddison2017the,
title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
author={Chris J. Maddison and Andriy Mnih and Yee Whye Teh},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=S1jE5L5gl}
}

@Article{bracci2023representational,
   Author="Bracci, S.  and Mraz, J.  and Zeman, A.  and Leys, G.  and Op de Beeck, H. ",
   Title="{{T}he representational hierarchy in human and artificial visual systems in the presence of object-scene regularities}",
   Journal="PLoS Comput Biol",
   Year="2023",
   Volume="19",
   Number="4",
   Pages="e1011086",
   Month="Apr"
}

@article{olah2017feature,
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  title = {Feature Visualization},
  journal = {Distill},
  year = {2017},
  note = {https://distill.pub/2017/feature-visualization},
  doi = {10.23915/distill.00007}
}

@misc{bbc-alphago,
  title        = "Artificial intelligence: Go master Lee Se-dol wins against AlphaGo program",
  author       = "{BBC}",
  howpublished = "\url{https://www.bbc.com/news/technology-35797102}",
  year         = 2016,
  note         = "Accessed: 2026-02-07"
}

@article{li2018rlhistory,
      title={Deep Reinforcement Learning: An Overview}, 
      author={Yuxi Li},
      year={2018},
      eprint={1701.07274},
      volume={1701.07274},
      archivePrefix={arXiv},
      journal={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1701.07274}, 
}

@misc{deepblue,
  title        = "Deep Blue",
  author       = "{IBM}",
  howpublished = "\url{https://www.ibm.com/history/deep-blue}",
  note         = "Accessed: 2026-02-07"
}

@article{kirby2014iterated,
title = {Iterated learning and the evolution of language},
journal = {Current Opinion in Neurobiology},
volume = {28},
pages = {108-114},
year = {2014},
note = {SI: Communication and language},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2014.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0959438814001421},
author = {Simon Kirby and Tom Griffiths and Kenny Smith},
abstract = {Iterated learning describes the process whereby an individual learns their behaviour by exposure to another individual's behaviour, who themselves learnt it in the same way. It can be seen as a key mechanism of cultural evolution. We review various methods for understanding how behaviour is shaped by the iterated learning process: computational agent-based simulations; mathematical modelling; and laboratory experiments in humans and non-human animals. We show how this framework has been used to explain the origins of structure in language, and argue that cultural evolution must be considered alongside biological evolution in explanations of language origins.}
}
