\section{Discussion}%
\label{sec:pb-discussion}

\subsection{Communicating in emergent language}
We observe (in \cref{fig:performance}) that the online--online parings perform near $100\%$ accuracy on training and test data with few outliers.
Both the offline sender and offline receiver perform almost as well, though with a marked decrease on the test data; this shows that most---though possibly not all---of in the information necessary for generalizing to the test data is is available in the annotated corpora alone (cf.\@ the online agents ``overfitting'' to each other).

The phrasebook-based agents, while underperforming the neural network-based agents, perform quite well with median scores of $100\%$ on the test set and seldom falling below $80\%$.
This, in turn, confirms the primary premise in question in this paper: it is possible to communicate in emergent language with phrasebook agents derived from an induced morphological inventory.
Nevertheless, it is very easy to distinguish between the messages generated by the neural sender compared to the phrasebook sender (\cref{app:qual}); the former is more varied in both its morpheme selection and its order compared to the rigid message structure of the phrasebook sender.

\subsection{Robustness of morpheme induction and phrasebook agents}
Varying the hyperparameters of the emergent language environment and the online agents allows us to get a fuller picture of how the morpheme induction, sender, and receiver algorithms perform (\cref{fig:env-ab,fig:env-ab1,fig:env-ab2}).
We categorize the results by the online and phrasebook agent performances:

\paragraph{High online, high phrasebook}
Both the online agents and the phrasebook agents perform well (${>}80\%$)\footnote{Parenthesized numbers are median accuracies.}, suggesting that generalization is achieved by the online agents employing a method interpretable to both sender and receiver phrasebook agents.
This includes the majority of the settings we tested in our ablations suggesting a fair degree of robustness to variations in the hyperparameters of the environment.

\paragraph{Low online, low phrasebook}
Both the online agent and the phrasebook agents struggle to generalize well (even after the online agents converge on the training set).
These settings include $n_\text{attribute}=2$ ($0\%$)  and $n_\text{value}=2$ ($0\%$) and, to a lesser degree, a test proportion of $75\%$ ($[30\%,45\%]$).
The results on these settings are expected insofar as the online agents serve as an upper bound for phrasebook agent performance, that is, the phrasebook agents cannot somehow generalize beyond what possible in the original language.

\paragraph{High online, low phrasebook}
The online agents maintain a high test performance while the phrasebook agents significantly underperform the online agents;
  the most notable examples of this are the non-GRU RNN cell type ($[90\%,100\%]$ vs.\@ $[15\%,35\%]$), cross-entropy loss ($95\%$ vs.\@ $[5\%,15\%]$), and a vocabulary size of $4$ ($100\%$ vs.\@ $5\%$).
These setting highlight the fact that the online agents are capable of generalizing to unseen observation in a way that is largely opaque to the morpheme induction algorithm and/or the phrasebook algorithms, that is, in a way that violates the assumption concatenative compositionality.
For example, in the case of a small vocabulary, size it would not be possible to create a one-to-one mapping between specific types in the vocabulary and specific values for specific attributes.
Two alternatives for conveying meaning include:
  (1) double articulation where groups of meaningless form tokens comprise meaningful ``words''
  and (2) the semantics of individual types exist in an entangled, non-linear embedding space which neural networks can learn but CSAR cannot.
Although further investigation would be required to determine if either of these two cases hold.
% \sv{%
% Re: ``but CSAR is designed \dots'' in the comment:
% I'm not sure if 100\% follow the details here but if I understand correctly, you're saying here that CSAR is designed to handle mapping between complex forms and complex meanings (beyond int-int pairs), not being able to induce a 1-1 mapping between types and attributes shouldn't rule out solutions.
% I think this is important to address, yeah.
% There's two aspects here: (1) Does a phrasebook exist for this language? (2) Is the phrasebook discoverable by CSAR?  I think this investigation conflates these two, in the sense that there is an implicit assumption that any phrasebook can be discovered by CSAR.
% I don't any objection to that argument (I would expect the phrasebook CSAR doesn't find to be the really complex explanations of the language), but this might be a good place to discuss more.
% I think this high online/low phrasebook setting is possibly most informative of the nature of emergent language? That is, the things about the emergent language that are not capturable in a phrasebook are what make it robust/efficient maybe? I'm thinking of this in analogy to natural language where non-compositionality is a central theme of what's uniquely human about language like context sensitivity, idiomatic use, etc.
% Personally the difference between XE loss and MSE loss is fascinating. We train LMs with XE loss and in this setting it results in qualitatively different behavior. If there is more to say about it, I think it would be a great inclusion.
% }

\subsection{Morphosyntax in the ELs}%
\label{sec:morphosyntax}
Looking at the performance variations in response to changing parameters of the induction and phrasebook agents, we can infer various properties about the morphology and syntax of the emergent languages in question.
With respect to the morpheme induction, we see two subtle patterns: (1) performance goes slightly up when increasing the max meaning size from $1$ to $2$ (\cref{fig:ag-ab1}) and (2) performance goes slightly \emph{down} with max form length increasing above $1$ (\cref{fig:ag-ab1}).
The former pattern suggests that the emergent languages show some small degree of fusionality/cumulative exponence, while the latter indicates that considering multi-token forms is not relevant to the morphology of the emergent languages.

Turning our attention to the variations in the sender algorithm, we see that one of the most important factors in high performance is permitting the repetition of morphemes.
When the algorithm employs just a single morpheme per meaning atom, the performance is notably worse (with a median accuracy of $60\%$ instead of $100\%$; \cref{fig:ag-ab2}).
Similarly, when the sender algorithm does not ablate meanings and uses subsequent lower-weighed morphemes for a given meaning atom in addition to the higher weighed meanings (i.g., meanings are not ablated), performance drops (\cref{fig:ag-ab2}).
These observations suggest that repetition of highly-weighted morphemes is a meaningful morphosyntactic feature of the emergent languages studied.

In addition to the importance of repetition, we also found the order of morphemes had a notable impact on the sender agent's performance, with performance decreasing when we perturb morphemes from their preferred place in the message (i.e., the \emph{affinity} method; \cref{fig:ag-ab2}).
This indicates a syntactic rule (albeit a simple one) that governs how morphemes should be combined.
Finally, the variations on the receiver algorithm did not have large impacts on the phrasebook agent performance (\cref{fig:ag-ab2}).
% \cmt{Do I want to say something about the fact that repeating morphemes is important when speaking (i.e., when the neural network sis receiving) whereas it's not used to make distinctions from the neural speaker's side?  I think it would be good, but it'd be necessary to double check how the variations are explained so that the connections are clear.}

\subsection{Compositionality in emergent language}

Compositionality is one of the most talked about topics in emergent language research, and the \emph{in situ} experiments with phrasebook agents provide a uniquely in-depth investigation of this phenomenon.
Namely, the morpheme induction algorithm and phrasebook agents are built on the following assumption: the meanings forming the observation correspond to substrings of the message in a disentangled way, such that the substrings can otherwise be recombined independently of each other while retaining their original meaning.
This aligns closely with the typical definition of compositionality in emergent language, namely that ``[t]he meaning of a compound expression is a function of its parts and of the way they are syntactically combine'' \citep{partee1984compositionality}, where syntactic combination is operationalized as concatenation of the forms in emergent language.

Essentially, the phrasebook agents should perform well if and only if
  (1) the emergent language is concatenatively compositional,
  (2) the morpheme induction algorithm actually induces meaningful morphemes,
  and (3) the sender and receiver algorithms sufficiently capture the syntax of the emergent language.
Insofar as we assume (2) and (3) to be true, we can use the performance of the phrasebook agents as a proxy for compositionality.
This notion of compositionality is deeper than more familiar metrics like topographic similarity because it not only takes into account the externally visible correlations between observations and messages but also tests these correlations with the original emergent language speakers and hearers (i.e., the online agents).
We could, in a way, see these experiments being closer to ``field linguistics'' compared to the ``corpus linguistics'' approach of toposim.
Continuing the analogy: field linguistics is more resource intensive than corpus linguistics, and likewise for testing phrasebook agents \emph{in situ} compared to computing toposim.

Yet, we find that morpheme bijectivity addresses the shortcomings of both measures of compositionality:
First, it is more predictive of phrasebook agent performance than toposim (\cref{fig:toposim}; $R^2=0.86$ vs.\@ $R^2=0.57$).
\unskip\footnote{Bag-of-symbols underperforms both of these as a predictor with $R^2=0.29$ (\cref{fig:acc-scatter-all}).}
And second, it can be computed with only the annotated corpus, not needing access to the original emergent language environment.


\paragraph{Compositionality without generalization}%
\label{sec:pb-comp-res}
An ancillary finding of the experiments across different environments is that they provide further evidence for the notion that neural network agents do not need to use concatenative compositionality to in order to generalize \citep{kharitonov-baroni-2020-emergent}.
Indeed, in the majority of our settings, we see that the performance of the online and phrasebook agents are relatively matched, suggesting that compositionality corresponds with generalization performance.
But in certain settings (e.g., cross-entropy loss, Elman and LSTM RNN cells, and a small vocabulary size of $4$; \cref{fig:env-ab1,fig:env-ab2}), there is a clear bifurcation: the online agents generalize well while the phrasebook agents do not (and even have trouble fitting the training data).
This shows that the neural network agents are capable of compositional as well as non-compositional generalization and may favor one over the other for non-obvious reasons (cf. GRUs vs.\@ LSTMs).
Furthermore, the lower correlation of toposim with phrasebook agent performance suggests that it might not be the best metric for detecting such instances of non-compositionality in comparison to the higher predictive power of morpheme bijectivity derived from CSAR\@.

% \cmt{Related work on compositionality and compression \url{https://openreview.net/forum?id=fXCfT7ErvL}.}
% \cmt{More related work on Compression and Kolmogorov complexity \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/be7430d22a4dae8516894e32f2fcc6db-Paper-Conference.pdf}}


