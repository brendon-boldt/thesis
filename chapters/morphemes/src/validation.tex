\section{Empirical Validation}%
\label{sec:val}
To validate the ability of CSAR to find well-founded morpheme inventories, we test it against procedurally generated datasets as well as human languages.
Since we do not have access to ground truth morphemes for emergent languages, we gauge the effectiveness of CSAR's morpheme induction in the next best way: by testing its performance in these adjacent domains.
Procedurally generated datasets (described in \cref{sec:data-gen}) both give us access to the ``ground truth'' morphemes and allow us to vary particular facets of the languages.
Having ground truth morphemes allows us to quantitatively evaluate CSAR against baseline methods (\cref{sec:baselines}).
Fine-grained control over the facets of the languages permits us to identify particular phenomena that are challenging for CSAR to induce correctly (\cref{sec:error-ana}).
We also test CSAR against human language data (\cref{sec:human-lang}) in order to give a qualitative sense of the effectiveness of the algorithm.


\subsection{Procedural datasets}
\unskip\label{sec:data-gen}

The dataset-generating procedure has the following basic structure:
  (1) Meanings are sampled according to some structure (viz.\@ a fixed attribute--value vector).
  (2) An utterance is produced from this meaning according to a mapping of meaning components to form components.
  (3) The form--meaning pairs that were used to generate the utterance are added to the set of ground truth morphemes.
In the basic case, for example, each particular attribute and value is associated with a unique sequence of tokens which are concatenated to form an utterance, creating a one-to-one mapping from meanings to forms.

\paragraph{Variations}
Such languages are trivial to induce morphemes from, so we introduce the following variations to produce more complex datasets:
\smallskip
\begin{description}[nosep,itemindent=-1em]
  \item[Synonymy] Multiple forms may correspond to the same meaning.
  \item[Polysemy] Multiple meanings may correspond to the same form.
  \item[Multi-token forms] A form may comprise more than one token, possibly overlapping with other forms.
  \item[Vocab size] Number of unique tokens.
  \item[Sparse meanings] Meanings occur independently of each other with no additional structure (i.e., not structured as attribute--value pairs).
  \item[Distribution imbalance] Meanings are sampled from non-uniform distributions.
  \item[Dataset size] Number of records in the dataset.
  \item[Number of meanings] Total number of meanings (e.g., varying number of attributes and values).
  \item[Noise forms] Form tokens not corresponding to any meanings are added.
  \item[Shuffle form] Inter-form order is varied randomly (while maintaining intra-form order).
  \item[Non-compositionality] A given form may correspond to multiple meanings simultaneously.
\end{description}
\medskip
For the following analyses, we report values for a collection of procedural datasets built from the Cartesian product of two values for each of the above variations (one where the variation is inactive and one where it is).
See \cref{app:synth-hparams} for details.


\paragraph{Evaluation metric}
% For many of the test cases, CSAR is not able to recover the exact morpheme inventory but is able to get close to the ground truth.
We use $F_1$ score (harmonic mean of precision and recall) to assess the quality of an induced morpheme inventory given the ground truth inventory.
We define precision as
\begin{equation}
  \frac1{|\mathcal I|} \sum_{i \in \mathcal I} \max_{g\in\mathcal G} s(i, g) ,
  \label{eq:precision}
\end{equation}
where
  $\mathcal I$ is the set of induced morphemes,
  $\mathcal G$ is the set of ground truth morphemes,
  and
  $s$ is the similarity function
  \unskip.
For exact $F_1$, the similarity function is $1$ if the morphemes are identical and $0$ otherwise.
In fuzzy $F_1$, the similarity function is the minimum of form similarity (normalized insertion--deletion ratio\footnote{$1 - (\text{insertions} + \text{deletions}) / (|s_1|+|s_2|)$}) and meaning similarity (Jaccard index).
Recall is defined similarly to precision except that the roles of $\mathcal{I}$ and $\mathcal{G}$ from \cref{eq:precision} are reversed.

% \paragraph{Results}
% Across all synthetic datasets, CSAR achieves $F_\text{exact}=0.72$ and $F_\text{fuzzy}=0.88$.
% In comparison, the naive baseline of simply using set of raw observations as the set of morphemes yields $F_\text{exact}=0.10$ and $F_\text{fuzzy}=0.55$.
% \cmt{How do we put these in perspective?}


\subsection{Comparison with baselines}
\unskip\label{sec:baselines}

\begin{figure}
  \centering
  \input{chapters/morphemes/assets/baselines-fuzzy.pgf}
  \caption{Fuzzy $F_1$ scores for CSAR and baseline methods across procedural datasets.
    Results reported for form--meaning inventories and form-only inventories.}
  \unskip\label{fig:baseline}
\end{figure}

Below we describe the baseline methods we use for comparison.
\smallskip
\begin{description}[nosep,itemindent=-1em]
  \item[IBM Model 1]
    Simple expectation-maximization approach to machine translation primarily through aligning words in a sentence-parallel corpus. \citep{ibm-model-1}
  \item[IBM Model 3]
    Built on top of the IBM Model 1 to handle phenomena such as allowing a form to align to no meaning.
  \item[Morfessor]
    A form-only segmentation algorithm built to handle human language; also uses an EM algorithm.
  \item[Byte pair encoding]
    A greedy form-only tokenization method which recursively merges frequently occurring pairs of tokens.
    Vocabulary size is selected according to a simple heuristic (see \Cref{app:vocab-size}).
    \citep{gage1994bpe,sennrich-etal-2016-neural}
  \item[Unigram LM]
    An EM-based form-only tokenization method which starts with a large vocabulary and iteratively removes tokens contributing least to the likelihood of the data.
    Vocabulary size is selected according to a simple heuristic (see \Cref{app:vocab-size}).
    \citep{kudo-2018-subword}
  \item[Record]
    A trivial baseline where the inventory is just the set of all records.
\end{description}
\medskip
For the baseline methods which do not handle meanings and only produce forms, we report the form-only $F_1$ score (i.e., $s(i,g)$ only takes the form into account), though CSAR and IBM models still have access to meanings.
For form-only metrics, we exclude datasets which include noise forms as form-only methods cannot identify which forms are noise.

\paragraph{Results}
The results of CSAR and the baselines on the procedural datasets are presented in \cref{fig:baseline},
  which shows the distributions of mean scores for each hyperparameter setting for the procedural datasets. Each setting was repeated over $3$ random seeds. Additional results are given in \cref{app:proc-table}.
For inducing full morphemes (form and meaning), CSAR performs the best by a large margin over the baselines (and even greater margin when considering exact $F_1$).
The IBM alignment models perform better than the trivial record-based baseline but still perform noticeably worse than CSAR\@.
While CSAR yields roughly equal precision and recall, the IBM models' precision is lower than their recall suggesting that they are more prone to inducing spurious morphemes than CSAR\@.

When evaluating the forms only, we find that CSAR is the best method with Morfessor exhibiting comparable performance.
The IBM alignment models exhibit roughly the same performance as the tokenization methods (BPE and Unigram LM).
As with the full morpheme results, CSAR is the only method to achieve comparable precision and recall with all other baselines having precisions lower than their recalls.



\subsection{Error Analysis}
\unskip\label{sec:error-ana}
For the most part, the errors CSAR makes are ``edit errors'': identifying a correct morpheme but adding or removing a form or meaning token.
This is reflected in the near-parity between precision and recall.
This is in contrast to the baseline methods which are more prone to inducing too many morphemes, leading to lower precision.

Generally speaking, as more variations are added to a dataset, the performance degrades further.
In particular, CSAR's performance decreases the most with small corpus sizes, overlapping multi-token forms, and non-compositional mappings.
On the other hand, using sparse meanings, shuffling the forms, and using a non-uniform meaning distribution have relatively little effect.

\begin{figure}
\centering
\begin{tabular}{lp{10em}}
  \toprule
  Dataset & Induced Morpheme \\
  \midrule
  \multirow2*{Morphology}     & (``ed\$'', \{PAST\}) \\
                              & (``\,'\,'', \{POSSESSIVE\}) \\
  \midrule
  \multirow3*{Image captions} & (``stop sign'', \{stop sign\}) \\
                              & (``woman'', \{person\}) \\
                              & (``skier'', \{person, skis\}) \\
  \midrule
  \multirow2*{Translation}    & (``Member States'', \{Mitgliedstaaten\}) \\
  \bottomrule
\end{tabular}
\caption{Examples of morphemes induced from various human language datasets and tasks.}%
\label{fig:human}
\end{figure}


\subsection{Human language data}
\unskip\label{sec:human-lang}

In this section we discuss the results of running CSAR on three different human language datasets.
While these datasets are not the intended domain of CSAR---and CSAR is certainly not the best algorithm for the tasks---the point of these experiments is to demonstrate the general effectiveness of the algorithm qualitatively (examples shown in \cref{fig:human}).
Since these datasets are larger, we employed heuristic optimizations to CSAR to reduce their runtime (described in \cref{app:opt}).
The top $100$ induced morphemes for each human language dataset are given in \cref{app:human-language}.

\paragraph{Morpho Challenge}
The first human language dataset we use is from the Morpho Challenge \citep{kurimo-etal-2010-morpho}.
This dataset is a human language approximation of the task of morpheme induction for emergent language.
Concretely, the utterances are single English words, divided up at the character level, while the meanings are the constituent morphemes.
% Thus, most morphemes have form sizes greater than what need to be recognized in order to be induced correctly.

CSAR is able to recover a wide variety of morphemes including:
  roots like (``\^{}fire'', \{fire\}),
  prefixes like (``\^{}re'', \{re-\}),
  suffixes like (``ed\$'', \{PAST\}),
  and other affixes like (``\,'\,'', \{POSSESSIVE\}).
While the vast majority of morphemes CSAR induces are accurate, a handful of the lowest-weighted morphemes are spurious (e.g., (``s\$'', \{boy\})) likely due to inaccurate decoding earlier in the process (i.e., part of the true form for a given meaning was included in a prior meaning).


\paragraph{Image captions}
The next dataset we employ is the MS COCO dataset \citep[CC BY 4.0]{lin2015microsoftcococommonobjects}.
In particular, we take the image captions to be the utterances, treating words as atomic units, and the meaning to be the labeled objects in the image (e.g., person, cat).
% This dataset presents a wide variety of forms with a relatively small selection of meanings, making for an asymmetric mapping.

The bulk of highest weighted induced morphemes are direct equivalents of the objects they describe (e.g., (``cat'', \{cat\})).
% Further down, we begin to see closely associated words that are not direct equivalents such as (``court'', \{sports ball\}).
% and (``wii'', \{remote\})\footnotemark.  \footnotetext{As in the Nintendo Wii game console whose controller is often called a ``Wii Remote''.}
% We also find synonyms induced such as (``bicycle'', \{bicycle\}) and (``bike'', \{bicycle\}) while polysemic mappings we did not observe likely in part due to the small number of semantic categories in the dataset.
We find instances of synonymy (e.g., (``bicycle'', \{bicycle\}) and (``bike'', \{bicycle\})) as well as polysemy (e.g., (``animals'', \{cow\}) and (``animals'', \{sheep\})).
Finally, we also observe compound forms like (``stop sign'', \{stop sign\}) as well as compound meanings such as (``skier'', \{person, skis\}).
As we go beyond the top $100$ or so, the associations between forms and meanings remain reasonable but become looser such as (``bride'', \{dining table, tie\}) or (``sink'', \{toothbrush\}).

\paragraph{Machine translation}
For machine translation, we use the WMT16 dataset and the English--German split, in particular \citep{bojar-EtAl:2016:WMT1}.
In this case, the English text is considered to be the utterance and the German text to be the meaning, with words being the atomic units on both sides.
% While the image caption dataset was asymmetric with a wide variety of utterances corresponding to a handful of meanings, machine translation presents a more symmetric mapping, presenting a wide variety of utterances and meanings.

As with the image caption results, the bulk of induced morphemes are direct equivalents (e.g., (``and'', \{und\})).
Beyond these simple one-to-one mappings, CSAR induces the polysemic relationship (``the'', \{der\}) and (``the'', \{die\}).
Finally, CSAR also picks up on multi-token forms like (``Member States'', \{Mitgliedstaaten\}).
