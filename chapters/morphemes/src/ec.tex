\section{Analysis of Emergent Languages}%
\label{sec:el}

Here we apply CSAR to a handful of emergent language environments and observe various metrics which can be derived from the induced morpheme inventories.
Although we do not have ground-truth morphemes for these environments, we can still observe patterns in the induced morphemes which demonstrate the diagnostic power of CSAR\@.

\subsection{Environments}
The selected emergent language environments span three different observation spaces:
  one-hot vectors,
  synthetic images of shapes,
  and natural images of birds.
Within each of the environments, further signalling game variations are described below.


\paragraph{Vector observations}
In the vector observation signalling game (based on the EGG framework \citep{egg}) the agents directly observe one-hot vectors without having to further extract representations.
Specifically, we use two variants of the observation space:
  (1) the standard attribute--value setting where each of $4$ attributes can take on $4$ distinct values
  and (2) the ``sparse'' setting where there are $8$ binary attributes which are either present or absent.
During the training of the emergent language agents, the sparse variant is identical to the attribute-value variant (except for the number of attributes and values), but when the corpora are passed to CSAR, the attributes with a value of $0$ are filtered out, yielding variable length meanings.
  % and only attributes which are ``true'' are included in the meanings given to CSAR\@.
Hyperparameters for both environments are given in \cref{app:el-hparams}.

For both observation spaces, we also test two variations of the signalling game.
The first is the \emph{discrimination game}, where the receiver must answer a multiple-choice question where the correct observation is accompanied by multiple (incorrect) distractors.
The second is the \emph{reconstruction game}, where the receiver must recreate the original observation similar to the decoding segment of an autoencoder.

\paragraph{Image observations}
The second environment is introduced by \citet{mu2021general} and uses both synthetic and natural images for its observation space.
The synthetic images come from ShapeWorld, a tool for generating images of shapes with varying properties \citep{kuhnle2017shapeworldnewtest} while the natural images are from the CUB-200-2011 dataset documenting various kinds of birds \citep{WahCUB_200_2011}.
The meanings for these environments are discrete attributes describing the images (e.g., ``red'' for ShapeWorld or ``wing color: brown'' for CUB) provided with the datasets.
In addition to a more standard discrimination game (termed \emph{reference game} in this paper), \citet{mu2021general} also introduce the \emph{set-reference} and \emph{concept} variants of the discrimination game.
These variations increase the level of abstraction in the game in order to encourage more generalizable (and compositional) languages to emerge.

Specifically, the reference game functions like the discrimination game described above except that there are multiple target images and the sender sees both the target images and the distractors.
The receiver, in this case, must classify the each image as being target or distractor (i.e., multi-label instead of multi-class).
In the reference game, the target images are all identical while in the set-reference game, the target images are variations of the same object (e.g., a red triangle in different positions/rotations).
Finally, in the concept game, the target images comprise different objects sharing one or more concepts (e.g., triangular).

\subsection{Metrics}%
\label{sec:ec-quant}

We present a handful of quantitative metrics to act as summary statistics for the morpheme inventories induced from the above emergent languages.
\smallskip
\begin{description}[nosep,itemindent=-1em]
  \item[Inventory entropy] Entropy (in bits) of the morphemes according to their prevalence (probability of occurring out of all morpheme occurrences identified during induction).
    We use the inventory entropy to give a rough sense of the breadth of the morpheme inventory as CSAR can induce a large number of low-prevalence, low-quality morphemes (whereas entropy is more robust to this).
    A rough translation to size is $H^2\approx$ equivalent number of equally-probable morphemes.
  \item[Morpheme bijectivity]
    Weighted mean normalized pointwise mutual information (NPMI) of morphemes in the inventory corresponding to how ``one-to-one'' the morphemes identified are;
      higher bijectivity corresponds to higher quality morphemes/inventory.
    See \cref{sec:exp-comp} for the introduction of this metric.
  \item[Topographic similarity] Correlation (Spearman's $\rho$) between distances in the utterance space and complete meaning space \citep{brighton2006toposim,lazaridou2018EmergenceOL} of the corpus (i.e., not based on the morpheme inventory).
  \item[Synonymy] Entropy across forms for a given meaning; computed using prevalence of forms normalized within the particular meaning.
  \item[Polysemy] Entropy across meanings for a given form; as with synonymy, \emph{mutatis mutandis}.
  \item[Form size] Mean number of tokens in a form, weighted by morpheme prevalence.
  \item[Meaning size] Mean number of tokens in a meaning, weighted by morpheme prevalence.
\end{description}
\smallskip
With the exception of inventory size and toposim, the above metrics are weighted by \emph{prevalence} which is the proportion of records from which the morpheme was ablated.

\subsection{Results}

\begin{table}
  \small
  \centering
  \input{chapters/morphemes/assets/csar-analysis-table.tex}
  \caption{Metrics (described in \cref{sec:ec-quant}) across the morpheme inventories of various emergent languages (averaged across $10$ runs).}%
  \label{tab:ec-quant}
\end{table}

The morpheme inventory metrics are given in \cref{tab:ec-quant} where they averaged across $10$ runs of each environment; some examples of morphemes from the inventories are given in \cref{app:ec-inv}.

\paragraph{Inventory, form, and meaning size}
Inventory entropy along with form and meaning size give sense of the general ``shape'' of the morpheme inventories---how large they are as a whole and what their components look like.
With respect to inventory entropy, we find that generally, the image-observation environments results in higher entropy morphemes, which is to be expected with richer input source.
On the other end of the interval, the (Vector, Attribute--value, Reconstruction) environment shows the lowest almost matching the lower bound of $4$ bits.\footnote{Since this environment as $4$ attributes which can each taken on $4$ values, that would correspond to $4\cdot4=16$ unique, equiprobable morphemes (order-dependence notwithstanding), corresponding to $\log_216=4$ bits of entropy.}

While both form length and meaning size remain small, their is a notable presence of sizes greater than $1$.
For form length, this suggests that the assumption that individual form tokens can be treated as words is not well founded; instead, the smallest meaningful units can comprise multiple tokens.
For meanings, this indicates some degree of ``fusionality'' where the minimal unit of meaning corresponds to multiple atomic meanings at once and inseparably.

\paragraph{Inventory quality}
Insofar as morpheme bijectivity measures the quality of the morpheme inventories induced, we find that the quality across most environments is medium-to-low with only the reconstruction game with attribute--value vector observations averages above $0.5$.\footnote{See \cref{fig:toposim} for a sense of the relationship between morpheme bijectivity and empirical inventory quality.}
Generally speaking, the bijectivity values for the vector-based environments are higher than the those of the image-based environments, suggesting that the noiseless, pre-disentangled observations result in languages which are more easily captured by CSAR\@.
The reference games in the image-based environments show especially low bijectivity potentially, in part, because the communication can reference low-level observation features without significant pressure towards generality.

\paragraph{Synonymy and polysemy}
Synonymy and polysemy, taken together, naturally correlate with morpheme bijectivity as the former metrics test for unidirectional correlation (e.g., how informative is a form for various meanings) while NPMI (the basis for bijectivity) takes bidirectional correlation (i.e., do a form and meaning correlate with each other exclusively).
Empirically, the vector-based environments have more synonymy than polysemy while the image-based have this pattern reversed.
Our initial hypothesis would be that synonymy would exceed polysemy insofar as polysemy incurs a cost with respect to communicative efficacy (i.e., ambiguity) while synonymy does not.
That being said, the fuzzier relationship between the observations and the semantics in image environments (since it is a lossy conversion) compared to the one-to-one relationship between meanings and observations in the vector-based environments may explain part of the unexpected behavior.



\paragraph{Compositionality}
We find that morpheme bijectivity and toposim are relatively well correlated in our quantitative analysis, as is expected since they both aim to be measures of compositionality.
On the other hand, we do not find any notable correlation between bijectivity/toposim and meaning size which could also be viewed a measure of compositionality; viz.\@ fewer meaning components per morpheme correspond with a more one-to-one relationship between forms and atomic meanings (cf.\@ holistic languages with many meaning components per form).
The bijectivity--toposim correlation is also reflected in the progression from reference to set-reference to concept games in the image-based environments: not only does the toposim generally increase moving along the game progression (as presented in \citet{mu2021general}) but the morpheme bijectivity as well.
This gives even stronger evidence for claims of \citet{mu2021general} as morpheme bijectivity is argued to be a better measure of compositionality than that of toposim (\cref{sec:pb-comp-res}).
