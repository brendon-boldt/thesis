% \appendix

\section{Algorithm}%
\label{app:alg}

\subsection{Candidate generation}%
\label{app:candidate}

For simplicity's sake (and inductive bias), we limit the candidate generation functions to all non-empty substrings for forms and all non-empty subsets for meanings.
Nevertheless, we could extend form candidate generation to non-contiguous forms to detect non-concatenative morphology (e.g., the form ``\mbox{x.z}'' matching ``\mbox{xyz}'' and ``\mbox{xwz}'').
In fact, we could could use arbitrary regular expressions to represent forms (or meanings) such as ``\^{}..x'' or ``x+'' to represent absolute position and optional repetitions, respectively.
We could consider empty forms and empty meanings to explicitly identify forms and meanings which do not have mappings (as opposed to implicitly not including them in the morphology).

Of course, part of the difficulty of extending the complexity of the candidate generation is that it expands the already (sometimes intractably) large search space.
One method of making this tractable, though, is adding heuristics that determine which form candidates should be considered rather than considering every possible candidate.


\subsection{Ambiguous pair application}%
\label{app:ambig-app}

In some cases of applying a morpheme to record in the dataset, there are multiple applications possible.
Say we have the utterance ``x y z x y'' meaning $\{A,B\}$ and we want to apply the morpheme (``x y'', $\{A\}$).
The form matches two substrings in the utterance, so there are two possible ways to apply the morpheme.
As a heuristic for selecting the best application, CSAR break ties by selecting the substring least likely to be a morpheme (as determined by the morpheme weights).
Going back to the above example, if it is the case the morpheme (``z x y'', $\{B\}$) has a higher weight than (``x y z'', $\{B\}$), then CSAR will apply (``x y'', $\{A\}$) to the first instance of ``x y'' instead of the second.

This search can be very computationally expensive since it can entail going through a large number of morpheme candidates.
Thus for the experiments with human language data, we do not perform this search and select the best form quasirandomly.

\subsection{Heuristic optimizations}%
\label{app:opt}

Below we include a summary of heuristic optimizations available in CSAR\@:
\smallskip
\begin{description}[nosep,itemindent=-1em]
  \item[max input records] Only consider a certain number of records from the input data; $20\,000$ for machine translation, image captions, and ShapeWorld.
  \item[max inventory size] Stop after inducing a certain number of morphemes; $300$ for image captions and machine translation settings.
  \item[\textit{n}-gram semantics] Treat complete meanings as ordered and generate meaning candidates identically to forms (i.e., as $n$-grams); used for machine translation data where the ``meanings'' are sentences.
  \item[max form/meaning size] Only consider form/meaning candidates up to a certain size; $3$ for machine translation (form and meaning) and image captions (form only), $2$ for image captions meaning.
  \item[no search best form] When ablating a form with multiple matches in an utterance, do not search for best form, simply choose it randomly; no search for image captions and machine translation.
  \item[form/meaning vocabulary size] Only consider the most common form/meaning candidates; $100\,000$ for image captions and machine translation.
  \item[token vocabulary size] Only consider the most common form/meaning tokens and ignore an form meaning candidates which contain an unknown token; $1000$ for image captions and $500$ for machine translation.
  \item[co-occurrence threshold] Zero out any co-occurrences which fall below a certain threshold (e.g., if a form and meaning candidate only occur once, treat it as never co-occurring); $1$ for ShapeWorld, $10$ for image captions, and $100$ for machine translation.
\end{description}


\section{Empirical Validation}%
\label{app:emp-val}


\subsection{Procedural dataset hyperparameters}%
\label{app:synth-hparams}

The following hyperparameters were used for generating the procedural datasets.
Each dataset uses $4$ attributes and $4$ values except for the sparse setting which uses $8$ independent values.
\begin{description}[nosep,itemindent=-1em]
  \item[Synonymy] $\{1,3\}$; forms per meaning
  \item[Polysemy] $\{0, 0.15\}$; proportion of meanings mapped to an already-used form
  \item[Multi-token forms] $\{\{1\}, \{1,2,3,4\}\}$; possible tokens per form
  \item[Vocab size] $\{10, 50\}$; only applies to non-unity multi-token forms
  \item[Sparse meanings] $\{\text{true}, \text{false}\}$
  \item[Distribution imbalance]  $\{\text{true}, \text{false}\}$; non-uniform distribution is based on the ramp function, i.e., probability of given value for an attribute is proportional to its $\text{index} +1$.
  \item[Dataset size] $\{50, 500\}$
  \item[Noise forms] $\{0, 0.5\}$; $1-p$ of parameter of geometric distribution
  \item[Shuffle form] $\{\text{true}, \text{false}\}$
  \item[Non-compositionality] $\{\text{true}, \text{false}\}$
  \item[Random seeds] $3$ per hyperparameter setting
\end{description}
Non-unity polysemy and synonymy rates for the non-compositional dataset implementation were not implemented and are excluded from the above grid.

\subsection{Tokenizer vocabulary size}%
\label{app:vocab-size}

The heuristic for the tokenizer vocabulary size is as follows:
\begin{align}
  |V| &= \left\lfloor
    \frac{|\mathcal T_\text{meaning}|}{|\mathcal R|} \sum_{r\in\mathcal R} \frac{|r_\text{form}|}{|r_\text{meaning}|}
  \right\rfloor
    + |\mathcal T_\text{form}|
  ,
\end{align}
where
  $\mathcal T_\text{meaning}$ is the set of all meaning tokens in the dataset (likewise for $\mathcal T_\text{form}$),
  $\mathcal R$ is the multiset of records in dataset,
  $r_\text{form}$ is the particular form (utterance) for an individual record (likewise for $r_\text{meaning}$.
This heuristic can be interpreted as the mean form tokens per meaning tokens times the number unique meaning tokens added to the number of unique form tokens (since each of them will automatically be included in the vocabulary).

\subsection{Additional procedural dataset results}%
\label{app:proc-table}

\Cref{tab:proc-all} shows all results of baseline methods on the procedural datasets.
\Cref{fig:baseline-exact} visualizes the results of the baseline methods with exact $F_1$ score.

\begin{table*}
\centering
\input{chapters/morphemes/assets/proc-table}
\caption{Results of baseline methods on the procedural datasets.}%
\label{tab:proc-all}
\end{table*}

\begin{figure}
\centering
\input{chapters/morphemes/assets/baselines-exact.pgf}
\caption{Exact $F_1$ scores of baseline methods on the procedural datasets}%
\label{fig:baseline-exact}
\end{figure}

\section{Analysis of Emergent Languages}

\subsection{Emergent language hyperparameters}%
\label{app:el-hparams}

The following hyperparameters were used for the vector observation environment:
\smallskip
\begin{description}[nosep,itemindent=-1em]
  \item[\textit{\textbf{n}} values] $4$, $2$ (sparse)
  \item[\textit{\textbf{n}} attributes] $4$, $8$ (sparse)
  \item[\textit{\textbf{n}} distractors] $3$
  \item[vocab size] $32$
  \item[max sequence length] $10$
  \item[dataset size (CSAR input)] $10\,000$ records
\end{description}
\medskip

The ShapeWorld observation environment uses the following hyperparameters
\smallskip
\begin{description}[nosep,itemindent=-1em]
  \item[observations] $5$ shapes, $6$ colors, $3$ operators (and, or, not); \emph{and} or \emph{or} may only be used once
  \item[\textit{n} examples] $20$ total; $10$ correct targets, $10$ distractors
  \item[vocab size] $32$
  \item[max sequence length] $8$
  \item[dataset size (CSAR input)] $20\,000$ records
\end{description}
\medskip

Both environments had any beginning-of-sentence and end-of-sentence tokens removed before being fed into CSAR\@.
Running the above experiments requires about $25$ GPU-hours on NVIDIA GeForce RTX 2080Ti.


\section{Morfessor Results on Emergent Language}
\unskip\label{sec:morf-ec}
\begin{table}
  \centering
  \begin{tabular}{lrr}
  \toprule
                 & $|\text{Inv.}|$ & $|\text{Form}|$ \\
  \midrule
  Vector, AV     &            $94$ & $3.59$ \\
  Vector, sparse &           $126$ & $3.51$ \\
  SW, ref        &          $2898$ & $6.93$ \\
  SW, setref     &          $2920$ & $8.13$ \\
  SW, concept    &          $1565$ & $7.77$ \\
  \bottomrule
  \end{tabular}
  \caption{Metrics for form-only morpheme inventories generated by Morfessor across various emergent languages.}
  \unskip\label{tab:morf-ec}
\end{table}

In \cref{tab:morf-ec} we show the results of running Morfessor on various emergent language corpora.
Compared to the metrics for CSAR's output on the same corpora (\cref{tab:ec-quant}), Morfessor's results do not match or even differ consistently (although Morfessor's forms do not have prevalence weighting like CSAR's).
For the vector environments, Morfessor yields smaller inventories than CSAR yet larger inventories for ShapeWorld.
Form lengths are similar for the vector environment, but for ShapeWorld, CSAR yields shorter forms than the vector environment while Morfessor yields much longer forms.
Since we do not have ground truth morphemes for these emergent language corpora, we cannot definitively say one algorithm has performed better than the other.
Yet Morfessor here is at a disadvantage here as it is not able to use the meanings of the utterances to guide its induction.

\section{Morpheme Inventories}
Top $100$ morphemes induced by CSAR from human and emergent language datasets.

\subsection{Human languages}%
\label{app:human-language}


\paragraph{Morpho Challenge}
\input{chapters/morphemes/assets/morpho-challenge}
\paragraph{Image captions}
\input{chapters/morphemes/assets/coco}
\paragraph{Machine translation}
\input{chapters/morphemes/assets/mt}

\subsection{Emergent languages}%
\label{app:ec-inv}

\paragraph{Vector, attribute--value}
Note that meanings are in the format \emph{\mbox{attribute\_value}} meaning that 1\_2 means the $1$\textsuperscript{st} attribute has value $2$.

\medskip
\noindent
{\input{chapters/morphemes/assets/ec-vector-av}}

\paragraph{Vector, bag of meanings}
{\input{chapters/morphemes/assets/ec-vector-sparse}}
\paragraph{Shapeworld, reference}
{\input{chapters/morphemes/assets/ec-shapeworld-ref}}
\paragraph{Shapeworld, set reference}
{\input{chapters/morphemes/assets/ec-shapeworld-setref}}
\paragraph{Shapeworld, concept}
{\input{chapters/morphemes/assets/ec-shapeworld-concept}}
