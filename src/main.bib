@inproceedings{boldt2024xferbench,
    title = "{X}fer{B}ench: a Data-Driven Benchmark for Emergent Language",
    author = "Boldt, Brendon  and
      Mortensen, David",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.82",
    pages = "1475--1489",
    abstract = "In this paper, we introduce a benchmark for evaluating the overall quality of emergent languages using data-driven methods. Specifically, we interpret the notion of the {``}quality{''} of an emergent language as its similarity to human language within a deep learning framework. We measure this by using the emergent language as pretraining data for a downstream NLP tasks in human language{---}the better the downstream performance, the better the emergent language. We implement this benchmark as an easy-to-use Python package that only requires a text file of utterances from the emergent language to be evaluated. Finally, we empirically test the benchmark{'}s validity using human, synthetic, and emergent language baselines.",
}

@article{boldt2022recommendations,
  title={Recommendations for systematic research on emergent language},
  author={Boldt, Brendon and Mortensen, David},
  journal={arXiv preprint arXiv:2206.11302},
  year={2022}
}

@article{
boldt2024review,
title={A Review of the Applications of Deep Learning-Based Emergent Communication},
author={Brendon Boldt and David R Mortensen},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=jesKcQxQ7j},
note={}
}

@misc{
boldt2022shaped,
title={Shaped Rewards Bias Emergent Language},
author={Brendon Boldt and Yonatan Bisk and David R Mortensen},
year={2022},
url={https://openreview.net/forum?id=057dxuWpfx}
}

@article{boldt2022mathematically,
  title={Mathematically Modeling the Lexicon Entropy of Emergent Language},
  author={Boldt, Brendon and Mortensen, David},
  journal={arXiv preprint arXiv:2211.15783},
  year={2022}
}

@inproceedings{prabhumoye-etal-2021-case,
    title = "Case Study: Deontological Ethics in {NLP}",
    author = "Prabhumoye, Shrimai  and
      Boldt, Brendon  and
      Salakhutdinov, Ruslan  and
      Black, Alan W",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.297",
    doi = "10.18653/v1/2021.naacl-main.297",
    pages = "3784--3798",
    abstract = "Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.",
}

@incollection{greenberg1963universals,
  title={Some universals of grammar with particular reference to the order of meaningful elements},
  pages={58--90},
  booktitle={Universals of Language},
  publisher={The MIT Press},
  address={Cambridge, MA},
  author={Joseph H. Greenberg},
  editor={Joseph H. Greenberg},
  year=1963,
  url={https://archive.org/details/universalsoflang00unse}, 
}

@inproceedings{foerster2016learning,
author = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
title = {Learning to communicate with Deep multi-agent reinforcement learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2145–2153},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@article{lazaridou2016multiagent,
    title={Multi-Agent Cooperation and the Emergence of (Natural) Language},
    author={Angeliki Lazaridou and Alexander Peysakhovich and Marco Baroni},
    year={2016},
    eprint={1612.07182},
    volume={1612.07182},
    archivePrefix={arXiv},
    journal={arXiv},
    primaryClass={cs.CL}
}

@article{werner_Dyer_1991,
    title={Evolution of Communication in Artificial Organisms},
    author={Werner, Gregory M. and Dyer, Michael G.},
    year={1991},
    pages={659 - 687},
    journal={Artificial Life II},
    editor={Langton, C. G. and Taylor, C. and Farmer, J. D., and Rasmussen, S.},
    publisher={Addison-Wesley}
}

@article{silver2017masteringchessshogiselfplay,
    title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}, 
    author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
    year={2017},
    eprint={1712.01815},
    volume={1712.01815},
    archivePrefix={arXiv},
    journal={arXiv},
    primaryClass={cs.AI},
    url={https://arxiv.org/abs/1712.01815}, 
}

@article{baker2020emergenttoolusemultiagent,
      title={Emergent Tool Use From Multi-Agent Autocurricula}, 
      author={Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
      year={2020},
      eprint={1909.07528},
      volume={1909.07528},
      archivePrefix={arXiv},
      journal={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1909.07528}, 
}


@article{brighton2005,
title = {Language as an evolutionary system},
journal = {Physics of Life Reviews},
volume = {2},
number = {3},
pages = {177-226},
year = {2005},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2005.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1571064505000229},
author = {Henry Brighton and Kenny Smith and Simon Kirby},
keywords = {Language, Evolution, Artificial life, Culture, Adaptation, Replication},
abstract = {John Maynard Smith and Eörs Szathmáry argued that human language signified the eighth major transition in evolution: human language marked a new form of information transmission from one generation to another [Maynard Smith J, Szathmáry E. The major transitions in evolution. Oxford: Oxford Univ. Press; 1995]. According to this view language codes cultural information and as such forms the basis for the evolution of complexity in human culture. In this article we develop the theory that language also codes information in another sense: languages code information on their own structure. As a result, languages themselves provide information that influences their own survival. To understand the consequences of this theory we discuss recent computational models of linguistic evolution. Linguistic evolution is the process by which languages themselves evolve. This article draws together this recent work on linguistic evolution and highlights the significance of this process in understanding the evolution of linguistic complexity. Our conclusions are that: (1) the process of linguistic transmission constitutes the basis for an evolutionary system, and (2), that this evolutionary system is only superficially comparable to the process of biological evolution.}
}

@book{lewis1970ConventionAP,
	address = {Cambridge, MA, USA},
	author = {David Kellogg Lewis},
	editor = {},
	publisher = {Wiley-Blackwell},
	title = {Convention: A Philosophical Study},
	year = {1969}
}

@article{lazaridou2020emergentmultiagentcommunicationdeep,
      title={Emergent Multi-Agent Communication in the Deep Learning Era}, 
      author={Angeliki Lazaridou and Marco Baroni},
      year={2020},
      eprint={2006.02419},
      volume={2006.02419},
      archivePrefix={arXiv},
      journal={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.02419}, 
}

@book{kuhn,
	address = {Chicago, IL},
	author = {Thomas S. Kuhn},
	publisher = {University of Chicago Press},
	title = {The Structure of Scientific Revolutions},
	year = {1962}
}

@article{boldt2023mathematicallymodelinglexiconentropy,
      title={Mathematically Modeling the Lexicon Entropy of Emergent Language}, 
      author={Brendon Boldt and David Mortensen},
      year={2023},
      eprint={2211.15783},
      volume={2211.15783},
      archivePrefix={arXiv},
      journal={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.15783}, 
}

@article{bitter-lesson,
  title={The {B}itter {L}esson},
  year={2019},
  author={Richard Sutton},
  url={http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
} \cmt{cite}

@article{boldt2024elcc,
      title={ELCC: the Emergent Language Corpus Collection}, 
      author={Brendon Boldt and David Mortensen},
      year={2024},
      volume={2407.04158},
      journal={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.04158}, 
}

@article{weidinger2021ethicalsocialrisksharm,
    title={Ethical and social risks of harm from Language Models}, 
    author={Laura Weidinger and John Mellor and Maribeth Rauh and Conor Griffin and Jonathan Uesato and Po-Sen Huang and Myra Cheng and Mia Glaese and Borja Balle and Atoosa Kasirzadeh and Zac Kenton and Sasha Brown and Will Hawkins and Tom Stepleton and Courtney Biles and Abeba Birhane and Julia Haas and Laura Rimell and Lisa Anne Hendricks and William Isaac and Sean Legassick and Geoffrey Irving and Iason Gabriel},
    year={2021},
    volume={2112.04359},
    journal={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2112.04359}, 
}

@article{carlini2021extractingtrainingdatalarge,
      title={Extracting Training Data from Large Language Models}, 
      author={Nicholas Carlini and Florian Tramer and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown and Dawn Song and Ulfar Erlingsson and Alina Oprea and Colin Raffel},
      year={2021},
      eprint={2012.07805},
      volume={2012.07805},
      archivePrefix={arXiv},
      journal={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2012.07805}, 
}

@misc{kang2020incorporatingpragmaticreasoningcommunication,
      title={Incorporating Pragmatic Reasoning Communication into Emergent Language}, 
      author={Yipeng Kang and Tonghan Wang and Gerard de Melo},
      year={2020},
      volume={2006.04109},
      journal={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2006.04109}, 
}

@article{terry2021pettingzoo,
  title={Petting{Z}oo: Gym for multi-agent reinforcement learning},
  author={Terry, J and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario and Hari, Ananth and Sullivan, Ryan and Santos, Luis S and Dieffendahl, Clemens and Horsch, Caroline and Perez-Vicente, Rodrigo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={15032--15043},
  year={2021}
}

@software{minari,
	author = {Younis, Omar G. and Perez-Vicente, Rodrigo and Balis, John U. and Dudley, Will and Davey, Alex and Terry, Jordan K},
	doi = {10.5281/zenodo.13767625},
	month = sep,
	publisher = {Zenodo},
	title = {Minari},
	url = {https://doi.org/10.5281/zenodo.13767625},
	version = {0.5.0},
	year = 2024,
	bdsk-url-1 = {https://doi.org/10.5281/zenodo.13767625}
}

@InProceedings{jaques2019social,
  title = 	 {Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning},
  author =       {Jaques, Natasha and Lazaridou, Angeliki and Hughes, Edward and Gulcehre, Caglar and Ortega, Pedro and Strouse, Dj and Leibo, Joel Z. and De Freitas, Nando},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3040--3049},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/jaques19a/jaques19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/jaques19a.html},
  abstract = 	 {We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents’ actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents’ behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. Consequently, the influence reward opens up a window of new opportunities for research in this area.}
}
