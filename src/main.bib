@inproceedings{xferbench,
    title = "{X}fer{B}ench: a Data-Driven Benchmark for Emergent Language",
    author = "Boldt, Brendon  and
      Mortensen, David",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.82",
    doi = "10.18653/v1/2024.naacl-long.82",
    pages = "1475--1489",
    abstract = "In this paper, we introduce a benchmark for evaluating the overall quality of emergent languages using data-driven methods. Specifically, we interpret the notion of the {``}quality{''} of an emergent language as its similarity to human language within a deep learning framework. We measure this by using the emergent language as pretraining data for a downstream NLP tasks in human language{---}the better the downstream performance, the better the emergent language. We implement this benchmark as an easy-to-use Python package that only requires a text file of utterances from the emergent language to be evaluated. Finally, we empirically test the benchmark{'}s validity using human, synthetic, and emergent language baselines.",
}

@article{boldt2022recommendations,
  title={Recommendations for systematic research on emergent language},
  author={Boldt, Brendon and Mortensen, David},
  journal={arXiv preprint arXiv:2206.11302},
  year={2022}
}

@article{
boldt2024review,
title={A Review of the Applications of Deep Learning-Based Emergent Communication},
author={Brendon Boldt and David R Mortensen},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=jesKcQxQ7j},
note={}
}

@misc{
boldt2022shaped,
title={Shaped Rewards Bias Emergent Language},
author={Brendon Boldt and Yonatan Bisk and David R Mortensen},
year={2022},
url={https://openreview.net/forum?id=057dxuWpfx}
}

article{boldt2022mathematically,
  title={Mathematically Modeling the Lexicon Entropy of Emergent Language},
  author={Boldt, Brendon and Mortensen, David},
  journal={arXiv preprint arXiv:2211.15783},
  year={2022}
}

@inproceedings{prabhumoye-etal-2021-case,
    title = "Case Study: Deontological Ethics in {NLP}",
    author = "Prabhumoye, Shrimai  and
      Boldt, Brendon  and
      Salakhutdinov, Ruslan  and
      Black, Alan W",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.297",
    doi = "10.18653/v1/2021.naacl-main.297",
    pages = "3784--3798",
    abstract = "Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.",
}

@incollection{greenberg1963universals,
  title={Some universals of grammar with particular reference to the order of meaningful elements},
  pages={58--90},
  booktitle={Universals of Language},
  publisher={The MIT Press},
  address={Cambridge, MA},
  author={Joseph H. Greenberg},
  editor={Joseph H. Greenberg},
  year=1963,
  url={https://archive.org/details/universalsoflang00unse}, 
}

@inproceedings{foerster2016learning,
author = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
title = {Learning to communicate with Deep multi-agent reinforcement learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2145–2153},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@article{lazaridou2016multiagent,
    title={Multi-Agent Cooperation and the Emergence of (Natural) Language},
    author={Angeliki Lazaridou and Alexander Peysakhovich and Marco Baroni},
    year={2016},
    eprint={1612.07182},
    volume={1612.07182},
    archivePrefix={arXiv},
    journal={arXiv},
    primaryClass={cs.CL}
}

@article{werner_Dyer_1991,
    title={Evolution of Communication in Artificial Organisms},
    author={Werner, Gregory M. and Dyer, Michael G.},
    year={1991},
    pages={659 - 687},
    journal={Artificial Life II},
    editor={Langton, C. G. and Taylor, C. and Farmer, J. D., and Rasmussen, S.},
    publisher={Addison-Wesley}
}

@article{silver2017masteringchessshogiselfplay,
    title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}, 
    author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
    year={2017},
    eprint={1712.01815},
    volume={1712.01815},
    archivePrefix={arXiv},
    journal={arXiv},
    primaryClass={cs.AI},
    url={https://arxiv.org/abs/1712.01815}, 
}

@article{baker2020emergenttoolusemultiagent,
      title={Emergent Tool Use From Multi-Agent Autocurricula}, 
      author={Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
      year={2020},
      eprint={1909.07528},
      volume={1909.07528},
      archivePrefix={arXiv},
      journal={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1909.07528}, 
}


@article{brighton2005,
title = {Language as an evolutionary system},
journal = {Physics of Life Reviews},
volume = {2},
number = {3},
pages = {177-226},
year = {2005},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2005.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1571064505000229},
author = {Henry Brighton and Kenny Smith and Simon Kirby},
keywords = {Language, Evolution, Artificial life, Culture, Adaptation, Replication},
abstract = {John Maynard Smith and Eörs Szathmáry argued that human language signified the eighth major transition in evolution: human language marked a new form of information transmission from one generation to another [Maynard Smith J, Szathmáry E. The major transitions in evolution. Oxford: Oxford Univ. Press; 1995]. According to this view language codes cultural information and as such forms the basis for the evolution of complexity in human culture. In this article we develop the theory that language also codes information in another sense: languages code information on their own structure. As a result, languages themselves provide information that influences their own survival. To understand the consequences of this theory we discuss recent computational models of linguistic evolution. Linguistic evolution is the process by which languages themselves evolve. This article draws together this recent work on linguistic evolution and highlights the significance of this process in understanding the evolution of linguistic complexity. Our conclusions are that: (1) the process of linguistic transmission constitutes the basis for an evolutionary system, and (2), that this evolutionary system is only superficially comparable to the process of biological evolution.}
}

@book{lewis1970ConventionAP,
	address = {Cambridge, MA, USA},
	author = {David Kellogg Lewis},
	editor = {},
	publisher = {Wiley-Blackwell},
	title = {Convention: A Philosophical Study},
	year = {1969}
}

@article{lazaridou2020review,
      title={Emergent Multi-Agent Communication in the Deep Learning Era}, 
      author={Angeliki Lazaridou and Marco Baroni},
      year={2020},
      eprint={2006.02419},
      volume={2006.02419},
      archivePrefix={arXiv},
      journal={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.02419}, 
}

@book{kuhn,
	address = {Chicago, IL},
	author = {Thomas S. Kuhn},
	publisher = {University of Chicago Press},
	title = {The Structure of Scientific Revolutions},
	year = {1962}
}

@article{boldt2023mathematicallymodelinglexiconentropy,
      title={Mathematically Modeling the Lexicon Entropy of Emergent Language}, 
      author={Brendon Boldt and David Mortensen},
      year={2023},
      eprint={2211.15783},
      volume={2211.15783},
      archivePrefix={arXiv},
      journal={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.15783}, 
}

@article{bitter-lesson,
  title={The {B}itter {L}esson},
  year={2019},
  author={Richard Sutton},
  url={http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
} \cmt{cite}

@article{elcc,
      title={{ELCC}: the {E}mergent {L}anguage {C}orpus {C}ollection}, 
      author={Brendon Boldt and David Mortensen},
      year={2024},
      volume={2407.04158},
      journal={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.04158}, 
}

@article{weidinger2021ethicalsocialrisksharm,
    title={Ethical and social risks of harm from Language Models}, 
    author={Laura Weidinger and John Mellor and Maribeth Rauh and Conor Griffin and Jonathan Uesato and Po-Sen Huang and Myra Cheng and Mia Glaese and Borja Balle and Atoosa Kasirzadeh and Zac Kenton and Sasha Brown and Will Hawkins and Tom Stepleton and Courtney Biles and Abeba Birhane and Julia Haas and Laura Rimell and Lisa Anne Hendricks and William Isaac and Sean Legassick and Geoffrey Irving and Iason Gabriel},
    year={2021},
    volume={2112.04359},
    journal={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2112.04359}, 
}

@article{carlini2021extractingtrainingdatalarge,
      title={Extracting Training Data from Large Language Models}, 
      author={Nicholas Carlini and Florian Tramer and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown and Dawn Song and Ulfar Erlingsson and Alina Oprea and Colin Raffel},
      year={2021},
      eprint={2012.07805},
      volume={2012.07805},
      archivePrefix={arXiv},
      journal={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2012.07805}, 
}

@misc{kang2020incorporatingpragmaticreasoningcommunication,
      title={Incorporating Pragmatic Reasoning Communication into Emergent Language}, 
      author={Yipeng Kang and Tonghan Wang and Gerard de Melo},
      year={2020},
      volume={2006.04109},
      journal={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2006.04109}, 
}

@article{terry2021pettingzoo,
  title={Petting{Z}oo: Gym for multi-agent reinforcement learning},
  author={Terry, J and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario and Hari, Ananth and Sullivan, Ryan and Santos, Luis S and Dieffendahl, Clemens and Horsch, Caroline and Perez-Vicente, Rodrigo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={15032--15043},
  year={2021}
}

@software{minari,
	author = {Younis, Omar G. and Perez-Vicente, Rodrigo and Balis, John U. and Dudley, Will and Davey, Alex and Terry, Jordan K},
	doi = {10.5281/zenodo.13767625},
	month = sep,
	publisher = {Zenodo},
	title = {Minari},
	url = {https://doi.org/10.5281/zenodo.13767625},
	version = {0.5.0},
	year = 2024,
	bdsk-url-1 = {https://doi.org/10.5281/zenodo.13767625}
}

@InProceedings{jaques2019social,
  title = 	 {Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning},
  author =       {Jaques, Natasha and Lazaridou, Angeliki and Hughes, Edward and Gulcehre, Caglar and Ortega, Pedro and Strouse, Dj and Leibo, Joel Z. and De Freitas, Nando},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3040--3049},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/jaques19a/jaques19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/jaques19a.html},
  abstract = 	 {We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents’ actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents’ behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. Consequently, the influence reward opens up a window of new opportunities for research in this area.}
}

@inproceedings{
lipinski2024speaking,
title={Speaking Your Language: Spatial Relationships in Interpretable Emergent Communication},
author={Olaf Lipinski and Adam Sobey and Federico Cerutti and Timothy J. Norman},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=vIP8IWmZlN}
}

@inproceedings{bouma2009npmi,
  title={{N}ormalized ({P}ointwise) {M}utual {I}nformation in collocation extraction},
  year=2009,
  author={Bouma, Gerlof},
  publisher={Gunter Narr Verlag},
  editors={Christian Chiarcos and Richard Eckart de Castilho and Manfred Stede},
  url={https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf},
  booktitle={Proceedings of the Biennial German Society for Computational Linguistics and Language Technology Conference},
}

@inproceedings{
ueda2022categorial,
title={Categorial Grammar Induction as a Compositionality Measure for Emergent Languages in Signaling Games},
author={Ryo Ueda and Taiga Ishii and Koki Washio and Yusuke Miyao},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=Sbgb7b0Q-5}
}

@inproceedings{chaabouni-etal-2019-word,
    title = "Word-order Biases in Deep-agent Emergent Communication",
    author = "Chaabouni, Rahma  and
      Kharitonov, Eugene  and
      Lazaric, Alessandro  and
      Dupoux, Emmanuel  and
      Baroni, Marco",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1509",
    doi = "10.18653/v1/P19-1509",
    pages = "5166--5175",
    abstract = "Sequence-processing neural networks led to remarkable progress on many NLP tasks. As a consequence, there has been increasing interest in understanding to what extent they process language as humans do. We aim here to uncover which biases such models display with respect to {``}natural{''} word-order constraints. We train models to communicate about paths in a simple gridworld, using miniature languages that reflect or violate various natural language trends, such as the tendency to avoid redundancy or to minimize long-distance dependencies. We study how the controlled characteristics of our miniature languages affect individual learning and their stability across multiple network generations. The results draw a mixed picture. On the one hand, neural networks show a strong tendency to avoid long-distance dependencies. On the other hand, there is no clear preference for the efficient, non-redundant encoding of information that is widely attested in natural language. We thus suggest inoculating a notion of {``}effort{''} into neural networks, as a possible way to make their linguistic behavior more human-like.",
}

@inproceedings{
conklin2023compositionality,
title={Compositionality with Variation Reliably Emerges in Neural Networks},
author={Henry Conklin and Kenny Smith},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=-Yzz6vlX7V-}
}

@inproceedings{jiang-etal-2019-tiger,
    title = "{TIGE}r: Text-to-Image Grounding for Image Caption Evaluation",
    author = "Jiang, Ming  and
      Huang, Qiuyuan  and
      Zhang, Lei  and
      Wang, Xin  and
      Zhang, Pengchuan  and
      Gan, Zhe  and
      Diesner, Jana  and
      Gao, Jianfeng",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1220/",
    doi = "10.18653/v1/D19-1220",
    pages = "2141--2152",
    abstract = "This paper presents a new metric called TIGEr for the automatic evaluation of image captioning systems. Popular metrics, such as BLEU and CIDEr, are based solely on text matching between reference captions and machine-generated captions, potentially leading to biased evaluations because references may not fully cover the image content and natural language is inherently ambiguous. Building upon a machine-learned text-image grounding model, TIGEr allows to evaluate caption quality not only based on how well a caption represents image content, but also on how well machine-generated captions match human-generated captions. Our empirical tests show that TIGEr has a higher consistency with human judgments than alternative existing metrics. We also comprehensively assess the metric`s effectiveness in caption evaluation by measuring the correlation between human judgments and metric scores."
}

@InProceedings{Yeh_2018_CVPR,
author = {Yeh, Raymond A. and Do, Minh N. and Schwing, Alexander G.},
title = {Unsupervised Textual Grounding: Linking Words to Image Concepts},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@article{Taniguchi17062016,
author = {Tadahiro Taniguchi and Takayuki Nagai and Tomoaki Nakamura and Naoto Iwahashi and Tetsuya Ogata and Hideki Asoh},
title = {Symbol emergence in robotics: a survey},
journal = {Advanced Robotics},
volume = {30},
number = {11-12},
pages = {706--728},
year = {2016},
publisher = {Taylor \& Francis},
doi = {10.1080/01691864.2016.1164622},
URL = {https://doi.org/10.1080/01691864.2016.1164622},
eprint = {https://doi.org/10.1080/01691864.2016.1164622}
}

@inproceedings{goldwater-etal-2006-contextual,
    title = "Contextual Dependencies in Unsupervised Word Segmentation",
    author = "Goldwater, Sharon  and
      Griffiths, Thomas L.  and
      Johnson, Mark",
    editor = "Calzolari, Nicoletta  and
      Cardie, Claire  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P06-1085/",
    doi = "10.3115/1220175.1220260",
    pages = "673--680"
}

@inproceedings{mochihashi-etal-2009-bayesian,
    title = "{B}ayesian Unsupervised Word Segmentation with Nested {P}itman-{Y}or Language Modeling",
    author = "Mochihashi, Daichi  and
      Yamada, Takeshi  and
      Ueda, Naonori",
    editor = "Su, Keh-Yih  and
      Su, Jian  and
      Wiebe, Janyce  and
      Li, Haizhou",
    booktitle = "Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP}",
    month = aug,
    year = "2009",
    address = "Suntec, Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P09-1012/",
    pages = "100--108"
}

@article{brent1999efficient,
author = {Brent, Michael R.},
title = {An Efficient, Probabilistically Sound Algorithm for Segmentation andWord Discovery},
year = {1999},
issue_date = {Feb. 1999},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {34},
number = {1–3},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1007541817488},
doi = {10.1023/A:1007541817488},
abstract = {This paper presents a model-based, unsupervised algorithm
for recovering word boundaries in a natural-language text from which
they have been deleted. The algorithm is derived from a probability
model of the source that generated the text. The fundamental
structure of the model is specified abstractly so that the detailed
component models of phonology, word-order, and word frequency can be
replaced in a modular fashion. The model yields a
language-independent, prior probability distribution on all possible
sequences of all possible words over a given alphabet, based on the
assumption that the input was generated by concatenating words from a
fixed but unknown lexicon. The model is unusual in that it treats
the generation of a complete corpus, regardless of length, as a
single event in the probability space. Accordingly, the algorithm
does not estimate a probability distribution on words; instead, it
attempts to calculate the prior probabilities of various word
sequences that could underlie the observed text. Experiments on
phonemic transcripts of spontaneous speech by parents to young
children suggest that our algorithm is more effective than other
proposed algorithms, at least when utterance boundaries are given and
the text includes a substantial number of short utterances.},
journal = {Mach. Learn.},
month = feb,
pages = {71–105},
numpages = {35},
keywords = {unsupervised learning, segmentation, probability models, minimum description length (MDL), language acquisition, bayesian grammar induction}
}

@inproceedings{kim-etal-2019-unsupervised,
    title = "Unsupervised Recurrent Neural Network Grammars",
    author = "Kim, Yoon  and
      Rush, Alexander  and
      Yu, Lei  and
      Kuncoro, Adhiguna  and
      Dyer, Chris  and
      Melis, G{\'a}bor",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1114/",
    doi = "10.18653/v1/N19-1114",
    pages = "1105--1117",
    abstract = "Recurrent neural network grammars (RNNG) are generative models of language which jointly model syntax and surface structure by incrementally generating a syntax tree and sentence in a top-down, left-to-right order. Supervised RNNGs achieve strong language modeling and parsing performance, but require an annotated corpus of parse trees. In this work, we experiment with unsupervised learning of RNNGs. Since directly marginalizing over the space of latent trees is intractable, we instead apply amortized variational inference. To maximize the evidence lower bound, we develop an inference network parameterized as a neural CRF constituency parser. On language modeling, unsupervised RNNGs perform as well their supervised counterparts on benchmarks in English and Chinese. On constituency grammar induction, they are competitive with recent neural language models that induce tree structures from words through attention mechanisms."
}

@inproceedings{kim-etal-2019-compound,
    title = "Compound Probabilistic Context-Free Grammars for Grammar Induction",
    author = "Kim, Yoon  and
      Dyer, Chris  and
      Rush, Alexander",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1228/",
    doi = "10.18653/v1/P19-1228",
    pages = "2369--2385",
    abstract = "We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models."
}

@inbook{Kirby_2002, place={Cambridge}, title={Learning, bottlenecks and the evolution of recursive syntax}, booktitle={Linguistic Evolution through Language Acquisition}, publisher={Cambridge University Press}, author={Kirby, Simon}, editor={Briscoe, TedEditor}, year={2002}, pages={173–204}}

@phdthesis{brighton2003thesis,
  author = "Brighton, Henry",
  year = 2003,
  title = "Simplicity as a driving force in linguistic evolution",
  school = "University of Edinburgh",
  address = "Edinburgh, Scotland, UK",
  url = {http://hdl.handle.net/1842/23810},
}
