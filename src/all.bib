@book{wals,
  address   = {Leipzig},
  editor    = {Matthew S. Dryer and Martin Haspelmath},
  publisher = {Max Planck Institute for Evolutionary Anthropology},
  title     = {WALS Online},
  url       = {https://wals.info/},
  year      = {2013}
}

@article{
  brandizzi2022rlupus,
  IDS={brandizzi2021rlupus,'2021rlupus,Brandizzi2021RLupusCT},
  year = 2022,
  title = "{RL}upus: Cooperation through the emergent communication in {T}he {W}erewolf social deduction game",
  author = "Nicolo' Brandizzi and Davide Grossi and Luca Iocchi",
  journal = "Intelligenza Artificiale",
  volume = "15",
  number = "2",
  pages = "55--70",
  url = {https://content.iospress.com/articles/intelligenza-artificiale/ia210081},
}

@article{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{perkins2021texrel,
  title={TexRel: a Green Family of Datasets for Emergent Communications on Relations},
  author={Perkins, Hugh},
  archivePrefix={arXiv},
  eprint={2105.12804},
  year={2021}
}

@article{blum2023grammars,
  title={Grammars Across Time Analyzed ({GATA}): a dataset of 52 languages},
  author={Blum, Frederic and Barrientos, Carlos and Ingunza, Adriano and Blasi, Dami{\'a}n E and Zariquiey, Roberto},
  journal={Scientific Data},
  volume={10},
  number={1},
  pages={835},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{bisk-etal-2020-experience,
    title = "Experience Grounds Language",
    author = "Bisk, Yonatan  and
      Holtzman, Ari  and
      Thomason, Jesse  and
      Andreas, Jacob  and
      Bengio, Yoshua  and
      Chai, Joyce  and
      Lapata, Mirella  and
      Lazaridou, Angeliki  and
      May, Jonathan  and
      Nisnevich, Aleksandr  and
      Pinto, Nicolas  and
      Turian, Joseph",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.703",
    doi = "10.18653/v1/2020.emnlp-main.703",
    pages = "8718--8735",
}

@article{MinigridMiniworld23,
  author       = {Maxime Chevalier-Boisvert and Bolun Dai and Mark Towers and Rodrigo de Lazcano and Lucas Willems and Salem Lahlou and Suman Pal and Pablo Samuel Castro and Jordan Terry},
  title        = {Minigrid \& Miniworld: Modular \& Customizable Reinforcement Learning Environments for Goal-Oriented Tasks},
  journal      = {CoRR},
  volume       = {abs/2306.13831},
  year         = {2023},
}

@article{chevalier2018babyai,
  title={Babyai: A platform to study the sample efficiency of grounded language learning},
  author={Chevalier-Boisvert, Maxime and Bahdanau, Dzmitry and Lahlou, Salem and Willems, Lucas and Saharia, Chitwan and Nguyen, Thien Huu and Bengio, Yoshua},
  archivePrefix={arXiv},
  eprint={1810.08272},
  year={2018}
}

@misc{mihai2021learning,
    title={Learning to Draw: Emergent Communication through Sketching},
    author={Daniela Mihai and Jonathon Hare},
    year={2021},
    eprint={2106.02067},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{
gibson2017color,
author = {Edward Gibson  and Richard Futrell  and Julian Jara-Ettinger  and Kyle Mahowald  and Leon Bergen  and Sivalogeswaran Ratnasingam  and Mitchell Gibson  and Steven T. Piantadosi  and Bevil R. Conway },
title = {Color naming across languages reflects color use},
journal = {Proceedings of the National Academy of Sciences},
volume = {114},
number = {40},
pages = {10785-10790},
year = {2017},
doi = {10.1073/pnas.1619666114},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1619666114},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1619666114},
}

@article{zaslavsky2018color,
author = {Zaslavsky, Noga and Kemp, Charles and Tishby, Naftali and Regier, Terry},
title = {Color Naming Reflects Both Perceptual Structure and Communicative Need},
journal = {Topics in Cognitive Science},
volume = {11},
number = {1},
pages = {207-219},
keywords = {Information theory, Color naming, Categorization},
doi = {https://doi.org/10.1111/tops.12395},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12395},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/tops.12395},
year = {2019}
}

@inproceedings{tucker2021discrete,
  IDS={tucker2021emergent,Tucker2021EmergentDC},
 author = {Tucker, Mycal and Li, Huao and Agrawal, Siddharth and Hughes, Dana and Sycara, Katia  and Lewis, Michael and Shah, Julie A},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {10574--10586},
 publisher = {Curran Associates, Inc.},
 title = {Emergent Discrete Communication in Semantic Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5812f92450ccaf17275500841c70924a-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{van-der-wal-etal-2020-grammar,
  IDS={van2020the},
    title = "The Grammar of Emergent Languages",
    author = "van der Wal, Oskar  and
      de Boer, Silvan  and
      Bruni, Elia  and
      Hupkes, Dieuwke",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.270",
    doi = "10.18653/v1/2020.emnlp-main.270",
    pages = "3339--3359",
    abstract = "In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language. We show that the considered UGI techniques are appropriate to analyse emergent languages and we then study if the languages that emerge in a typical referential game setup exhibit syntactic structure, and to what extent this depends on the maximum message length and number of symbols that the agents are allowed to use. Our experiments demonstrate that a certain message length and vocabulary size are required for structure to emerge, but they also illustrate that more sophisticated game scenarios are required to obtain syntactic properties more akin to those observed in human language. We argue that UGI techniques should be part of the standard toolkit for analysing emergent languages and release a comprehensive library to facilitate such analysis for future researchers.",
}

@inproceedings{croissant,
    author = {Akhtar, Mubashara and Benjelloun, Omar and Conforti, Costanza and Gijsbers, Pieter and Giner-Miguelez, Joan and Jain, Nitisha and Kuchnik, Michael and Lhoest, Quentin and Marcenac, Pierre and Maskey, Manil and Mattson, Peter and Oala, Luis and Ruyssen, Pierre and Shinde, Rajat and Simperl, Elena and Thomas, Goeffry and Tykhonov, Slava and Vanschoren, Joaquin and van der Velde, Jos and Vogler, Steffen and Wu, Carole-Jean},
    title = {Croissant: A Metadata Format for ML-Ready Datasets},
    year = {2024},
    isbn = {9798400706110},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3650203.3663326},
    doi = {10.1145/3650203.3663326},
    pages = {1–6},
    numpages = {6},
    keywords = {ML datasets, discoverability, reproducibility, responsible AI},
    location = {Santiago, Chile},
    series = {DEEM '24}
}

@Inbook{Tanaka-Ishii2021,
author="Tanaka-Ishii, Kumiko",
title="Articulation of Elements",
bookTitle="Statistical Universals of Language: Mathematical Chance vs. Human Choice",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="115--124",
abstract="The previous two parts of this book considered statistical universals of language. Sequences were input to specific analysis methods to examine the behavior of words or characters. The resulting phenomena were studied from the two viewpoints of the poplulation and sequence. As shown by the thick rightward arrow in Fig. 1.1, Parts II and III studied language corpora to reveal the statistical universals.",
isbn="978-3-030-59377-3",
doi="10.1007/978-3-030-59377-3_11",
url="https://doi.org/10.1007/978-3-030-59377-3_11"
}

@article{harris,
 ISSN = {00978507, 15350665},
 URL = {http://www.jstor.org/stable/411036},
 author = {Zellig S. Harris},
 journal = {Language},
 number = {2},
 pages = {190--222},
 publisher = {Linguistic Society of America},
 title = {From Phoneme to Morpheme},
 urldate = {2024-11-29},
 volume = {31},
 year = {1955}
}

@inproceedings{optuna,
  title={{O}ptuna: A Next-Generation Hyperparameter Optimization Framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle={The 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2623--2631},
  year={2019}
}

@inproceedings{ozaki2020tpe-mo,
author = {Ozaki, Yoshihiko and Tanigaki, Yuki and Watanabe, Shuhei and Onishi, Masaki},
title = {Multiobjective tree-structured parzen estimator for computationally expensive optimization problems},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3389817},
doi = {10.1145/3377930.3389817},
abstract = {Practitioners often encounter computationally expensive multiobjective optimization problems to be solved in a variety of real-world applications. On the purpose of challenging these problems, we propose a new surrogate-based multiobjective optimization algorithm that does not require a large evaluation budget. It is called Multiobjective Tree-structured Parzen Estimator (MOTPE) and is an extension of the tree-structured Parzen estimator widely used to solve expensive single-objective optimization problems. Our empirical evidences reveal that MOTPE can approximate Pareto fronts of many benchmark problems better than existing methods with a limited budget. In this paper, we discuss furthermore the influence of MOTPE configurations to understand its behavior.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {533–541},
numpages = {9},
keywords = {bayesian optimization, computationally expensive optimization, infill criteria, machine learning, multiobjective optimization, surrogate modeling, tree-structured parzen estimator},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@misc{watanabe2023tpe-tutorial,
      title={Tree-Structured Parzen Estimator: Understanding Its Algorithm Components and Their Roles for Better Empirical Performance}, 
      author={Shuhei Watanabe},
      year={2023},
      eprint={2304.11127},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.11127}, 
}

@inproceedings{bergstra2011tpe,
 author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Algorithms for Hyper-Parameter Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
 volume = {24},
 year = {2011}
}

@inproceedings{
jang2017categorical,
title={Categorical Reparameterization with Gumbel-Softmax},
author={Eric Jang and Shixiang Gu and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rkE3y85ee}
}

@InCollection{sep-linguistics,
	author       =	{Scholz, Barbara C. and Pelletier, Francis Jeffry and Pullum, Geoffrey K. and Nefdt, Ryan},
	title        =	{{Philosophy of Linguistics}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta and Uri Nodelman},
	howpublished =	{\url{https://plato.stanford.edu/archives/spr2024/entries/linguistics/}},
	year         =	{2024},
	edition      =	{{S}pring 2024},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@article{piantadosi2014zipf,
  title={Zipf’s word frequency law in natural language: A critical review and future directions},
  author={Piantadosi, S.T.},
  year=2014,
  journal={Psychon Bull Rev},
  volume={21},
  pages={1112-–1130},
  url={https://doi.org/10.3758/s13423-014-0585-6},
}

@article{schutzenberger1963,
title = {On context-free languages and push-down automata},
journal = {Information and Control},
volume = {6},
number = {3},
pages = {246-264},
year = {1963},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(63)90306-1},
url = {https://www.sciencedirect.com/science/article/pii/S0019995863903061},
author = {M.P. Schützenberger},
abstract = {This note describes a special type of one-way, one-tape automata in the sense of Rabin and Scott that idealizes some of the elementary formal features used in the so-called “push-down store” programming techniques. It is verified that the sets of words accepted by these automata form a proper subset of the family of the unambiguous context-free languages of Chomsky's and that this property admits a weak converse.}
}

@inproceedings{gupta-etal-2020-compositionality,
    title = "Compositionality and Capacity in Emergent Languages",
    author = "Gupta, Abhinav  and
      Resnick, Cinjon  and
      Foerster, Jakob  and
      Dai, Andrew  and
      Cho, Kyunghyun",
    editor = "Gella, Spandana  and
      Welbl, Johannes  and
      Rei, Marek  and
      Petroni, Fabio  and
      Lewis, Patrick  and
      Strubell, Emma  and
      Seo, Minjoon  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 5th Workshop on Representation Learning for NLP",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.repl4nlp-1.5",
    doi = "10.18653/v1/2020.repl4nlp-1.5",
    pages = "34--38",
    abstract = "Recent works have discussed the extent to which emergent languages can exhibit properties of natural languages particularly learning compositionality. In this paper, we investigate the learning biases that affect the efficacy and compositionality in multi-agent communication in addition to the communicative bandwidth. Our foremost contribution is to explore how the capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.",
}

@InProceedings{kharitonov2020entmin,
  IDS={kharitonov2019entropy,Kharitonov2020EntropyMI,kharitonov2020entropy},
  title = 	 {Entropy Minimization In Emergent Languages},
  author =       {Kharitonov, Eugene and Chaabouni, Rahma and Bouchacourt, Diane and Baroni, Marco},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5220--5230},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/kharitonov20a/kharitonov20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/kharitonov20a.html},
  abstract = 	 {There is growing interest in studying the languages that emerge when neural agents are jointly trained to solve tasks requiring communication through a discrete channel. We investigate here the information-theoretic complexity of such languages, focusing on the basic two-agent, one-exchange setup. We find that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the communicating agent’s inputs and the messages is minimized, within the range afforded by the need for successful communication. That is, emergent languages are (nearly) as simple as the task they are developed for allow them to be. This pressure is amplified as we increase communication channel discreteness. Further, we observe that stronger discrete-channel-driven entropy minimization leads to representations with increased robustness to overfitting and adversarial attacks. We conclude by discussing the implications of our findings for the study of natural and artificial communication systems.}
}

@article{
chaabouni2021color,
  IDS={Chaabouni2021CommunicatingAN},
author = {Rahma Chaabouni  and Eugene Kharitonov  and Emmanuel Dupoux  and Marco Baroni },
title = {Communicating artificial neural networks develop efficient color-naming systems},
journal = {Proceedings of the National Academy of Sciences},
volume = {118},
number = {12},
pages = {e2016569118},
year = {2021},
doi = {10.1073/pnas.2016569118},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2016569118},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2016569118},
abstract = {Color names in human languages are organized into efficient systems optimizing an accuracy/complexity trade-off. We show that artificial neural networks trained with generic deep-learning methods to play a color-discrimination game develop color-naming systems whose distribution on the accuracy/complexity plane is strikingly similar to that of human languages. We proceed to show that efficiency and narrow complexity crucially depend on the discrete nature of communication, acting as an information bottleneck on the emergent code. This suggests that efficient categorization of colors (and possibly other semantic domains) in natural languages does not depend on specific biological constraints of humans, but it is instead a general property of discrete communication systems. Words categorize the semantic fields they refer to in ways that maximize communication accuracy while minimizing complexity. Focusing on the well-studied color domain, we show that artificial neural networks trained with deep-learning techniques to play a discrimination game develop communication systems whose distribution on the accuracy/complexity plane closely matches that of human languages. The observed variation among emergent color-naming systems is explained by different degrees of discriminative need, of the sort that might also characterize different human communities. Like human languages, emergent systems show a preference for relatively low-complexity solutions, even at the cost of imperfect communication. We demonstrate next that the nature of the emergent systems crucially depends on communication being discrete (as is human word usage). When continuous message passing is allowed, emergent systems become more complex and eventually less efficient. Our study suggests that efficient semantic categorization is a general property of discrete communication systems, not limited to human language. It suggests moreover that it is exactly the discrete nature of such systems that, acting as a bottleneck, pushes them toward low complexity and optimal efficiency.}}

@inproceedings{suzgun-etal-2019-lstm,
    title = "{LSTM} Networks Can Perform Dynamic Counting",
    author = "Suzgun, Mirac  and
      Belinkov, Yonatan  and
      Shieber, Stuart  and
      Gehrmann, Sebastian",
    editor = "Eisner, Jason  and
      Gall{\'e}, Matthias  and
      Heinz, Jeffrey  and
      Quattoni, Ariadna  and
      Rabusseau, Guillaume",
    booktitle = "Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges",
    month = aug,
    year = "2019",
    address = "Florence",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3905",
    doi = "10.18653/v1/W19-3905",
    pages = "44--54",
    abstract = "In this paper, we systematically assess the ability of standard recurrent networks to perform dynamic counting and to encode hierarchical representations. All the neural models in our experiments are designed to be small-sized networks both to prevent them from memorizing the training sets and to visualize and interpret their behaviour at test time. Our results demonstrate that the Long Short-Term Memory (LSTM) networks can learn to recognize the well-balanced parenthesis language (Dyck-1) and the shuffles of multiple Dyck-1 languages, each defined over different parenthesis-pairs, by emulating simple real-time k-counter machines. To the best of our knowledge, this work is the first study to introduce the shuffle languages to analyze the computational power of neural networks. We also show that a single-layer LSTM with only one hidden unit is practically sufficient for recognizing the Dyck-1 language. However, none of our recurrent networks was able to yield a good performance on the Dyck-2 language learning task, which requires a model to have a stack-like mechanism for recognition.",
}

@Inbook{Kennison2011,
author="Kennison, Shelia M.",
editor="Goldstein, Sam
and Naglieri, Jack A.",
title="Morpheme",
bookTitle="Encyclopedia of Child Behavior and Development",
year="2011",
publisher="Springer US",
address="Boston, MA",
pages="974--975",
isbn="978-0-387-79061-9",
doi="10.1007/978-0-387-79061-9_1843",
url="https://doi.org/10.1007/978-0-387-79061-9_1843"
}

@InCollection{sep-meaning,
	author       =	{Speaks, Jeff},
	title        =	{{Theories of Meaning}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta and Uri Nodelman},
	howpublished =	{\url{https://plato.stanford.edu/archives/win2024/entries/meaning/}},
	year         =	{2024},
	edition      =	{{W}inter 2024},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@phdthesis{brighton,
  author = {Henry Brighton},
  url = {https://era.ed.ac.uk/handle/1842/23810},
  year = 2003,
  title = {Simplicity as a driving force in linguistic evolution},
  school = {University of Edinburgh},
  address = {Edinburgh, UK}
}

@article{ibm-model-1,
  title = "The Mathematics of Statistical Machine Translation: Parameter Estimation",
  author = "Brown, Peter F.  and
    Della Pietra, Stephen A.  and
    Della Pietra, Vincent J.  and
    Mercer, Robert L.",
  editor = "Hirschberg, Julia",
  journal = "Computational Linguistics",
  volume = "19",
  number = "2",
  year = "1993",
  address = "Cambridge, MA",
  publisher = "MIT Press",
  url = "https://aclanthology.org/J93-2003/",
  pages = "263--311"
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162/",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725"
}

@article{gage1994bpe,
  IDS={Gage1994ANA},
author = {Gage, Philip},
title = {A new algorithm for data compression},
year = {1994},
issue_date = {Feb. 1994},
publisher = {R \& D Publications, Inc.},
address = {USA},
volume = {12},
number = {2},
issn = {0898-9788},
journal = {C Users J.},
month = feb,
pages = {23–38},
numpages = {16}
}

@inproceedings{kudo-2018-subword,
    title = "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates",
    author = "Kudo, Taku",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1007/",
    doi = "10.18653/v1/P18-1007",
    pages = "66--75",
    abstract = "Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings."
}

@inproceedings{creutz-lagus-2002-unsupervised,
    title = "Unsupervised Discovery of Morphemes",
    author = "Creutz, Mathias  and
      Lagus, Krista",
    booktitle = "Proceedings of the {ACL}-02 Workshop on Morphological and Phonological Learning",
    month = jul,
    year = "2002",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W02-0603/",
    doi = "10.3115/1118647.1118650",
    pages = "21--30"
}

@techreport{morfessor2,
  title = {Morfessor 2.0: Python Implementation and Extensions for {M}orfessor Baseline},
  author      = "Sami Virpioja and Peter Smit and Stig-Arne Grönroos and Mikko Kurimo",
  institution = "Aalto University",
  address     = "Helsinki, Finland",
  number      = "ISBN 978-952-60-5501-5",
  year        = 2013,
}

@article{Harris01081954,
author = {Zellig S. Harris},
title = {Distributional Structure},
journal = {WORD},
volume = {10},
number = {2-3},
pages = {146--162},
year = {1954},
publisher = {Routledge},
doi = {10.1080/00437956.1954.11659520},
URL = {https://doi.org/10.1080/00437956.1954.11659520},
eprint = {https://doi.org/10.1080/00437956.1954.11659520}
}

@ARTICLE{scipy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@inproceedings{numba,
author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
title = {Numba: a LLVM-based Python JIT compiler},
year = {2015},
isbn = {9781450340052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2833157.2833162},
doi = {10.1145/2833157.2833162},
abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition, we share our experience in building a JIT compiler using LLVM[1].},
booktitle = {Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC},
articleno = {7},
numpages = {6},
keywords = {LLVM, Python, compiler},
location = {Austin, Texas},
series = {LLVM '15}
}

@inproceedings{kurimo-etal-2010-morpho,
    title = "Morpho Challenge 2005-2010: Evaluations and Results",
    author = "Kurimo, Mikko  and
      Virpioja, Sami  and
      Turunen, Ville  and
      Lagus, Krista",
    editor = "Heinz, Jeffrey  and
      Cahill, Lynne  and
      Wicentowski, Richard",
    booktitle = "Proceedings of the 11th Meeting of the {ACL} Special Interest Group on Computational Morphology and Phonology",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W10-2211/",
    pages = "87--95"
}

@misc{lin2015microsoftcococommonobjects,
      title={Microsoft COCO: Common Objects in Context}, 
      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
      year={2015},
      eprint={1405.0312},
      archivePrefix={aaXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1405.0312}, 
}

@InProceedings{bojar-EtAl:2016:WMT1,
  author    = {Bojar, Ond
{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Graham, Yvette  and  Haddow, Barry  and  Huck, Matthias  and  Jimeno Yepes, Antonio  and  Koehn, Philipp  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Neveol, Aurelie  and  Neves, Mariana  and  Popel, Martin  and  Post, Matt  and  Rubino, Raphael  and  Scarton, Carolina  and  Specia, Lucia  and  Turchi, Marco  and  Verspoor, Karin  and  Zampieri, Marcos},
  title     = {Findings of the 2016 Conference on Machine Translation},
  booktitle = {Proceedings of the First Conference on Machine Translation},
  month     = {August},
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {131--198},
  url       = {http://www.aclweb.org/anthology/W/W16/W16-2301}
}

@misc{kuhnle2017shapeworldnewtest,
  IDS={Kuhnle2017ShapeWorldA},
      title={ShapeWorld - A new test methodology for multimodal language understanding}, 
      author={Alexander Kuhnle and Ann Copestake},
      year={2017},
      eprint={1704.04517},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1704.04517}, 
}

@inbook{chaabouni2019antiefficient,
  IDS={1905.12561,chaabouni2019antiefficientencodingemergentcommunication,chaabouni2019anti,Chaabouni2019AntiefficientEI},
author = {Chaabouni, Rahma and Kharitonov, Eugene and Dupoux, Emmanuel and Baroni, Marco},
title = {Anti-efficient encoding in emergent communication},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Despite renewed interest in emergent language simulations with neural networks, little is known about the basic properties of the induced code, and how they compare to human language. One fundamental characteristic of the latter, known as Zipf's Law of Abbreviation (ZLA), is that more frequent words are efficiently associated to shorter strings. We study whether the same pattern emerges when two neural networks, a "speaker" and a "listener", are trained to play a signaling game. Surprisingly, we find that networks develop an anti-efficient encoding scheme, in which the most frequent inputs are associated to the longest messages, and messages in general are skewed towards the maximum length threshold. This anti-efficient code appears easier to discriminate for the listener, and, unlike in human communication, the speaker does not impose a contrasting least-effort pressure towards brevity. Indeed, when the cost function includes a penalty for longer messages, the resulting message distribution starts respecting ZLA. Our analysis stresses the importance of studying the basic features of emergent communication in a highly controlled setup, to ensure the latter will not depart too far from human language. Moreover, we present a concrete illustration of how different functional pressures can lead to successful communication codes that lack basic properties of human language, thus highlighting the role such pressures play in the latter.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {565},
numpages = {11}
}

@ARTICLE{brighton2006toposim,
  IDS={brighton2006UnderstandingLE},
  author={Brighton, Henry and Kirby, Simon},
  journal={Artificial Life},
  title={Understanding Linguistic Evolution by Visualizing the Emergence of Topographic Mappings},
  year={2006},
  volume={12},
  number={2},
  pages={229-242},
  keywords={Language;evolution;visualization;replicators;learning},
  doi={10.1162/artl.2006.12.2.229}}

@inproceedings{batsuren-etal-2022-sigmorphon,
    title = "The {SIGMORPHON} 2022 Shared Task on Morpheme Segmentation",
    author = "Batsuren, Khuyagbaatar  and
      Bella, G{\'a}bor  and
      Arora, Aryaman  and
      Martinovic, Viktor  and
      Gorman, Kyle  and
      {\v{Z}}abokrtsk{\'y}, Zden{\v{e}}k  and
      Ganbold, Amarsanaa  and
      Dohnalov{\'a}, {\v{S}}{\'a}rka  and
      {\v{S}}ev{\v{c}}{\'i}kov{\'a}, Magda  and
      Pelegrinov{\'a}, Kate{\v{r}}ina  and
      Giunchiglia, Fausto  and
      Cotterell, Ryan  and
      Vylomova, Ekaterina",
    editor = "Nicolai, Garrett  and
      Chodroff, Eleanor",
    booktitle = "Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sigmorphon-1.11/",
    doi = "10.18653/v1/2022.sigmorphon-1.11",
    pages = "103--116",
    abstract = "The SIGMORPHON 2022 shared task on morpheme segmentation challenged systems to decompose a word into a sequence of morphemes and covered most types of morphology: compounds, derivations, and inflections. Subtask 1, word-level morpheme segmentation, covered 5 million words in 9 languages (Czech, English, Spanish, Hungarian, French, Italian, Russian, Latin, Mongolian) and received 13 system submissions from 7 teams and the best system averaged 97.29{\%} F1 score across all languages, ranging English (93.84{\%}) to Latin (99.38{\%}). Subtask 2, sentence-level morpheme segmentation, covered 18,735 sentences in 3 languages (Czech, English, Mongolian), received 10 system submissions from 3 teams, and the best systems outperformed all three state-of-the-art subword tokenization methods (BPE, ULM, Morfessor2) by 30.71{\%} absolute. To facilitate error analysis and support any type of future studies, we released all system predictions, the evaluation script, and all gold standard datasets."
}

@inproceedings{kottur-etal-2017-natural,
  IDS={Kottur2017NaturalLD,kottur2017natural},
    title = "Natural Language Does Not Emerge {\textquoteleft}Naturally' in Multi-Agent Dialog",
    author = "Kottur, Satwik  and
      Moura, Jos{\'e}  and
      Lee, Stefan  and
      Batra, Dhruv",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1321/",
    doi = "10.18653/v1/D17-1321",
    pages = "2962--2967",
    abstract = "A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task {\&} Talk reference game between two agents as a testbed, we present a sequence of {\textquoteleft}negative' results culminating in a {\textquoteleft}positive' one {--} showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge {\textquoteleft}naturally',despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate."
}

@misc{korbak2020measuringnontrivialcompositionalityemergent,
  IDS={korbak2020measuring,Korbak2020MeasuringNC},
      title={Measuring non-trivial compositionality in emergent communication}, 
      author={Tomasz Korbak and Julian Zubek and Joanna Rączaszek-Leonardi},
      year={2020},
      eprint={2010.15058},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2010.15058}, 
}

@article{narasimhan-etal-2015-unsupervised,
    title = "An Unsupervised Method for Uncovering Morphological Chains",
    author = "Narasimhan, Karthik  and
      Barzilay, Regina  and
      Jaakkola, Tommi",
    editor = "Collins, Michael  and
      Lee, Lillian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "3",
    year = "2015",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q15-1012/",
    doi = "10.1162/tacl_a_00130",
    pages = "157--167",
    abstract = "Most state-of-the-art systems today produce morphological analysis based only on orthographic patterns. In contrast, we propose a model for unsupervised morphological analysis that integrates orthographic and semantic views of words. We model word formation in terms of morphological chains, from base words to the observed words, breaking the chains into parent-child relations. We use log-linear models with morpheme and word-level features to predict possible parents, including their modifications, for each word. The limited set of candidate parents for each word render contrastive estimation feasible. Our model consistently matches or outperforms five state-of-the-art systems on Arabic, English and Turkish."
}

@inproceedings{boldt2025csar,
    title = "Morpheme Induction for Emergent Language",
    author = "Boldt, Brendon  and
      Mortensen, David R.",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.1284/",
    pages = "25275--25290",
    ISBN = "979-8-89176-332-6"
}

@inproceedings{carmeli-etal-2024-concept,
    title = "Concept-Best-Matching: Evaluating Compositionality In Emergent Communication",
    author = "Carmeli, Boaz  and
      Belinkov, Yonatan  and
      Meir, Ron",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.189/",
    doi = "10.18653/v1/2024.findings-acl.189",
    pages = "3186--3194",
    abstract = "Artificial agents that learn to communicate in order to accomplish a given task acquire communication protocols that are typically opaque to a human. A large body of work has attempted to evaluate the emergent communication via various evaluation measures, with **compositionality** featuring as a prominent desired trait. However, current evaluation procedures do not directly expose the compositionality of the emergent communication. We propose a procedure to assess the compositionality of emergent communication by finding the best-match between emerged words and natural language concepts.The best-match algorithm provides both a global score and a translation-map from emergent words to natural language concepts. To the best of our knowledge, it is the first time that such direct and interpretable mapping between emergent words and human concepts is provided."
}

@misc{levy2025unsupervisedtranslationemergentcommunication,
      title={Unsupervised Translation of Emergent Communication},
      author={Ido Levy and Orr Paradise and Boaz Carmeli and Ron Meir and Shafi Goldwasser and Yonatan Belinkov},
      year={2025},
      eprint={2502.07552},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.07552},
}

@misc{gilberti2025discoveringpropertiesinflectionalmorphology,
      title={Discovering Properties of Inflectional Morphology in Neural Emergent Communication},
      author={Miles Gilberti and Shane Storks and Huteng Dai},
      year={2025},
      eprint={2508.05843},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2508.05843},
}

@inproceedings{chaabouni-etal-2020-compositionality,
  IDS={chaabouni2020compositionality,2004.09124},
    title = "Compositionality and Generalization In Emergent Languages",
    author = "Chaabouni, Rahma  and
      Kharitonov, Eugene  and
      Bouchacourt, Diane  and
      Dupoux, Emmanuel  and
      Baroni, Marco",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.407/",
    doi = "10.18653/v1/2020.acl-main.407",
    pages = "4427--4442",
    abstract = "Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive."
}

@inproceedings{kharitonov-baroni-2020-emergent,
  IDS={kharitonov2020emergent,Kharitonov2020EmergentLG,kharitonov2020emergentlanguagegeneralizationacquisition,2004.03420},
    title = "Emergent Language Generalization and Acquisition Speed are not tied to Compositionality",
    author = "Kharitonov, Eugene  and
      Baroni, Marco",
    editor = "Alishahi, Afra  and
      Belinkov, Yonatan  and
      Chrupa{\l}a, Grzegorz  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.blackboxnlp-1.2/",
    doi = "10.18653/v1/2020.blackboxnlp-1.2",
    pages = "11--15",
    abstract = "Studies of discrete languages emerging when neural agents communicate to solve a joint task often look for evidence of compositional structure. This stems for the expectation that such a structure would allow languages to be acquired faster by the agents and enable them to generalize better. We argue that these beneficial properties are only loosely connected to compositionality. In two experiments, we demonstrate that, depending on the task, non-compositional languages might show equal, or better, generalization performance and acquisition speed than compositional ones. Further research in the area should be clearer about what benefits are expected from compositionality, and how the latter would lead to them."
}

@article{forcada2011apertium,
  title={Apertium: a free/open-source platform for rule-based machine translation},
  author={Forcada, Mikel L and Ginest{\'\i}-Rosell, Mireia and Nordfalk, Jacob and O’Regan, Jim and Ortiz-Rojas, Sergio and P{\'e}rez-Ortiz, Juan Antonio and S{\'a}nchez-Mart{\'\i}nez, Felipe and Ram{\'\i}rez-S{\'a}nchez, Gema and Tyers, Francis M},
  journal={Machine translation},
  volume={25},
  number={2},
  pages={127--144},
  year={2011},
  publisher={Springer}
}

@article{partee1984compositionality,
  title={Compositionality},
  author={Partee, Barbara and others},
  journal={Varieties of formal semantics},
  volume={3},
  pages={281--311},
  year={1984},
  publisher={Foris Dordrecht}
}

@inproceedings{maddison2017concrete,
  IDS={maddison2017the},
    title = "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
    author = "Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye",
    booktitle = {Proceedings of the 2017 International Conference on Learning Representations (ICLR)},
    year = "2017",
    url = "https://openreview.net/forum?id=S1jE5L5gl",
}

@article{galke2024easy,
   title={Deep neural networks and humans both benefit from compositional language structure},
   volume={15},
   ISSN={2041-1723},
   url={http://dx.doi.org/10.1038/s41467-024-55158-1},
   DOI={10.1038/s41467-024-55158-1},
   number={1},
   journal={Nature Communications},
   publisher={Springer Science and Business Media LLC},
   author={Galke, Lukas and Ram, Yoav and Raviv, Limor},
   year={2024},
   month=dec }

@misc{boldt2022modeling,
Author = {Brendon Boldt and David R. Mortensen},
Title = {Modeling Emergent Lexicon Formation with a Self-Reinforcing Stochastic Process},
Year = {2022},
Eprint = {arXiv:2206.11146},
url = {https://arxiv.org/abs/2206.11146},
}

@misc{auersperger2022defending,
Author = {Michal Auersperger and Pavel Pecina},
Title = {Defending Compositionality in Emergent Languages},
Year = {2022},
Eprint = {arXiv:2206.04751},
url = {https://arxiv.org/abs/2206.04751},
}

@misc{taniguchi2022emergent,
Author = {Tadahiro Taniguchi and Yuto Yoshida and Akira Taniguchi and Yoshinobu Hagiwara},
Title = {Emergent Communication through Metropolis-Hastings Naming Game with Deep Generative Models},
Year = {2022},
Eprint = {arXiv:2205.12392},
url = {https://arxiv.org/abs/2205.12392},
}

@misc{bradley2022emergent,
Author = {Michael Bradley Johanson and Edward Hughes and Finbarr Timbers and Joel Z. Leibo},
Title = {Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning},
Year = {2022},
Eprint = {arXiv:2205.06760},
url = {https://arxiv.org/abs/2205.06760},
}

@misc{galke2022emergent,
  IDS={Galke2022EmergentCF},
Author = {Lukas Galke and Yoav Ram and Limor Raviv},
Title = {Emergent Communication for Understanding Human Language Evolution: What's Missing?},
Year = {2022},
Eprint = {arXiv:2204.10590},
url = {https://arxiv.org/abs/2204.10590},
}

@misc{ohmer2022emergence,
  IDS={2203.13176},
Author = {Xenia Ohmer and Marko Duda and Elia Bruni},
Title = {Emergence of hierarchical reference systems in multi-agent communication},
Year = {2022},
Eprint = {arXiv:2203.13176},
url = {https://arxiv.org/abs/2203.13176},
}

@misc{karten2022the,
  IDS={Karten2022TheEC},
Author = {Seth Karten and Siddharth Agrawal and Mycal Tucker and Dana Hughes and Michael Lewis and Julie Shah and Katia Sycara},
Title = {The Enforcers: Consistent Sparse-Discrete Methods for Constraining Informative Emergent Communication},
Year = {2022},
Eprint = {arXiv:2201.07452},
url = {https://arxiv.org/abs/2201.07452},
}

@misc{wang2022emergence,
Author = {Yuqi Wang and Xu-Yao Zhang and Cheng-Lin Liu and Zhaoxiang Zhang},
Title = {Emergence of Machine Language: Towards Symbolic Intelligence with Neural Networks},
Year = {2022},
Eprint = {arXiv:2201.05489},
url = {https://arxiv.org/abs/2201.05489},
}

@misc{ohmer2021mutual,
  IDS={2112.14518,Ohmer2021MutualIB},
Author = {Xenia Ohmer and Michael Marino and Michael Franke and Peter König},
Title = {Mutual influence between language and perception in multi-agent communication games},
Year = {2021},
Eprint = {arXiv:2112.14518},
url = {https://arxiv.org/abs/2112.14518},
}

@misc{qiu2021emergent,
Author = {Shuwen Qiu and Sirui Xie and Lifeng Fan and Tao Gao and Song-Chun Zhu and Yixin Zhu},
Title = {Emergent Graphical Conventions in a Visual Communication Game},
Year = {2021},
Eprint = {arXiv:2111.14210},
url = {https://arxiv.org/abs/2111.14210},
}

@misc{kucinski2021catalytic,
  IDS={2111.06464},
Author = {{\L}ukasz Kuci{\'n}ski and Tomasz Korbak and Pawe{\l} Ko{\l}odziej and Piotr Mi{\l}o{\'s}},
Title = {Catalytic Role Of Noise And Necessity Of Inductive Biases In The Emergence Of Compositional Communication},
Year = {2021},
Eprint = {arXiv:2111.06464},
url = {https://arxiv.org/abs/2111.06464},
}

@misc{eloff2021towards,
  IDS={Eloff2021TowardsLT,eloff2023learning},
Author = {Kevin Eloff and Arnu Pretorius and Okko Räsänen and Herman A. Engelbrecht and Herman Kamper},
Title = {Towards Learning to Speak and Hear Through Multi-Agent Communication over a Continuous Acoustic Channel},
Year = {2021},
Eprint = {arXiv:2111.02827},
url = {https://arxiv.org/abs/2111.02827},
}

@misc{lin2021learning,
Author = {Toru Lin and Minyoung Huh and Chris Stauffer and Ser-Nam Lim and Phillip Isola},
Title = {Learning to Ground Multi-Agent Communication with Autoencoders},
Year = {2021},
Eprint = {arXiv:2110.15349},
url = {https://arxiv.org/abs/2110.15349},
}

@misc{okuda2021directional,
Author = {Shimpei Okuda and Michio Hosaka and Kazutoshi Sasahara},
Title = {Directional forces in the evolution of grammar},
Year = {2021},
Eprint = {arXiv:2110.08567},
url = {https://arxiv.org/abs/2110.08567},
}

@misc{foguelman2021simulation,
Author = {Daniel Foguelman and Esteban Lanzarotti and Emanuel Ferreyra and Rodrigo Castro},
Title = {Simulation of emergence in artificial societies: a practical model-based approach with the EB-DEVS formalism},
Year = {2021},
Eprint = {arXiv:2110.08170},
url = {https://arxiv.org/abs/2110.08170},
}

@misc{patel2021interpretation,
Author = {Shivansh Patel and Saim Wani and Unnat Jain and Alexander Schwing and Svetlana Lazebnik and Manolis Savva and Angel X. Chang},
Title = {Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents},
Year = {2021},
Eprint = {arXiv:2110.05769},
url = {https://arxiv.org/abs/2110.05769},
}

@misc{ma2021learn,
Author = {Yifan Ma and Yifei Shen and Xianghao Yu and Jun Zhang and S. H. Song and Khaled B. Letaief},
Title = {Learn to Communicate with Neural Calibration: Scalability and Generalization},
Year = {2021},
Eprint = {arXiv:2110.00272},
url = {https://arxiv.org/abs/2110.00272},
}

@misc{d2021visual,
Author = {Robert D. Hawkins and Megumi Sano and Noah D. Goodman and Judith E. Fan},
Title = {Visual resemblance and communicative context constrain the emergence of graphical conventions},
Year = {2021},
Eprint = {arXiv:2109.13861},
url = {https://arxiv.org/abs/2109.13861},
}

@misc{hagiwara2021multiagent,
Author = {Yoshinobu Hagiwara and Kazuma Furukawa and Akira Taniguchi and Tadahiro Taniguchi},
Title = {Multiagent Multimodal Categorization for Symbol Emergence: Emergent Communication via Interpersonal Cross-modal Inference},
Year = {2021},
Eprint = {arXiv:2109.07194},
url = {https://arxiv.org/abs/2109.07194},
}

@misc{yu2021predicting,
Author = {Lei Yu and Yang Xu},
Title = {Predicting emergent linguistic compositions through time: Syntactic frame extension via multimodal chaining},
Year = {2021},
Eprint = {arXiv:2109.04652},
url = {https://arxiv.org/abs/2109.04652},
}

@misc{timothy2021symbol,
Author = {Michael Timothy Bennett},
Title = {Symbol Emergence and The Solutions to Any Task},
Year = {2021},
Eprint = {arXiv:2109.01281},
url = {https://arxiv.org/abs/2109.01281},
}

@misc{a2021multi,
Author = {Niko A. Grupen and Daniel D. Lee and Bart Selman},
Title = {Multi-Agent Curricula and Emergent Implicit Signaling},
Year = {2021},
Eprint = {arXiv:2106.11156},
url = {https://arxiv.org/abs/2106.11156},
}

@misc{dessì2021interpretable,
  IDS={2106.04258,Dessi2021InterpretableAC},
Author = {Roberto Dessì and Eugene Kharitonov and Marco Baroni},
Title = {Interpretable agent communication from scratch (with a generic visual processor emerging on the side)},
Year = {2021},
Eprint = {arXiv:2106.04258},
url = {https://arxiv.org/abs/2106.04258},
}

@misc{hazra2021zero,
Author = {Rishi Hazra and Sonu Dixit and Sayambhu Sen},
Title = {Zero-Shot Generalization using Intrinsically Motivated Compositional Emergent Protocols},
Year = {2021},
Eprint = {arXiv:2105.05069},
url = {https://arxiv.org/abs/2105.05069},
}

@misc{timothy2021intensional,
Author = {Michael Timothy Bennett and Yoshihiro Maruyama},
Title = {Intensional Artificial Intelligence: From Symbol Emergence to Explainable and Empathetic AI},
Year = {2021},
Eprint = {arXiv:2104.11573},
url = {https://arxiv.org/abs/2104.11573},
}

@misc{perkins2021neural,
  IDS={2103.04180},
Author = {Hugh Perkins},
Title = {Neural networks can understand compositional functions that humans do not, in the context of emergent communication},
Year = {2021},
Eprint = {arXiv:2103.04180},
url = {https://arxiv.org/abs/2103.04180},
}

@misc{li2021learning,
Author = {Sheng Li and Yutai Zhou and Ross Allen and Mykel J. Kochenderfer},
Title = {Learning Emergent Discrete Message Communication for Cooperative Reinforcement Learning},
Year = {2021},
Eprint = {arXiv:2102.12550},
url = {https://arxiv.org/abs/2102.12550},
}

@misc{noukhovitch2021emergent,
Author = {Michael Noukhovitch and Travis LaCroix and Angeliki Lazaridou and Aaron Courville},
Title = {Emergent Communication under Competition},
Year = {2021},
Eprint = {arXiv:2101.10276},
url = {https://arxiv.org/abs/2101.10276},
}

@misc{mihai2021the,
  IDS={Mihai2021TheEO,2101.10253},
Author = {Daniela Mihai and Jonathon Hare},
Title = {The emergence of visual semantics through communication games},
Year = {2021},
Eprint = {arXiv:2101.10253},
url = {https://arxiv.org/abs/2101.10253},
}

@misc{denamganai2020on,
  IDS={2012.10776},
Author = {Kevin Denamganaï and James Alfred Walker},
Title = {On (Emergent) Systematic Generalisation and Compositionality in Visual Referential Games with Straight-Through Gumbel-Softmax Estimator},
Year = {2020},
Eprint = {arXiv:2012.10776},
url = {https://arxiv.org/abs/2012.10776},
}

@misc{denamganaï2020referentialgym,
Author = {Kevin Denamganaï and James Alfred Walker},
Title = {ReferentialGym: A Nomenclature and Framework for Language Emergence \& Grounding in (Visual) Referential Games},
Year = {2020},
Eprint = {arXiv:2012.09486},
url = {https://arxiv.org/abs/2012.09486},
}

@misc{hazra2020infinite,
Author = {Rishi Hazra and Sonu Dixit and Sayambhu Sen},
Title = {Infinite use of finite means: Zero-Shot Generalization using Compositional Emergent Protocols},
Year = {2020},
Eprint = {arXiv:2012.05011},
url = {https://arxiv.org/abs/2012.05011},
}

@misc{guo2020inductive,
  IDS={Guo2020InductiveBA,2012.02875},
Author = {Shangmin Guo and Yi Ren and Agnieszka Słowik and Kory Mathewson},
Title = {Inductive Bias and Language Expressivity in Emergent Communication},
Year = {2020},
Eprint = {arXiv:2012.02875},
url = {https://arxiv.org/abs/2012.02875},
}

@misc{a2020low,
  IDS={Grupen2020LowBandwidthCE},
Author = {Niko A. Grupen and Daniel D. Lee and Bart Selman},
Title = {Low-Bandwidth Communication Emerges Naturally in Multi-Agent Learning Systems},
Year = {2020},
Eprint = {arXiv:2011.14890},
url = {https://arxiv.org/abs/2011.14890},
}

@misc{beguš2020deep,
Author = {Gašper Beguš},
Title = {Deep Sound Change: Deep and Iterative Learning, Convolutional Neural Networks, and Language Change},
Year = {2020},
Eprint = {arXiv:2011.05463},
url = {https://arxiv.org/abs/2011.05463},
}

@misc{bullard2020exploring,
  IDS={Bullard2020ExploringZE},
Author = {Kalesha Bullard and Franziska Meier and Douwe Kiela and Joelle Pineau and Jakob Foerster},
Title = {Exploring Zero-Shot Emergent Communication in Embodied Multi-Agent Populations},
Year = {2020},
Eprint = {arXiv:2010.15896},
url = {https://arxiv.org/abs/2010.15896},
}

@misc{chowdhury2020symbolic,
  IDS={2008.09866,Chowdhury2020SymbolicSS},
Author = {Aritra Chowdhury and Alberto Santamaria-Pang and James R. Kubricht and Jianwei Qiu and Peter Tu},
Title = {Symbolic Semantic Segmentation and Interpretation of COVID-19 Lung Infections in Chest CT volumes based on Emergent Languages},
Year = {2020},
Eprint = {arXiv:2008.09866},
url = {https://arxiv.org/abs/2008.09866},
}

@misc{blumenkamp2020the,
Author = {Jan Blumenkamp and Amanda Prorok},
Title = {The Emergence of Adversarial Communication in Multi-Agent Reinforcement Learning},
Year = {2020},
Eprint = {arXiv:2008.02616},
url = {https://arxiv.org/abs/2008.02616},
}

@misc{santamaria2020towards,
Author = {Alberto Santamaria-Pang and James Kubricht and Aritra Chowdhury and Chitresh Bhushan and Peter Tu},
Title = {Towards Emergent Language Symbolic Semantic Segmentation and Model Interpretability},
Year = {2020},
Eprint = {arXiv:2007.09448},
url = {https://arxiv.org/abs/2007.09448},
}

@misc{karjus2020communicative,
Author = {Andres Karjus and Richard A. Blythe and Simon Kirby and Kenny Smith},
Title = {Communicative need modulates competition in language change},
Year = {2020},
Eprint = {arXiv:2006.09277},
url = {https://arxiv.org/abs/2006.09277},
}

@misc{kang2020incorporating,
  IDS={Kang2020IncorporatingPR,kang2020incorporatingpragmaticreasoningcommunication},
Author = {Yipeng Kang and Tonghan Wang and Gerard de Melo},
Title = {Incorporating Pragmatic Reasoning Communication into Emergent Language},
Year = {2020},
Eprint = {arXiv:2006.04109},
url = {https://arxiv.org/abs/2006.04109},
}

@misc{bailly2020emergence,
Author = {Raphaël Bailly and Kata Gábor},
Title = {Emergence of Syntax Needs Minimal Supervision},
Year = {2020},
Eprint = {arXiv:2005.01119},
url = {https://arxiv.org/abs/2005.01119},
}

@misc{geffen2020on,
  IDS={2005.00110,Lan2020OnTS},
Author = {Nur Geffen Lan and Emmanuel Chemla and Shane Steinert-Threlkeld},
Title = {On the Spontaneous Emergence of Discrete and Compositional Signals},
Year = {2020},
Eprint = {arXiv:2005.00110},
url = {https://arxiv.org/abs/2005.00110},
}

@misc{nevens2020a,
Author = {Jens Nevens and Paul Van Eecke and Katrien Beuls},
Title = {A Practical Guide to Studying Emergent Communication through Grounded Language Games},
Year = {2020},
Eprint = {arXiv:2004.09218},
url = {https://arxiv.org/abs/2004.09218},
}

@misc{gupta2020networked,
Author = {Shubham Gupta and Rishi Hazra and Ambedkar Dukkipati},
Title = {Networked Multi-Agent Reinforcement Learning with Emergent Communication},
Year = {2020},
Eprint = {arXiv:2004.02780},
url = {https://arxiv.org/abs/2004.02780},
}

@misc{kajić2020learning,
Author = {Ivana Kajić and Eser Aygün and Doina Precup},
Title = {Learning to cooperate: Emergent communication in multi-agent navigation},
Year = {2020},
Eprint = {arXiv:2004.01097},
url = {https://arxiv.org/abs/2004.01097},
}

@misc{pu2020on,
Author = {Paul Pu Liang and Jeffrey Chen and Ruslan Salakhutdinov and Louis-Philippe Morency and Satwik Kottur},
Title = {On Emergent Communication in Competitive Multi-Agent Teams},
Year = {2020},
Eprint = {arXiv:2003.01848},
url = {https://arxiv.org/abs/2003.01848},
}

@misc{cowenrivers2020emergent,
Author = {Alexander I. {Cowen-Rivers} and Jason Naradowsky},
Title = {Emergent Communication with World Models},
Year = {2020},
Eprint = {arXiv:2002.09604},
url = {https://arxiv.org/abs/2002.09604},
}

@misc{pesce2020learning,
Author = {Emanuele Pesce and Giovanni Montana},
Title = {Learning Multi-Agent Coordination through Connectivity-driven Communication},
Year = {2020},
Eprint = {arXiv:2002.05233},
url = {https://arxiv.org/abs/2002.05233},
}

@misc{słowik2020structural,
  IDS={Sowik2020ExploringSI,2002.01335,Sowik2020StructuralIB},
Author = {Agnieszka Słowik and Abhinav Gupta and William L. Hamilton and Mateja Jamnik and Sean B. Holden and Christopher Pal},
Title = {Structural Inductive Biases in Emergent Communication},
Year = {2020},
Eprint = {arXiv:2002.01335},
url = {https://arxiv.org/abs/2002.01335},
}

@misc{lowe2020on,
  IDS={2002.01093,Lowe2020OnTI},
Author = {Ryan Lowe and Abhinav Gupta and Jakob Foerster and Douwe Kiela and Joelle Pineau},
Title = {On the interaction between supervision and self-play in emergent communication},
Year = {2020},
Eprint = {arXiv:2002.01093},
url = {https://arxiv.org/abs/2002.01093},
}

@misc{słowik2020towards,
Author = {Agnieszka Słowik and Abhinav Gupta and William L. Hamilton and Mateja Jamnik and Sean B. Holden},
Title = {Towards Graph Representation Learning in Emergent Communication},
Year = {2020},
Eprint = {arXiv:2001.09063},
url = {https://arxiv.org/abs/2001.09063},
}

@misc{keresztury2020compositional,
  IDS={2001.08618,Keresztury2020CompositionalPO},
Author = {Bence Keresztury and Elia Bruni},
Title = {Compositional properties of emergent languages in deep learning},
Year = {2020},
Eprint = {arXiv:2001.08618},
url = {https://arxiv.org/abs/2001.08618},
}

@misc{dagan2020co,
  IDS={2001.03361},
Author = {Gautier Dagan and Dieuwke Hupkes and Elia Bruni},
Title = {Co-evolution of language and agents in referential games},
Year = {2020},
Eprint = {arXiv:2001.03361},
url = {https://arxiv.org/abs/2001.03361},
}

@misc{a2020generalizing,
  IDS={unger2020GeneralizingEC},
Author = {Thomas A. Unger and Elia Bruni},
Title = {Generalizing Emergent Communication},
Year = {2020},
Eprint = {arXiv:2001.01772},
url = {https://arxiv.org/abs/2001.01772},
}

@misc{eccles2019biases,
Author = {Tom Eccles and Yoram Bachrach and Guy Lever and Angeliki Lazaridou and Thore Graepel},
Title = {Biases for Emergent Communication in Multi-agent Reinforcement Learning},
Year = {2019},
Eprint = {arXiv:1912.05676},
url = {https://arxiv.org/abs/1912.05676},
}

@misc{kolb2019learning,
Author = {Benjamin Kolb and Leon Lang and Henning Bartsch and Arwin Gansekoele and Raymond Koopmanschap and Leonardo Romor and David Speck and Mathijs Mul and Elia Bruni},
Title = {Learning to Request Guidance in Emergent Communication},
Year = {2019},
Eprint = {arXiv:1912.05525},
url = {https://arxiv.org/abs/1912.05525},
}

@misc{mihai2019avoiding,
  IDS={Mihai2019AvoidingHA,1911.05546},
Author = {Daniela Mihai and Jonathon Hare},
Title = {Avoiding hashing and encouraging visual semantics in referential emergent language games},
Year = {2019},
Eprint = {arXiv:1911.05546},
url = {https://arxiv.org/abs/1911.05546},
}

@misc{dessì2019focus,
  IDS={Dessi2019FocusOW,1911.01892},
Author = {Roberto Dessì and Diane Bouchacourt and Davide Crepaldi and Marco Baroni},
Title = {Focus on What's Informative and Ignore What's not: Communication Strategies in a Referential Game},
Year = {2019},
Eprint = {arXiv:1911.01892},
url = {https://arxiv.org/abs/1911.01892},
}

@misc{guo2019emergence,
  IDS={Guo2019EmergenceON},
Author = {Shangmin Guo},
Title = {Emergence of Numeric Concepts in Multi-Agent Autonomous Communication},
Year = {2019},
Eprint = {arXiv:1911.01098},
url = {https://arxiv.org/abs/1911.01098},
}

@misc{resnick2019capacity,
Author = {Cinjon Resnick and Abhinav Gupta and Jakob Foerster and Andrew M. Dai and Kyunghyun Cho},
Title = {Capacity, Bandwidth, and Compositionality in Emergent Language Learning},
Year = {2019},
Eprint = {arXiv:1910.11424},
url = {https://arxiv.org/abs/1910.11424},
}

@misc{korbak2019developmentally,
Author = {Tomasz Korbak and Julian Zubek and Łukasz Kuciński and Piotr Miłoś and Joanna Rączaszek-Leonardi},
Title = {Developmentally motivated emergence of compositional communication via template transfer},
Year = {2019},
Eprint = {arXiv:1910.06079},
url = {https://arxiv.org/abs/1910.06079},
}

@misc{guo2019the,
  IDS={Guo2019TheEO,1910.05291},
Author = {Shangmin Guo and Yi Ren and Serhii Havrylov and Stella Frank and Ivan Titov and Kenny Smith},
Title = {The Emergence of Compositional Languages for Numeric Concepts Through Iterated Learning in Neural Agents},
Year = {2019},
Eprint = {arXiv:1910.05291},
url = {https://arxiv.org/abs/1910.05291},
}

@misc{miletitch2019emergent,
Author = {Roman Miletitch and Andreagiovanni Reina and Marco Dorigo and Vito Trianni},
Title = {Emergent naming conventions in a foraging robot swarm},
Year = {2019},
Eprint = {arXiv:1910.02274},
url = {https://arxiv.org/abs/1910.02274},
}

@article{verma2019emergence,
Author = {Shresth Verma and Joydip Dhar},
Title = {Emergence of Writing Systems Through Multi-Agent Cooperation},
Year = {2019},
Eprint = {arXiv:1910.00741},
url = {https://arxiv.org/abs/1910.00741},
Doi = {10.1609/aaai.v34i10.7243},
}

@misc{steinert2019paying,
  IDS={1909.11060},
Author = {Shane Steinert-Threlkeld},
Title = {Paying Attention to Function Words},
Year = {2019},
Eprint = {arXiv:1909.11060},
url = {https://arxiv.org/abs/1909.11060},
}

@misc{lee2019countering,
Author = {Jason Lee and Kyunghyun Cho and Douwe Kiela},
Title = {Countering Language Drift via Visual Grounding},
Year = {2019},
Eprint = {arXiv:1909.04499},
url = {https://arxiv.org/abs/1909.04499},
}

@misc{mul2019mastering,
Author = {Mathijs Mul and Diane Bouchacourt and Elia Bruni},
Title = {Mastering emergent language: learning to guide in simulated navigation},
Year = {2019},
Eprint = {arXiv:1908.05135},
url = {https://arxiv.org/abs/1908.05135},
}

@misc{hagiwara2019symbol,
Author = {Yoshinobu Hagiwara and Hiroyoshi Kobayashi and Akira Taniguchi and Tadahiro Taniguchi},
Title = {Symbol Emergence as an Interpersonal Multimodal Categorization},
Year = {2019},
Eprint = {arXiv:1905.13443},
url = {https://arxiv.org/abs/1905.13443},
}

@misc{chaabouni2019word,
Author = {Rahma Chaabouni and Eugene Kharitonov and Alessandro Lazaric and Emmanuel Dupoux and Marco Baroni},
Title = {Word-order biases in deep-agent emergent communication},
Year = {2019},
Eprint = {arXiv:1905.12330},
url = {https://arxiv.org/abs/1905.12330},
}

@misc{bouchacourt2019miss,
Author = {Diane Bouchacourt and Marco Baroni},
Title = {Miss Tools and Mr Fruit: Emergent communication in agents learning about object affordances},
Year = {2019},
Eprint = {arXiv:1905.11871},
url = {https://arxiv.org/abs/1905.11871},
}

@misc{cogswell2019emergence,
Author = {Michael Cogswell and Jiasen Lu and Stefan Lee and Devi Parikh and Dhruv Batra},
Title = {Emergence of Compositional Language with Deep Generational Transmission},
Year = {2019},
Eprint = {arXiv:1904.09067},
url = {https://arxiv.org/abs/1904.09067},
}

@misc{hwang2019a,
Author = {Jungsik Hwang and Nadine Wirkuttis and Jun Tani},
Title = {A Neurorobotics Approach to Investigating the Emergence of Communication in Robots},
Year = {2019},
Eprint = {arXiv:1904.02858},
url = {https://arxiv.org/abs/1904.02858},
}

@misc{lowe2019on,
Author = {Ryan Lowe and Jakob Foerster and Y-Lan Boureau and Joelle Pineau and Yann Dauphin},
Title = {On the Pitfalls of Measuring Emergent Communication},
Year = {2019},
Eprint = {arXiv:1903.05168},
url = {https://arxiv.org/abs/1903.05168},
}

@article{degiuli2019emergence,
Author = {E. DeGiuli},
Title = {Emergence of order in random languages},
Year = {2019},
Eprint = {arXiv:1902.07516},
url = {https://arxiv.org/abs/1902.07516},
Doi = {10.1088/1751-8121/ab293c},
}

@misc{graesser2019emergent,
Author = {Laura Graesser and Kyunghyun Cho and Douwe Kiela},
Title = {Emergent Linguistic Phenomena in Multi-Agent Communication Games},
Year = {2019},
Eprint = {arXiv:1901.08706},
url = {https://arxiv.org/abs/1901.08706},
}

@misc{khomtchouk2018modeling,
  IDS={1812.01431,Khomtchouk2018ModelingNL},
Author = {Bohdan Khomtchouk and Shyam Sudhakaran},
Title = {Modeling natural language emergence with integral transform theory and reinforcement learning},
Year = {2018},
Eprint = {arXiv:1812.01431},
url = {https://arxiv.org/abs/1812.01431},
}

@misc{bogin2018emergence,
Author = {Ben Bogin and Mor Geva and Jonathan Berant},
Title = {Emergence of Communication in an Interactive World with Consistent Speakers},
Year = {2018},
Eprint = {arXiv:1809.00549},
url = {https://arxiv.org/abs/1809.00549},
}

@misc{bouchacourt2018how,
  IDS={1808.10696},
Author = {Diane Bouchacourt and Marco Baroni},
Title = {How agents see things: On visual representations in an emergent language game},
Year = {2018},
Eprint = {arXiv:1808.10696},
url = {https://arxiv.org/abs/1808.10696},
}

@misc{cao2018emergent,
Author = {Kris Cao and Angeliki Lazaridou and Marc Lanctot and Joel Z Leibo and Karl Tuyls and Stephen Clark},
Title = {Emergent Communication through Negotiation},
Year = {2018},
Eprint = {arXiv:1804.03980},
url = {https://arxiv.org/abs/1804.03980},
}

@misc{taniguchi2018symbol,
Author = {Tadahiro Taniguchi and Emre Ugur and Matej Hoffmann and Lorenzo Jamone and Takayuki Nagai and Benjamin Rosman and Toshihiko Matsuka and Naoto Iwahashi and Erhan Oztop and Justus Piater and Florentin Wörgötter},
Title = {Symbol Emergence in Cognitive Developmental Systems: a Survey},
Year = {2018},
Eprint = {arXiv:1801.08829},
url = {https://arxiv.org/abs/1801.08829},
}

@misc{lee2017emergent,
Author = {Jason Lee and Kyunghyun Cho and Jason Weston and Douwe Kiela},
Title = {Emergent Translation in Multi-Agent Communication},
Year = {2017},
Eprint = {arXiv:1710.06922},
url = {https://arxiv.org/abs/1710.06922},
}

@misc{evtimova2017emergent,
Author = {Katrina Evtimova and Andrew Drozdov and Douwe Kiela and Kyunghyun Cho},
Title = {Emergent Communication in a Multi-Modal, Multi-Step Referential Game},
Year = {2017},
Eprint = {arXiv:1705.10369},
url = {https://arxiv.org/abs/1705.10369},
}

@misc{shibata2017communications,
Author = {Katsunari Shibata},
Title = {Communications that Emerge through Reinforcement Learning Using a (Recurrent) Neural Network},
Year = {2017},
Eprint = {arXiv:1703.03543},
url = {https://arxiv.org/abs/1703.03543},
}

@misc{lazaridou2016multi,
  IDS={lazaridou2016multiagent,Lazaridou2017MultiAgentCA},
Author = {Angeliki Lazaridou and Alexander Peysakhovich and Marco Baroni},
Title = {Multi-Agent Cooperation and the Emergence of (Natural) Language},
Year = {2016},
Eprint = {arXiv:1612.07182},
url = {https://arxiv.org/abs/1612.07182},
}

@misc{taniguchi2015symbol,
Author = {Tadahiro Taniguchi and Takayuki Nagai and Tomoaki Nakamura and Naoto Iwahashi and Tetsuya Ogata and Hideki Asoh},
Title = {Symbol Emergence in Robotics: A Survey},
Year = {2015},
Eprint = {arXiv:1509.08973},
url = {https://arxiv.org/abs/1509.08973},
}

@misc{khorrami2019can,
Author = {Khazar Khorrami, Okko Räsänen},
Title = {Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation},
Year = {2019},
Eprint = {arXiv:2109.14200},
url = {https://arxiv.org/abs/2109.14200},
}

@misc{yuan2021emergence,
Author = {Luyao Yuan and Zipeng Fu and Linqi Zhou and Kexin Yang and Song-Chun Zhu},
Title = {Emergence of Theory of Mind Collaboration in Multiagent Systems},
Year = {2021},
Eprint = {arXiv:2110.00121},
url = {https://arxiv.org/abs/2110.00121},
}

@misc{long2022learning,
Author = {Yat Long Lo, Biswa Sengupta},
Title = {Learning to Ground Decentralized Multi-Agent Communication with Contrastive Learning},
Year = {2022},
Eprint = {arXiv:2203.03344},
url = {https://arxiv.org/abs/2203.03344},
}

@article{guo2021expressivity,
Author = {Shangmin Guo and Yi Ren and Kory Mathewson and Simon Kirby and Stefano V. Albrecht and Kenny Smith},
Title = {Expressivity of Emergent Language is a Trade-off between Contextual Complexity and Unpredictability},
Year = {2021},
Eprint = {arXiv:2106.03982},
url = {https://arxiv.org/abs/2106.03982},
Howpublished = {International Conference on Learning Representation 2022},
}

@article{vani2021iterated,
Author = {Ankit Vani and Max Schwarzer and Yuchen Lu and Eeshan Dhekane and Aaron Courville},
Title = {Iterated learning for emergent systematicity in VQA},
Year = {2021},
Eprint = {arXiv:2105.01119},
url = {https://arxiv.org/abs/2105.01119},
Howpublished = {9th International Conference on Learning Representations (ICLR
  2021)},
}

@article{cope2021learning,
Author = {Dylan Cope and Nandi Schoots},
Title = {Learning to Communicate with Strangers via Channel Randomisation Methods},
Year = {2021},
Eprint = {arXiv:2104.09557},
url = {https://arxiv.org/abs/2104.09557},
Howpublished = {4th Workshop on Emergent Communication at NeurIPS 2020},
}

@article{foguelman2020ebdevs,
Author = {Daniel J. Foguelman and Philipp Henning and Adelinde Uhrmacher and Rodrigo Castro},
Title = {EB-DEVS: A Formal Framework for Modeling and Simulation of Emergent Behavior in Dynamic Complex Systems},
Year = {2020},
Eprint = {arXiv:2010.05042},
url = {https://arxiv.org/abs/2010.05042},
Howpublished = {Journal of Computational Science Volume 53, July 2021, 101387},
Doi = {10.1016/j.jocs.2021.101387},
}

@article{chowdhury2020escell,
Author = {Aritra Chowdhury and James R. Kubricht and Anup Sood and Peter Tu and Alberto Santamaria-Pang},
Title = {ESCELL: Emergent Symbolic Cellular Language},
Year = {2020},
Eprint = {arXiv:2007.09469},
url = {https://arxiv.org/abs/2007.09469},
Howpublished = {2020 IEEE 17th International Symposium on Biomedical Imaging
  (ISBI), Iowa City, IA, USA, 2020, pp. 1604-1607},
Doi = {10.1109/ISBI45749.2020.9098343},
}

@article{luck2020coexistence,
Author = {Jean-Marc Luck and Anita Mehta},
Title = {On the coexistence of competing languages},
Year = {2020},
Eprint = {arXiv:2003.04748},
url = {https://arxiv.org/abs/2003.04748},
Howpublished = {Eur. Phys. J. B (2020) 93, 73},
Doi = {10.1140/epjb/e2020-10038-1},
}

@article{moulinfrier2020multiagent,
  IDS={MoulinFrier2020MultiAgentRL},
Author = {Clément Moulin-Frier and Pierre-Yves Oudeyer},
Title = {Multi-Agent Reinforcement Learning as a Computational Tool for Language Evolution Research: Historical Context and Future Challenges},
Year = {2020},
Eprint = {arXiv:2002.08878},
url = {https://arxiv.org/abs/2002.08878},
Howpublished = {Challenges and Opportunities for Multi-Agent Reinforcement
  Learning (COMARL AAAI 2020-2021), AAAI Spring Symposium Series, Stanford
  University, Palo Alto, California, USA},
}

@article{ren2020compositional,
  IDS={Ren2020CompositionalLE,2002.01365},
Author = {Yi Ren and Shangmin Guo and Matthieu Labeau and Shay B. Cohen and Simon Kirby},
Title = {Compositional Languages Emerge in a Neural Iterated Learning Model},
Year = {2020},
Eprint = {arXiv:2002.01365},
url = {https://arxiv.org/abs/2002.01365},
Howpublished = {ICLR-2020},
}

@article{yuan2020emergence,
  Author = {Luyao Yuan and Zipeng Fu and Jingyue Shen and Lu Xu and Junhong Shen and Song-Chun Zhu},
  Title = {Emergence of Pragmatics from Referential Game between Theory of Mind Agents},
  Year = {2020},
  Eprint = {arXiv:2001.07752},
  url = {https://arxiv.org/abs/2001.07752},
  Howpublished = {Emergent Communication Workshop, 33rd Conference on Neural
    Information Processing Systems (NeurIPS 2019)},
}

@article{lipowska2018emergence,
  Author = {Dorota Lipowska and Adam Lipowski},
  Title = {Emergence of linguistic conventions in multi-agent reinforcement learning},
  Year = {2018},
  Eprint = {arXiv:1811.07208},
  url = {https://arxiv.org/abs/1811.07208},
  Howpublished = {PLoS ONE 13(11): e0208095 (2018)},
  Doi = {10.1371/journal.pone.0208095},
}

@article{xu2022compositional,
  IDS={xu2022compositionalgeneralizationunsupervisedcompositional},
  title={Compositional generalization in unsupervised compositional representation learning: A study on disentanglement and emergent language},
  author={Xu, Zhenlin and Niethammer, Marc and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25074--25087},
  year={2022}
}

@inproceedings{rita2022emergent,
  IDS={2209.15342},
  title={Emergent Communication: Generalization and Overfitting in Lewis Games},
  author={Mathieu Rita and Corentin Tallec and Paul Michel and Jean-Bastien Grill and Olivier Pietquin and Emmanuel Dupoux and Florian Strub},
  booktitle={Advances in Neural Information Processing Systems},
  editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year={2022},
  url={https://openreview.net/forum?id=qqHMvHbfu6}
}

@misc{denamganai2023visual,
  Author = {Kevin Denamganaï and Sondess Missaoui and James Alfred Walker},
  Title = {Visual Referential Games Further the Emergence of Disentangled Representations},
  Year = {2023},
  Eprint = {arXiv:2304.14511},
  url = {https://arxiv.org/abs/2304.14511},
}

@misc{chen2023emergence,
  Author = {Yang Chen and Liangxuan Guo and Shan Yu},
  Title = {Emergence of Symbols in Neural Networks for Semantic Understanding and Communication},
  Year = {2023},
  Eprint = {arXiv:2304.06377},
  url = {https://arxiv.org/abs/2304.06377},
}

@misc{zubek2023models,
  Author = {Julian Zubek and Tomasz Korbak and Joanna Rączaszek-Leonardi},
  Title = {Models of symbol emergence in communication: a conceptual review and a guide for avoiding local minima},
  Year = {2023},
  Eprint = {arXiv:2303.04544},
  url = {https://arxiv.org/abs/2303.04544},
}

@misc{karten2023on,
  Author = {Seth Karten and Siva Kailas and Huao Li and Katia Sycara},
  Title = {On the Role of Emergent Communication for Social Learning in Multi-Agent Reinforcement Learning},
  Year = {2023},
  Eprint = {arXiv:2302.14276},
  url = {https://arxiv.org/abs/2302.14276},
}

@misc{mahaut2023referential,
  IDS={2302.08913},
  Author = {Matéo Mahaut and Francesca Franzon and Roberto Dessì and Marco Baroni},
  Title = {Referential communication in heterogeneous communities of pre-trained visual deep networks},
  Year = {2023},
  Eprint = {arXiv:2302.08913},
  url = {https://arxiv.org/abs/2302.08913},
}

@misc{feng2023learning,
  Author = {Yicheng Feng and Boshi An and Zongqing Lu},
  Title = {Learning Multi-Object Positional Relationships via Emergent Communication},
  Year = {2023},
  Eprint = {arXiv:2302.08084},
  url = {https://arxiv.org/abs/2302.08084},
}

@misc{piazza2023theory,
  Author = {Nancirose Piazza and Vahid Behzadan},
  Title = {A Theory of Mind Approach as Test-Time Mitigation Against Emergent Adversarial Communication},
  Year = {2023},
  Eprint = {arXiv:2302.07176},
  url = {https://arxiv.org/abs/2302.07176},
}

@misc{masquil2022intrinsically,
  Author = {Elías Masquil and Gautier Hamon and Eleni Nisioti and Clément Moulin-Frier},
  Title = {Intrinsically-Motivated Goal-Conditioned Reinforcement Learning in Multi-Agent Environments},
  Year = {2022},
  Eprint = {arXiv:2211.06082},
  url = {https://arxiv.org/abs/2211.06082},
}

@misc{carmeli2022emergent,
  IDS={2211.02412},
  Author = {Boaz Carmeli and Ron Meir and Yonatan Belinkov},
  Title = {Emergent Quantized Communication},
  Year = {2022},
  Eprint = {arXiv:2211.02412},
  url = {https://arxiv.org/abs/2211.02412},
}

@misc{kalinowska2022over,
  Author = {Aleksandra Kalinowska and Elnaz Davoodi and Florian Strub and Kory W Mathewson and Ivana Kajic and Michael Bowling and Todd D Murphey and Patrick M Pilarski},
  Title = {Over-communicate no more: Situated RL agents learn concise communication protocols},
  Year = {2022},
  Eprint = {arXiv:2211.01480},
  url = {https://arxiv.org/abs/2211.01480},
}

@misc{fulker2022spontaneous,
  Author = {Zachary Fulker and Patrick Forber and Rory Smead and Christoph Riedl},
  Title = {Spontaneous emergence of groups and signaling diversity in dynamic networks},
  Year = {2022},
  Eprint = {arXiv:2210.17309},
  url = {https://arxiv.org/abs/2210.17309},
}

@misc{thomas2022neuro,
  Author = {Christo Kurisummoottil Thomas and Walid Saad},
  Title = {Neuro-Symbolic Causal Reasoning Meets Signaling Game for Emergent Semantic Communications},
  Year = {2022},
  Eprint = {arXiv:2210.12040},
  url = {https://arxiv.org/abs/2210.12040},
}

@misc{karch2022contrastive,
  Author = {Tristan Karch and Yoann Lemesle and Romain Laroche and Clément Moulin-Frier and Pierre-Yves Oudeyer},
  Title = {Contrastive Multimodal Learning for Emergence of Graphical Sensory-Motor Communication},
  Year = {2022},
  Eprint = {arXiv:2210.06468},
  url = {https://arxiv.org/abs/2210.06468},
}

@misc{lazaridou2018emergence,
  IDS={lazaridou2018referential,lazaridou2018EmergenceOL},
      title={Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input}, 
      author={Angeliki Lazaridou and Karl Moritz Hermann and Karl Tuyls and Stephen Clark},
      year={2018},
      eprint={1804.03984},
      url = {https://arxiv.org/abs/1804.03984},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@article{baker1985mirror,
 ISSN = {00243892, 15309150},
 URL = {http://www.jstor.org/stable/4178442},
 author = {Mark Baker},
 journal = {Linguistic Inquiry},
 number = {3},
 pages = {373--415},
 publisher = {The MIT Press},
 title = {The Mirror Principle and Morphosyntactic Explanation},
 urldate = {2023-08-21},
 volume = {16},
 year = {1985}
}

@book{Bybee1985MorphologyAS,
  title={Morphology: A study of the relation between meaning and form},
  author={Joan L. Bybee},
  year={1985},
  publisher={John Benjamins},
}

@book{flemming2013auditory,
  title={Auditory representations in phonology},
  author={Flemming, Edward S},
  year={2013},
  publisher={Routledge}
}

@book{blevins2004evolutionary,
  title={Evolutionary phonology: The emergence of sound patterns},
  author={Blevins, Juliette},
  year={2004},
  publisher={Cambridge University Press}
}

@Article{roberts2017linguists,
  author = 	 {Roberts, Gareth},
  title = 	 {The linguist's {Drosophila}: Experiments in language change},
  journal = 	 {Linguistics Vanguard},
  year = 	 {2017},
  volume =	 {2017},
  pages =	 {1--13}
}

@InCollection{weinreich1968empirical,
  author = 	 {Weinreich, Uriel and Labov, William and Herzog, Marvin},
  title = 	 {Empirical foundations for a theory of language change},
  booktitle = 	 {Directions for Historical Linguistics},
  publisher =	 {University of Texas Press},
  year =	 {1968},
  editor =	 {Lehman, Winfred},
  pages =	 {95--88},
  address =	 {Austin}
}

@article{Smith_Kirby_Brighton_2003, title={Iterated Learning: A Framework for the Emergence of Language}, volume={9}, ISSN={1064-5462}, DOI={10.1162/106454603322694825}, abstractNote={Language is culturally transmitted. Iterated learning, the process by which the output of one individual’s learning becomes the input to other individuals’ learning, provides a framework for investigating the cultural evolution of linguistic structure. We present two models, based upon the iterated learning framework, which show that the poverty of the stimulus available to language learners leads to the emergence of linguistic structure. Compositionality is language’s adaptation to stimulus poverty.}, number={4}, journal={Artificial Life}, publisher={MIT Press}, author={Smith, Kenny and Kirby, Simon and Brighton, Henry}, year={2003}, month={Oct}, pages={371–386} }

@article{resnick_capacity_2020,
	title = {Capacity, {Bandwidth}, and {Compositionality} in {Emergent} {Language} {Learning}},
	url = {http://arxiv.org/abs/1910.11424},
	urldate = {2020-06-01},
	journal = {International Conference on Autonomous Agents and Multi-Agent Systems},
	author = {Resnick, Cinjon and Gupta, Abhinav and Foerster, Jakob and Dai, Andrew M. and Cho, Kyunghyun},
	month = apr,
	year = {2020},
}

@article{goldwater2011pitmanyor,
  title={Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models},
  author={Sharon Goldwater and Thomas L. Griffiths and Mark Johnson},
  journal={J. Mach. Learn. Res.},
  year={2011},
  volume={12},
  pages={2335-2382}
}

@ARTICLE{shannon,  author={Shannon, C. E.},  journal={The Bell System Technical Journal},   title={A mathematical theory of communication},   year={1948},  volume={27},  number={3},  pages={379-423},  doi={10.1002/j.1538-7305.1948.tb01338.x}}

@misc{hazra2021zeroshot,
  title = {Zero-Shot Generalization using Intrinsically Motivated Compositional Emergent Protocols},
  author = {Rishi Hazra and Sonu Dixit and Sayambhu Sen},
  howpublished = {Visually Grounded Interaction and Language Workshop, NAACL},
  year         = 2021,
}

@misc{bullard2021quasiequivalence,
  IDS={Bullard2021QuasiEquivalenceDF,2103.08067,bullard2021quasi},
    title={Quasi-Equivalence Discovery for Zero-Shot Emergent Communication},
    author={Kalesha Bullard and Douwe Kiela and Franziska Meier and Joelle Pineau and Jakob Foerster},
    year={2021},
    eprint={2103.08067},
    archivePrefix={arXiv},
    primaryClass={cs.MA}
}

@Inbook{Wiewiora2010,
author="Wiewiora, Eric",
editor="Sammut, Claude
and Webb, Geoffrey I.",
title="Reward Shaping",
bookTitle="Encyclopedia of Machine Learning",
year="2010",
publisher="Springer US",
address="Boston, MA",
pages="863--865",
isbn="978-0-387-30164-8",
doi="10.1007/978-0-387-30164-8_731",
url="https://doi.org/10.1007/978-0-387-30164-8_731"
}

@article{schulman2017ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  archivePrefix={arXiv},
  eprint={1707.06347},
  year={2017}
}

@misc{blei2007crp,
    author = {Blei, David},
    title = {The Chinese Restaurant Process},
    url = {https://www.cs.princeton.edu/courses/archive/fall07/cos597C/scribe/20070921.pdf},
    year = {2007},
}

@InProceedings{aldous1985exchangeability,
author="Aldous, David J.",
editor="Hennequin, P. L.",
title="Exchangeability and related topics",
booktitle="{\'E}cole d'{\'E}t{\'e} de Probabilit{\'e}s de Saint-Flour XIII --- 1983",
year="1985",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--198",
isbn="978-3-540-39316-0"
}

@inproceedings{tomlin2018incremental,
  title={Incremental pragmatics and emergent communication},
  author={Tomlin, Nicholas and Pavlick, Ellie},
  booktitle={Neural Information Processing Systems Workshop on Emergent Communication},
  year={2018}
}

@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@inproceedings{kingma2015adam,
  author={Diederik P. Kingma and Jimmy Ba},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  cdate={1420070400000},
  url={http://arxiv.org/abs/1412.6980},
  booktitle={ICLR (Poster)},
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@misc{stable-baselines3,
  author = {Raffin, Antonin and Hill, Ashley and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Dormann, Noah},
  title = {Stable Baselines3},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/stable-baselines3}},
}

@article{vogel1979sunflower,
title = {A better way to construct the sunflower head},
journal = {Mathematical Biosciences},
volume = {44},
number = {3},
pages = {179-189},
year = {1979},
issn = {0025-5564},
doi = {https://doi.org/10.1016/0025-5564(79)90080-4},
url = {https://www.sciencedirect.com/science/article/pii/0025556479900804},
author = {Helmut Vogel},
}

@misc{arumugam2020information,
  title = {An Information-Theoretic Perspective on Credit Assignment in Reinforcement Learning},
  author = {Arumugam, Dilip and Henderson, Peter and Bacon, Pierre-Luc},
  howpublished = {Biological and Artificial Reinforcement Learning Workshop, NeurIPS},
  year         = 2020,
}

@inproceedings{trott2019keeping,
 author = {Trott, Alexander and Zheng, Stephan and Xiong, Caiming and Socher, Richard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards},
 url = {https://proceedings.neurips.cc/paper/2019/file/64c26b2a2dcf068c49894bd07e0e6389-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{
    amiranashvili2018analyzing,
    title={Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning},
    author={Artemij Amiranashvili and Alexey Dosovitskiy and Vladlen Koltun and Thomas Brox},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=HyiAuyb0b},
}

@inproceedings{lu2020seedediterated,
  title={Countering language drift with seeded iterated learning},
  author={Lu, Yuchen and Singhal, Soumye and Strub, Florian and Courville, Aaron and Pietquin, Olivier},
  booktitle={International Conference on Machine Learning},
  pages={6437--6447},
  year={2020},
  organization={PMLR}
}

@InProceedings{fujimoto2019offpolicy, title = {Off-Policy Deep Reinforcement Learning without Exploration}, author = {Fujimoto, Scott and Meger, David and Precup, Doina}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {2052--2062}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/fujimoto19a/fujimoto19a.pdf}, url = { http://proceedings.mlr.press/v97/fujimoto19a.html }}

@inproceedings{kimdambi2020morel,
 author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {21810--21823},
 publisher = {Curran Associates, Inc.},
 title = {MOReL: Model-Based Offline Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{agarwal2020optimistic,
  title={An optimistic perspective on offline reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@InProceedings{lu2020countering,
    title = {Countering Language Drift with Seeded Iterated Learning}, author = {Lu, Yuchen and Singhal, Soumye and Strub, Florian and Courville, Aaron and Pietquin, Olivier}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {6437--6447}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, address = {Virtual}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/lu20c/lu20c.pdf}, url = {http://proceedings.mlr.press/v119/lu20c.html}
}

@Book{berlin1969basic,
 author = {Berlin, Brent and Kay, Paul},
 title = {Basic Color Terms : Their Universality and Evolution},
 publisher = {University of California Press},
 year = {1969},
 address = {Berkeley, Calif},
 isbn = {0520014421}
}

@inproceedings{havrylov2017advances,
  IDS={havrylov2017emergence,Havrylov2017EmergenceOL,havrylov2017sequence},
 author = {Havrylov, Serhii and Titov, Ivan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {2149--2159},
 publisher = {Curran Associates, Inc.},
 title = {Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols},
 url = {https://proceedings.neurips.cc/paper/2017/file/70222949cc0db89ab32c9969754d4758-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{kajic2020learning,
    title={Learning to cooperate: Emergent communication in multi-agent navigation},
    author={Ivana Kaji\'c and Eser Ayg\"un and Doina Precup},
    booktitle={42nd Annual Meeting of the Cognitive Science Society},
    year={2020},
    pages={1993--1999},
    publisher={Cognitive Science Society},
    address={Toronto, ON},
    pdf={http://compneuro.uwaterloo.ca/files/publications/kajic.2020.pdf},
    abstract={
              Emergent communication in artificial agents has been studied to
              understand language evolution, as well as to develop artificial
              systems that learn to communicate with humans. We show that
              agents performing a cooperative navigation task in various
              gridworld environments learn an interpretable communication
              protocol that enables them to efficiently, and in many cases,
              optimally, solve the task. An analysis of the agents' policies
              reveals that emergent signals spatially cluster the state space,
              with signals referring to specific locations and spatial
              directions such as left, up, or upper left
              room. Using populations of agents, we show that the emergent
    protocol has basic compositional structure, thus exhibiting a core property
    of natural language.}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{Francis2021,
    author = {Francis, David and Rabinovich, Ella and Samir, Farhan and Mortensen, David R. and Stevenson, Suzanne},
    title = "{Quantifying Cognitive Factors in Lexical Decline}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {1529-1545},
    year = {2021},
    month = {12},
    abstract = "{We adopt an evolutionary view on language change in which cognitive factors (in addition to social ones) affect the fitness of words and their success in the linguistic ecosystem. Specifically, we propose a variety of psycholinguistic factors—semantic, distributional, and phonological—that we hypothesize are predictive of lexical decline, in which words greatly decrease in frequency over time. Using historical data across three languages (English, French, and German), we find that most of our proposed factors show a significant difference in the expected direction between each curated set of declining words and their matched stable words. Moreover, logistic regression analyses show that semantic and distributional factors are significant in predicting declining words. Further diachronic analysis reveals that declining words tend to decrease in the diversity of their lexical contexts over time, gradually narrowing their ‘ecological niches’.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00441},
    url = {https://doi.org/10.1162/tacl\_a\_00441},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00441/1979747/tacl\_a\_00441.pdf},
}

@article{andreas_measuring_2019,
	title = {Measuring {Compositionality} in {Representation} {Learning}},
	url = {http://arxiv.org/abs/1902.07181},
	urldate = {2021-12-23},
	archivePrefix = {arXiv},
	eprint = {1902.07181},
	author = {Andreas, Jacob},
	month = apr,
	year = {2019},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/2KHXETYI/Andreas - 2019 - Measuring Compositionality in Representation Learn.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/AIY9DBY2/1902.html:text/html},
}

@inproceedings{
baker2020emergent,
title={Emergent Tool Use From Multi-Agent Autocurricula},
author={Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkxpxJBKwS}
}

@misc{devaux_list_2021,
	title = {The list of synthetic data companies — 2021},
	url = {https://elise-deux.medium.com/the-list-of-synthetic-data-companies-2021-5aa246265b42},
	abstract = {Are you looking for a synthetic data company? Check out this list of synthetic data vendors.},
	language = {en},
	urldate = {2022-07-25},
	journal = {Medium},
	author = {Devaux, Elise},
	month = oct,
	year = {2021},
	file = {Snapshot:/home/brendon/academic/misc/Zotero/storage/RVFZKP97/the-list-of-synthetic-data-companies-2021-5aa246265b42.html:text/html},
}

@inproceedings{Andreas2018LearningWL,
  title={Learning with Latent Language},
  author={Jacob Andreas and Dan Klein and Sergey Levine},
  booktitle={NAACL},
  year={2018}
}

@misc{Singh2019LearningWT,
  title={Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks},
  author={Amanpreet Singh and Tushar Jain and Sainbayar Sukhbaatar},
  archivePrefix={arXiv},
  year={2019},
  eprint={1812.09755},
}

@misc{Das2019TarMACTM,
  title={TarMAC: Targeted Multi-Agent Communication},
  author={Abhishek Das and Th{\'e}ophile Gervet and Joshua Romoff and Dhruv Batra and Devi Parikh and Michael G. Rabbat and Joelle Pineau},
  archivePrefix={arXiv},
  year={2019},
  eprint={1810.11187},
}

@conference{dekker2020contact,
  title = "Neural Agent-based Models To Study Language Contact Using Linguistic Data",
  keywords = "agent-based models, neural networks, emergent communication, language change, language contact, deep learning",
  author = "Peter Dekker and {De Boer}, Bart",
  year = "2020",
  month = "12",
  day = "12",
  language = "English",
  pages = "1--6",
  note = "4th NeurIPS Workshop on Emergent Communication : Talking to Strangers: Zero-Shot Emergent Communication ; Conference date: 12-12-2020 Through 12-12-2020",
  url = "https://sites.google.com/view/emecom2020/home",
}

@inproceedings{
choi2018multiagent,
title={Multi-Agent Compositional Communication Learning from Raw Visual Input},
author={Edward Choi and Angeliki Lazaridou and Nando de Freitas},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rknt2Be0-},
}

@inproceedings{
herrmann2022sifting,
title={Sifting the Signal from the Noise},
author={Daniel Alexander Herrmann and Jacob VanDrunen},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=rcUeWQbRQb5}
}

@inproceedings{
ossenkopf2022which,
title={Which Language Evolves Between Heterogeneous Agents? - Communicating Movement Instructions With Widely Different Time Scopes},
author={Marie Ossenkopf and Kevin Sebastian Luck and Kory Wallace Mathewson},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=BnfgM7-0mW5}
}

@article{ha2018worldmodels,
  doi = {10.5281/ZENODO.1207631},
  url = {https://zenodo.org/record/1207631},
  author = {Ha, David and Schmidhuber, Jürgen},
  title = {World Models},
  publisher = {Zenodo},
  year = {2018},
  copyright = {Creative Commons Attribution 4.0}
}

@inproceedings{
cope2022joining,
title={Joining the Conversation: Towards Language Acquisition for Ad Hoc Team Play},
author={Dylan Cope and Peter McBurney},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=SLqgf7ZCQbq}
}

@inproceedings{
lo2022learning,
title={Learning To Ground Decentralized Multi-Agent Communication with Contrastive Learning},
author={Yat Long Lo and Biswa Sengupta},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=rLceWXWCmZc}
}

@article{Kirby2008CumulativeCE,
  title={Cumulative cultural evolution in the laboratory: An experimental approach to the origins of structure in human language},
  author={Simon Kirby and Hannah Cornish and Kenny Smith},
  journal={Proceedings of the National Academy of Sciences},
  year={2008},
  volume={105},
  pages={10681 - 10686}
}

@inbook{kirby_2000, place={Cambridge}, title={Syntax Without Natural Selection: How Compositionality Emerges from Vocabulary in a Population of Learners}, DOI={10.1017/CBO9780511606441.019}, booktitle={The Evolutionary Emergence of Language: Social Function and the Origins of Linguistic Form}, publisher={Cambridge University Press}, author={Kirby, Simon}, editor={Knight, Chris and Studdert-Kennedy, Michael and Hurford, JamesEditors}, year={2000}, pages={303–323}}

@article{rogers2021bertology,
    author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
    title = "{A Primer in BERTology: What We Know About How BERT Works}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {842-866},
    year = {2021},
    month = {01},
    abstract = "{Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00349},
    url = {https://doi.org/10.1162/tacl\_a\_00349},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00349/1923281/tacl\_a\_00349.pdf},
}

@inproceedings{kucinski2020emergence,
  IDS={Kuciski2020EmergenceOC},
  title={Emergence of compositional language in communication through noisy channel},
  author={{\L}ukasz Kuci{\'n}ski and Pawe{\l} Ko{\l}odziej and Piotr Mi{\l}o{\'s}},
  booktitle={Language in Reinforcement Learning Workshop at ICML 2020},
  year={2020},
  url={https://openreview.net/forum?id=ZbXlSL_xwtA}
}

@article{kriegeskorte2008rsa,
  author={Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter},
  title={Representational similarity analysis - connecting the branches of systems neuroscience},
  journal={Frontiers in Systems Neuroscience},
  volume={2},
  year={2008},
  url={https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008},
  doi={10.3389/neuro.06.004.2008},
  issn={1662-5137},
}

@article{Silver2017MasteringTG,
  IDS={silver2017mastering,silver2017masteringchessshogiselfplay},
  title={Mastering the game of Go without human knowledge},
  author={David Silver and Julian Schrittwieser and Karen Simonyan and Ioannis Antonoglou and Aja Huang and Arthur Guez and Thomas Hubert and Lucas baker and Matthew Lai and Adrian Bolton and Yutian Chen and Timothy P. Lillicrap and Fan Hui and L. Sifre and George van den Driessche and Thore Graepel and Demis Hassabis},
  journal={Nature},
  year={2017},
  volume={550},
  pages={354-359}
}

@article{parry_2001, title={Triangle centers and central triangles, by Clark Kimberling (Congress Numerantium Vol. 129) Pp. 295. $42.50 1998. ISSN 0316-1282 (Utilitas Mathematica Publishing, Inc., Winnipeg).}, volume={85}, DOI={10.2307/3620531}, number={502}, journal={The Mathematical Gazette}, publisher={Cambridge University Press}, author={Parry, C. F.}, year={2001}, pages={172–173}}

@misc{openaigym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
  url = {https://arxiv.org/abs/1606.01540},
}

@misc{lipowski2008computational,
  doi = {10.48550/ARXIV.0801.1658},
  url = {https://arxiv.org/abs/0801.1658},
  author = {Lipowski, Adam and Lipowska, Dorota},
  keywords = {Physics and Society (physics.soc-ph), Computation and Language (cs.CL), Multiagent Systems (cs.MA), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Computational approach to the emergence and evolution of language - evolutionary naming game model},
  publisher = {arXiv},
  year = {2008},
  copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004}
}

@Inbook{kirby2002lingstruct,
  author="Kirby, Simon
  and Hurford, James R.",
  editor="Cangelosi, Angelo
  and Parisi, Domenico",
  title="The Emergence of Linguistic Structure: An Overview of the Iterated Learning Model",
  bookTitle="Simulating the Evolution of Language",
  year="2002",
  publisher="Springer London",
  address="London",
  pages="121--147",
  abstract="As language users humans possess a culturally transmitted system of unparalleled complexity in the natural world. Linguistics has revealed over the past 40 years the degree to which the syntactic structure of language in particular is strikingly complex. Furthermore, as Pinker and Bloom point out in their agenda-setting paper Natural Language and Natural Selection ``grammar is a complex mechanism tailored to the transmission of propositional structures through a serial interface'' (Pinker and Bloom, 1990: 707). These sorts of observations, along with influential arguments from linguistics and psychology about the innateness of language (see Chomsky, 1986; Pinker, 1994), have led many authors to the conclusion that an explanation for the origin of syntax must invoke neo-Darwinian natural selection.",
  isbn="978-1-4471-0663-0",
  doi="10.1007/978-1-4471-0663-0_6",
  url="https://doi.org/10.1007/978-1-4471-0663-0_6"
}

@article{kegl1999creation,
  title={Creation through contact: Sign language emergence and sign language change in Nicaragua},
  author={Kegl, Judy and Senghas, Ann and others},
  journal={Language creation and language change: Creolization, diachrony, and development},
  pages={179--237},
  year={1999}
}

@article{kirby2008cumulative,
  title={Cumulative cultural evolution in the laboratory: An experimental approach to the origins of structure in human language},
  author={Kirby, Simon and Cornish, Hannah and Smith, Kenny},
  journal={Proceedings of the National Academy of Sciences},
  volume={105},
  number={31},
  pages={10681--10686},
  year={2008},
  publisher={National Acad Sciences}
}

@article{
kirby2007innateness,
author = {Simon Kirby  and Mike Dowman  and Thomas L. Griffiths },
title = {Innateness and culture in the evolution of language},
journal = {Proceedings of the National Academy of Sciences},
volume = {104},
number = {12},
pages = {5241-5245},
year = {2007},
doi = {10.1073/pnas.0608222104},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0608222104},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0608222104},
abstract = {Human language arises from biological evolution, individual learning, and cultural transmission, but the interaction of these three processes has not been widely studied. We set out a formal framework for analyzing cultural transmission, which allows us to investigate how innate learning biases are related to universal properties of language. We show that cultural transmission can magnify weak biases into strong linguistic universals, undermining one of the arguments for strong innate constraints on language learning. As a consequence, the strength of innate biases can be shielded from natural selection, allowing these genes to drift. Furthermore, even when there is no natural selection, cultural transmission can produce apparent adaptations. Cultural transmission thus provides an alternative to traditional nativist and adaptationist explanations for the properties of human languages.}}

@article{Kgebck2018DeepColorRL,
  title={DeepColor: Reinforcement Learning optimizes information efficiency and well-formedness in color name partitioning},
  author={Mikael K{\aa}geb{\"a}ck and Devdatt P. Dubhashi and Asad B. Sayeed},
  journal={Cognitive Science},
  year={2018}
}

@article{Zipf1949HumanBA,
  IDS={zipf,zipf1949least},
title = {Human behavior and the principle of least effort},
  publisher={Addison-Wesley},
  address={Cambridge, (Mass.)},
journal = {Journal of Clinical Psychology},
author = {Zipf, George K.},
volume = {6},
number = {3},
pages = {306-306},
doi = {https://doi.org/10.1002/1097-4679(195007)6:3<306::AID-JCLP2270060331>3.0.CO;2-7},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1097-4679%28195007%296%3A3%3C306%3A%3AAID-JCLP2270060331%3E3.0.CO%3B2-7},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/1097-4679%28195007%296%3A3%3C306%3A%3AAID-JCLP2270060331%3E3.0.CO%3B2-7},
year = {1950}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report},
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      url={https://arxiv.org/abs/2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@phdthesis{naikthesis,
  title = {Adapting to the Long Tail in Language Understanding},
  school = {Carnegie Mellon University},
  author = {Aakanksha Naik},
  year = {2022},
  url = {https://www.lti.cs.cmu.edu/sites/default/files/naik%2C%20aakanksha%20-%20Thesis_1.pdf},
}

@misc{Misra2021DoLM,
  title={Do language models learn typicality judgments from text?},
  author={Kanishka Misra and Allyson Ettinger and Julia Taylor Rayz},
  archivePrefix={arXiv},
  year={2021},
  eprint={2105.02987},
}

@article {Schrimpf2020ArtificialNN,
	author = {Martin Schrimpf and Idan Blank and Greta Tuckute and Carina Kauf and Eghbal A. Hosseini and Nancy Kanwisher and Joshua Tenenbaum and Evelina Fedorenko},
	title = {Artificial Neural Networks Accurately Predict Language Processing in the Brain},
	elocation-id = {2020.06.26.174482},
	year = {2020},
	doi = {10.1101/2020.06.26.174482},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {The ability to share ideas through language is our species{\textquoteright} signature cognitive skill, but how this feat is achieved by the brain remains unknown. Inspired by the success of artificial neural networks (ANNs) in explaining neural responses in perceptual tasks (Kell et al., 2018; Khaligh-Razavi \&amp; Kriegeskorte, 2014; Schrimpf et al., 2018; Yamins et al., 2014; Zhuang et al., 2017), we here investigated whether state-of-the-art ANN language models (e.g. Devlin et al., 2018; Pennington et al., 2014; Radford et al., 2019) capture human brain activity elicited during language comprehension. We tested 43 language models spanning major current model classes on three neural datasets (including neuroimaging and intracranial recordings) and found that the most powerful generative transformer models (Radford et al., 2019) accurately predict neural responses, in some cases achieving near-perfect predictivity relative to the noise ceiling. In contrast, simpler word-based embedding models (e.g. Pennington et al., 2014) only poorly predict neural responses (\&lt;10\% predictivity). Models{\textquoteright} predictivities are consistent across neural datasets, and also correlate with their success on a next-word-prediction task (but not other language tasks) and ability to explain human comprehension difficulty in an independent behavioral dataset. Intriguingly, model architecture alone drives a large portion of brain predictivity, with each model{\textquoteright}s untrained score predictive of its trained score. These results support the hypothesis that a drive to predict future inputs may shape human language processing, and perhaps the way knowledge of language is learned and organized in the brain. In addition, the finding of strong correspondences between ANNs and human representations opens the door to using the growing suite of tools for neural network interpretation to test hypotheses about the human mind.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2020/06/27/2020.06.26.174482},
	eprint = {https://www.biorxiv.org/content/early/2020/06/27/2020.06.26.174482.full.pdf},
	journal = {bioRxiv}
}

@article{mahowald2023dissociating,
  title={Dissociating language and thought in large language models: a cognitive perspective},
  author={Mahowald, Kyle and Ivanova, Anna A and Blank, Idan A and Kanwisher, Nancy and Tenenbaum, Joshua B and Fedorenko, Evelina},
  eprint={2301.06627},
  archivePrefix={arXiv},
  year={2023}
}

@book{pollard1994head,
  title={Head-driven phrase structure grammar},
  author={Pollard, Carl and Sag, Ivan A},
  year={1994},
  publisher={University of Chicago Press}
}

@Inbook{Lindblom1990,
author="Lindblom, B.",
editor="Hardcastle, William J.
and Marchal, Alain",
title="Explaining Phonetic Variation: A Sketch of the H{\&}H Theory",
bookTitle="Speech Production and Speech Modelling",
year="1990",
publisher="Springer Netherlands",
address="Dordrecht",
pages="403--439",
abstract="The H{\&}H theory is developed from evidence showing that speaking and listening are shaped by biologically general processes. Speech production is adaptive. Speakers can, and typically do, tune their performance according to communicative and situational demands, controlling the interplay between production-oriented factors on the one hand, and output-oriented constraints on the other. For the ideal speaker, H{\&}H claims that such adaptations reflect his tacit awareness of the listener's access to sources of information independent of the signal and his judgement of the short-term demands for explicit signal information. Hence speakers are expected to vary their output along a continuum of hyper- and hypospeech. The theory suggests that the lack of invariance that speech signals commonly exhibit (Perkell and Klatt 1986) is a direct consequence of this adaptive organization (cf MacNeilage 1970). Accordingly, in the H{\&}H program the quest for phonetic invariance is replaced by another research task: Explicating the notion of sufficient discriminability and defining the class of speech signals that meet that criterion.",
isbn="978-94-009-2037-8",
doi="10.1007/978-94-009-2037-8_16",
url="https://doi.org/10.1007/978-94-009-2037-8_16"
}

@article{ullman_2001, title={The neural basis of lexicon and grammar in first and second language: the declarative/procedural model}, volume={4}, DOI={10.1017/S1366728901000220}, number={2}, journal={Bilingualism: Language and Cognition}, publisher={Cambridge University Press}, author={Ullman, Michael T.}, year={2001}, pages={105–122}}

@article{Ullman2001ANP,
  title={A neurocognitive perspective on language: The declarative/procedural model},
  author={Michael T. Ullman},
  journal={Nature Reviews Neuroscience},
  year={2001},
  volume={2},
  pages={717-726}
}

@misc{qiu2023pragmatic,
 title={Pragmatic Implicature Processing in ChatGPT},
 url={osf.io/preprints/psyarxiv/qtbh9},
 DOI={10.31234/osf.io/qtbh9},
 publisher={PsyArXiv},
 author={Qiu, Zhuang and Duan, Xufeng and Cai, Zhenguang G},
 year={2023},
 month={May}
}

@misc{guo2023close,
    title={How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection},
    author={Biyang Guo and Xin Zhang and Ziyuan Wang and Minqi Jiang and Jinran Nie and Yuxuan Ding and Jianwei Yue and Yupeng Wu},
    year={2023},
    eprint={2301.07597},
    url={https://arxiv.org/abs/2301.07597},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{instructgpt,
    title={Training language models to follow instructions with human feedback},
    author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
    year={2022},
    eprint={2203.02155},
    url={https://arxiv.org/abs/2203.02155},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{warstadt2022artificial,
  title={What artificial neural networks can tell us about human language acquisition},
  author={Warstadt, Alex and Bowman, Samuel R},
  journal={Algebraic Structures in Natural Language},
  pages={17--60},
  year={2022},
  publisher={CRC Press}
}

@article{chang2022word,
    author = {Chang, Tyler A. and Bergen, Benjamin K.},
    title = "{Word Acquisition in Neural Language Models}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {1-16},
    year = {2022},
    month = {01},
    abstract = "{We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words’ ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00444},
    url = {https://doi.org/10.1162/tacl\_a\_00444},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00444/1986589/tacl\_a\_00444.pdf},
}

@misc{warstadt2020neural,
      title={Can neural networks acquire a structural bias from raw linguistic data?},
      author={Alex Warstadt and Samuel R. Bowman},
      year={2020},
      eprint={2007.06761},
      url={https://arxiv.org/abs/2007.06761},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
bosc2022varying,
title={Varying meaning complexity to explain and measure compositionality},
author={Tom Bosc},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=BnGzfmZ07bq}
}

@inproceedings{
steinert-threlkeld2022emergent,
title={Emergent Communication Fine-tuning ({EC}-{FT}) for Pretrained Language Models},
author={Shane Steinert-Threlkeld and Xuhui Zhou and Zeyu Liu and C. M. Downey},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=SUqrM7WR7W5}
}

@inproceedings{
kalinowska2022situated,
title={Situated Communication: A Solution to Over-communication between Artificial Agents},
author={Aleksandra Kalinowska and Elnaz Davoodi and Florian Strub and Kory Mathewson and Todd Murphey and Patrick Pilarski},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=HLqzzQWA7Z9}
}

@inproceedings{
raviv2022what,
title={What makes a language easy to learn? A preregistered study on how systematic structure and community size affect language learnability},
author={Limor Raviv and Marianne de Heer Kloots and Antje Meyer},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=BdbexQ-0XW9}
}

@inproceedings{
ossenkopf2020comaze,
title={CoMaze: A cooperative game for zero-shot coordination},
author={Marie Ossenkopf},
booktitle={Emergent Communication Workshop at NeurIPS 2020},
year={2020},
url={https://drive.google.com/file/d/1GnPBV3Q-w65TZSaLD4YASt6nIuTZq7bM/view}
}

@inproceedings{
ossenkopf2019enhancing,
title={Enhancing Communication Learning through Empathic Prediction},
author={Marie Ossenkopf},
booktitle={Emergent Communication Workshop at NeurIPS 2019},
year={2019},
url={https://drive.google.com/file/d/19VWGANQUBWBBqEgXi1zbRgQ9uv0zh8jJ/view}
}

@inproceedings{
fitzgerald2019populate,
title={To Populate Is To Regulate},
author={Nicole Fitzgerald},
booktitle={Emergent Communication Workshop at NeurIPS 2019},
year={2019},
url={https://drive.google.com/file/d/1mINOlJvyWxGtqW_qNadMEhWNNJ4ZPIIa/view}
}

@misc{leni2018seq2seq,
      title={Seq2Seq Mimic Games: A Signaling Perspective}, 
      author={Juan Leni and John Levine and John Quigley},
      year={2018},
      eprint={1811.06564},
      url={https://arxiv.org/abs/1811.06564},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{
lipinski2022emergent,
title={Emergent Password Signalling in the Game of Werewolf},
author={Olaf Lipinski and Adam Sobey and Federico Cerutti and Timothy J Norman},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=B4xM-Qb0mbq}
}

@inproceedings{lu2020sil,
author = {Lu, Yuchen and Singhal, Soumye and Strub, Florian and Pietquin, Olivier and Courville, Aaron},
title = {Countering Language Drift with Seeded Iterated Learning},
year = {2020},
publisher = {JMLR.org},
abstract = {Pretraining on human corpus and then finetuning in a simulator has become a standard pipeline for training a goal-oriented dialogue agent. Nevertheless, as soon as the agents are finetuned to maximize task completion, they suffer from the so-called language drift phenomenon: they slowly lose syntactic and semantic properties of language as they only focus on solving the task. In this paper, we propose a generic approach to counter language drift called Seeded iterated learning (SIL). We periodically refine a pretrained student agent by imitating data sampled from a newly generated teacher agent. At each time step, the teacher is created by copying the student agent, before being finetuned to maximize task completion. SIL does not require external syntactic constraint nor semantic knowledge, making it a valuable taskagnostic finetuning protocol. We evaluate SIL in a toy-setting Lewis Game, and then scale it up to the translation game with natural language. In both settings, SIL helps counter language drift as well as it improves the task completion compared to baselines.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {597},
numpages = {11},
series = {ICML'20}
}

@inproceedings{cogswell2020dialog,
 author = {Cogswell, Michael and Lu, Jiasen and Jain, Rishabh and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {19988--19999},
 publisher = {Curran Associates, Inc.},
 title = {Dialog without Dialog Data: Learning Visual Dialog Agents from VQA Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/e7023ba77a45f7e84c5ee8a28dd63585-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{lee-etal-2019-countering,
    title = "Countering Language Drift via Visual Grounding",
    author = "Lee, Jason  and
      Cho, Kyunghyun  and
      Kiela, Douwe",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1447",
    doi = "10.18653/v1/D19-1447",
    pages = "4385--4395",
    abstract = "Emergent multi-agent communication protocols are very different from natural language and not easily interpretable by humans. We find that agents that were initially pretrained to produce natural language can also experience detrimental language drift: when a non-linguistic reward is used in a goal-based task, e.g. some scalar success metric, the communication protocol may easily and radically diverge from natural language. We recast translation as a multi-agent communication game and examine auxiliary training constraints for their effectiveness in mitigating language drift. We show that a combination of syntactic (language model likelihood) and semantic (visual grounding) constraints gives the best communication performance, allowing pre-trained agents to retain English syntax while learning to accurately convey the intended meaning.",
}

@InProceedings{lake2018generalization,
  title = 	 {Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks},
  author =       {Lake, Brenden and Baroni, Marco},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2873--2882},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/lake18a/lake18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/lake18a.html},
  abstract = 	 {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks’ notorious training data thirst.}
}

@misc{zhang2022unveiling,
    title={Unveiling Transformers with LEGO: a synthetic reasoning task},
    author={Yi Zhang and Arturs Backurs and Sébastien Bubeck and Ronen Eldan and Suriya Gunasekar and Tal Wagner},
    year={2022},
    eprint={2206.04301},
    url={https://arxiv.org/abs/2206.04301},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{mirzaee-kordjamshidi-2022-transfer,
    title = "Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning",
    author = "Mirzaee, Roshanak  and
      Kordjamshidi, Parisa",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.413",
    doi = "10.18653/v1/2022.emnlp-main.413",
    pages = "6148--6165",
    abstract = "Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is less explored for spatial language. We provide two new data resources on multiple spatial language processing tasks. The first dataset is synthesized for transfer learning on spatial question answering (SQA) and spatial role labeling (SpRL). Compared to previous SQA datasets, we include a larger variety of spatial relation types and spatial expressions. Our data generation process is easily extendable with new spatial expression lexicons. The second one is a real-world SQA dataset with human-generated questions built on an existing corpus with SPRL annotations. This dataset can be used to evaluate spatial language processing models in realistic situations. We show pretraining with automatically generated data significantly improves the SOTA results on several SQA and SPRL benchmarks, particularly when the training data in the target domain is small.",
}

@article{bar2002general,
  title={General features of complex systems},
  author={Bar-Yam, Yaneer},
  journal={Encyclopedia of Life Support Systems (EOLSS), UNESCO, EOLSS Publishers, Oxford, UK},
  volume={1},
  year={2002}
}

@ARTICLE{li2023metadrive,
  author={Li, Quanyi and Peng, Zhenghao and Feng, Lan and Zhang, Qihang and Xue, Zhenghai and Zhou, Bolei},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning}, 

  year={2023},

  volume={45},

  number={3},

  pages={3461-3475},

  doi={10.1109/TPAMI.2022.3190471}}

@Article{bracci2023representational,
   Author="Bracci, S.  and Mraz, J.  and Zeman, A.  and Leys, G.  and Op de Beeck, H. ",
   Title="{{T}he representational hierarchy in human and artificial visual systems in the presence of object-scene regularities}",
   Journal="PLoS Comput Biol",
   Year="2023",
   Volume="19",
   Number="4",
   Pages="e1011086",
   Month="Apr"
}

@article{olah2017feature,
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  title = {Feature Visualization},
  journal = {Distill},
  year = {2017},
  note = {https://distill.pub/2017/feature-visualization},
  doi = {10.23915/distill.00007}
}

@misc{bbc-alphago,
  title        = "Artificial intelligence: Go master Lee Se-dol wins against AlphaGo program",
  author       = "{BBC}",
  howpublished = "\url{https://www.bbc.com/news/technology-35797102}",
  year         = 2016,
  note         = "Accessed: 2026-02-07"
}

@misc{li2018rlhistory,
      title={Deep Reinforcement Learning: An Overview}, 
      author={Yuxi Li},
      year={2018},
      eprint={1701.07274},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1701.07274}, 
}

@misc{deepblue,
  title        = "Deep Blue",
  author       = "{IBM}",
  howpublished = "\url{https://www.ibm.com/history/deep-blue}",
  note         = "Accessed: 2026-02-07"
}

@article{kirby2014iterated,
title = {Iterated learning and the evolution of language},
journal = {Current Opinion in Neurobiology},
volume = {28},
pages = {108-114},
year = {2014},
note = {SI: Communication and language},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2014.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0959438814001421},
author = {Simon Kirby and Tom Griffiths and Kenny Smith},
abstract = {Iterated learning describes the process whereby an individual learns their behaviour by exposure to another individual's behaviour, who themselves learnt it in the same way. It can be seen as a key mechanism of cultural evolution. We review various methods for understanding how behaviour is shaped by the iterated learning process: computational agent-based simulations; mathematical modelling; and laboratory experiments in humans and non-human animals. We show how this framework has been used to explain the origins of structure in language, and argue that cultural evolution must be considered alongside biological evolution in explanations of language origins.}
}

@article{SierraSantibez2018AnAM,
  title={An agent-based model of the emergence and evolution of a language system for boolean coordination},
  author={Josefina Sierra-Santib{\'a}{\~n}ez},
  journal={Autonomous Agents and Multi-Agent Systems},
  year={2018},
  volume={32},
  pages={417-458}
}

@inproceedings{Gupta2020AnalyzingSP,
  title={Analyzing structural priors in multi-agent communication},
  author={Abhinav Kumar Gupta and Agnieszka Slowik and William L. Hamilton and Mateja Jamnik and S. Holden and Christopher Joseph Pal},
  year={2020}
}

@article{Motamedi2017AnEA,
  title={An evolutionary approach to sign language emergence: From state to process},
  author={Yasamin Motamedi and Marieke Schouwstra and Simon Kirby},
  journal={Behavioral and Brain Sciences},
  year={2017},
  volume={40}
}

@inproceedings{Berea2018ApplicationsOL,
  title={Applications of Language Emergence: The Future of Languages, Scenarios, and How to Use Computational Social Science Tools},
  author={Anamaria Berea},
  year={2018}
}

@misc{Nevens2020APG,
  title={A Practical Guide to Studying Emergent Communication through Grounded Language Games},
  author={Jens Nevens and Paul Van Eecke and Katrien Beuls},
  archivePrefix={arXiv},
  year={2020},
  eprint={2004.09218},
}

@article{GonzlezRodrguez2018ArtificialIA,
  title={Artificial intelligence and constructed-language emergence},
  author={Diego Gonz{\'a}lez-Rodr{\'i}guez and Jose Rodolfo Hernandez-Carri{\'o}n},
  journal={Cybernetics and Systems},
  year={2018}
}

@phdthesis{Gaya2017AutotelicPT,
  title={Autotelic Principle: the role of intrinsic motivation in the emergence and development of artificial language.},
  author={Miquel Cornudella Gaya},
  year={2017},
  url={https://hal.science/tel-01765234v1},
  school={Universit{\'e} de recherche Paris Sciences et Lettres},
}

@inproceedings{Eccles2019BiasesFE,
  title={Biases for Emergent Communication in Multi-agent Reinforcement Learning},
  author={Tom Eccles and Yoram Bachrach and Guy Lever and Angeliki Lazaridou and Thore Graepel},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{Resnick2020CapacityBA,
  title={Capacity, Bandwidth, and Compositionality in Emergent Language Learning},
  author={Cinjon Resnick and Abhinav Gupta and Jakob N. Foerster and Andrew M. Dai and Kyunghyun Cho},
  booktitle={AAMAS},
  year={2020}
}

@inproceedings{Odlo2017CombiningEA,
  title={Combining Evolutionary Algorithms and Language Games to Simulate Language Emergence},
  author={Per Odlo},
  year={2017}
}

@inproceedings{Todd2019CommunicationAL,
  title={Communication and Language Emergence Among Populations and Clusters of Agents},
  author={Graham Todd},
  year={2019}
}

@article{Lannelongue2019CompositionalGL,
  title={Compositional Grounded Language for Agent Communication in Reinforcement Learning Environment},
  author={K. Lannelongue and M. De Milly and Roberto Marcucci and S. Selevarangame and A. Supizet and A. Grincourt},
  journal={Journal of Autonomous Intelligence},
  year={2019}
}

@inproceedings{Gupta2020CompositionalityAC,
  title={Compositionality and Capacity in Emergent Languages},
  author={Abhinav Gupta and Cinjon Resnick and Jakob N. Foerster and Andrew M. Dai and Kyunghyun Cho},
  booktitle={REPL4NLP},
  year={2020}
}

@article{Todo2020ContributionOR,
  title={Contribution of reflection in language emergence with an under-restricted situation},
  author={Kense Todo and Masayuki Yamamura},
  journal={2020 International Research Conference on Smart Computing and Systems Engineering (SCSE)},
  year={2020},
  pages={147-154}
}

@article{Hildreth2019CoordinatingMN,
  title={Coordinating Multi-Agent Navigation by Learning Communication},
  author={Dalton Hildreth and Stephen J. Guy},
  journal={Proc. ACM Comput. Graph. Interact. Tech.},
  year={2019},
  volume={2},
  pages={20:1-20:17}
}

@misc{Grupen2021CurriculumDrivenML,
  title={Curriculum-Driven Multi-Agent Learning and the Role of Implicit Communication in Teamwork},
  author={Niko A. Grupen and Daniel D. Lee and Bart Selman},
  archivePrefix={arXiv},
  year={2021},
  eprint={2106.11156},
}

@article{Abudu2020DeepEC,
  title={Deep Emergent Communication for the IoT},
  author={Prince Abudu and A. Markham},
  journal={2020 IEEE International Conference on Smart Computing (SMARTCOMP)},
  year={2020},
  pages={130-137}
}

@article{Chen2022DeepRL,
  title={Deep reinforcement learning with emergent communication for coalitional negotiation games.},
  author={Siqi Chen and Yang Yang and Ran Su},
  journal={Mathematical biosciences and engineering : MBE},
  year={2022},
  volume={19 5},
  pages={
          4592-4609
        }
}

@misc{Korbak2019DevelopmentallyME,
  title={Developmentally motivated emergence of compositional communication via template transfer},
  author={Tomasz Korbak and Julian Zubek and Lukasz Kucinski and Piotr Milos and Joanna Rączaszek-Leonardi},
  archivePrefix={arXiv},
  year={2019},
  eprint={1910.06079},
}

@inproceedings{Garcia2022DisentanglingCI,
  title={Disentangling Categorization in Multi-agent Emergent Communication},
  author={Washington Garcia and H S Clouse and Kevin Butler},
  booktitle={NAACL},
  year={2022}
}

@misc{Li2019EaseofTeachingAL,
  title={Ease-of-Teaching and Language Structure from Emergent Communication},
  author={Fushan Li and Michael Bowling},
  archivePrefix={arXiv},
  year={2019},
  eprint={1906.02403},
}

@inproceedings{Sedlcek2015EmergenceOC,
  title={Emergence of communication: Triadic interaction},
  author={Martin Sedl{\'a}cek and Veronika Kundlova},
  year={2015}
}

@misc{Cogswell2019EmergenceOC,
  title={Emergence of Compositional Language with Deep Generational Transmission},
  author={Michael Cogswell and Jiasen Lu and Stefan Lee and Devi Parikh and Dhruv Batra},
  archivePrefix={arXiv},
  year={2019},
  eprint={1904.09067},
}

@inproceedings{Zeigler2017EmergenceOH,
  title={Emergence of human language: a DEVS-based systems approach},
  author={Bernard P. Zeigler},
  year={2017}
}

@article{Lipowska2018EmergenceOL,
  title={Emergence of linguistic conventions in multi-agent reinforcement learning},
  author={Dorota Lipowska and Adam Lipowski},
  journal={PLoS ONE},
  year={2018},
  volume={13}
}

@misc{DeGiuli2019EmergenceOO,
  title={Emergence of order in random languages},
  author={E. DeGiuli},
  archivePrefix={arXiv},
  year={2019},
  eprint={1902.07516},
}

@misc{Yuan2020EmergenceOP,
  title={Emergence of Pragmatics from Referential Game between Theory of Mind Agents},
  author={Luyao Yuan and Zipeng Fu and Jingyue Shen and Lu Xu and Junhong Shen and Song-Chun Zhu},
  archivePrefix={arXiv},
  year={2020},
  eprint={2001.07752},
}

@inproceedings{Verma2020EmergenceOW,
  title={Emergence of Writing Systems Through Multi-Agent Cooperation},
  author={Shresth Verma and Joydip Dhar},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{Perrot2015EmergenceOD,
  title={Emergence on Decreasing Sandpile Models},
  author={K{\'e}vin Perrot and {\'E}ric R{\'e}mila},
  booktitle={MFCS},
  year={2015}
}

@inproceedings{
Chaabouni2022EMC,
  IDS={chaabouni2022emergent},
title={Emergent Communication at Scale},
author={Rahma Chaabouni and Florian Strub and Florent Altch{\'e} and Eugene Tarassov and Corentin Tallec and Elnaz Davoodi and Kory Wallace Mathewson and Olivier Tieleman and Angeliki Lazaridou and Bilal Piot},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=AUGBfDIV9rL}
}

@inproceedings{
Evtimova2018EmergentCI,
title={Emergent Communication in a Multi-Modal, Multi-Step Referential Game},
author={Katrina Evtimova and Andrew Drozdov and Douwe Kiela and Kyunghyun Cho},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJGZq6g0-},
}

@inproceedings{
NicoloBrandizzi2022EMC,
title={Emergent communication in human-machine games},
author={Nicolo' Brandizzi and Luca Iocchi},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=rqLgeQWCXZ9}
}

@misc{Taniguchi2022EmergentCT,
  title={Emergent Communication through Metropolis-Hastings Naming Game with Deep Generative Models},
  author={Tadahiro Taniguchi and Yuto Yoshida and Akira Taniguchi and Yoshinobu Hagiwara},
  archivePrefix={arXiv},
  year={2022},
  eprint={2205.12392},
}

@misc{Cao2018EmergentCT,
  title={Emergent Communication through Negotiation},
  author={Kris Cao and Angeliki Lazaridou and Marc Lanctot and Joel Z. Leibo and Karl Tuyls and Stephen Clark},
  archivePrefix={arXiv},
  year={2018},
  eprint={1804.03980},
}

@inproceedings{Noukhovitch2021EmergentCU,
  title={Emergent Communication under Competition},
  author={Michael Noukhovitch and Travis LaCroix and Angeliki Lazaridou and Aaron C. Courville},
  booktitle={AAMAS},
  year={2021}
}

@inproceedings{Kim2021EmergentCU,
 author = {Kim, Jooyeon and Oh, Alice},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {17579--17591},
 publisher = {Curran Associates, Inc.},
 title = {Emergent Communication under Varying Sizes and Connectivities},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/92dfa194391a59dc65b88b704599dbd6-Paper.pdf},
 volume = {34},
 year = {2021}
}

@misc{CowenRivers2020EmergentCW,
  title={Emergent Communication with World Models},
  author={Alexander Imani Cowen-Rivers and Jason Naradowsky},
  archivePrefix={arXiv},
  year={2020},
  eprint={2002.09604},
}

@inproceedings{Tomlin2019EmergentCI,
  title={Emergent Compositionality in Signaling Games},
  author={Nicholas Tomlin and Ellie Pavlick},
  booktitle={CogSci},
  year={2019}
}

@inproceedings{Phan2022EmergentCF,
  title={Emergent Cooperation from Mutual Acknowledgment Exchange},
  author={Thomy Phan and Felix Sommer and Philipp Altmann and Fabian Ritz and Lenz Belzner and Claudia Linnhoff-Popien},
  booktitle={AAMAS},
  year={2022}
}

@inproceedings{Yu2022EMC,
title={Emergent Covert Signaling in Adversarial Reference Games},
author={Dhara Yu and Jesse Mu and Noah Goodman},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=H-eMQbR7Z5}
}

@inproceedings{Qiu2021EmergentGC,
  title={Emergent Graphical Conventions in a Visual Communication Game},
  author={Shuwen Qiu and Sirui Xie and Lifeng Fan and Tao Gao and Song-Chun Zhu and Yixin Zhu},
  year={2021}
}

@misc{Evtimova2017EmergentLI,
  title={Emergent Language in a Multi-Modal, Multi-Step Referential Game},
  author={Katrina Evtimova and Andrew Drozdov and Douwe Kiela and Kyunghyun Cho},
  archivePrefix={arXiv},
  year={2017},
  eprint={1705.10369},
}

@article{Kubricht2020EmergentLF,
  title={Emergent Languages from Pretrained Embeddings Characterize Latent Concepts in Dynamic Imagery},
  author={James R. Kubricht and Alberto Santamar\'ia-Pang and Chinmaya Devaraj and Aritra Chowdhury and Peter H. Tu},
  journal={Int. J. Semantic Comput.},
  year={2020},
  volume={14},
  pages={357-373}
}

@misc{Graesser2019EmergentLP,
  title={Emergent Linguistic Phenomena in Multi-Agent Communication Games},
  author={Laura Graesser and Kyunghyun Cho and Douwe Kiela},
  archivePrefix={arXiv},
  year={2019},
  eprint={1901.08706},
}

@article{Manning2020EmergentLS,
  title={Emergent linguistic structure in artificial neural networks trained by self-supervision},
  author={Christopher D. Manning and Kevin Clark and John Hewitt and Urvashi Khandelwal and Omer Levy},
  journal={Proceedings of the National Academy of Sciences},
  year={2020},
  volume={117},
  pages={30046 - 30054}
}

@article{CastroGarcia2019EmergentML,
  title={Emergent Multilingual Language Acquisition Using Developmental Networks},
  author={Juan L. Castro-Garcia and Juyang Weng},
  journal={2019 International Joint Conference on Neural Networks (IJCNN)},
  year={2019},
  pages={1-8}
}

@misc{Miletitch2019EmergentNO,
  title={Emergent naming of resources in a foraging robot swarm},
  author={Roman Miletitch and Andreagiovanni Reina and Marco Dorigo and V. Trianni},
  archivePrefix={arXiv},
  year={2019},
  eprint={1910.02274},
}

@article{Chowdhury2021EmergentSL,
  IDS={2008.09860,chowdhury2020emergent},
  title={Emergent Symbolic Language Based Deep Medical Image Classification},
  author={Aritra Chowdhury and Alberto Santamar\'ia-Pang and James R. Kubricht and Peter H. Tu},
  journal={2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)},
  year={2021},
  pages={689-692}
}

@misc{Lee2018EmergentTI,
  title={Emergent Translation in Multi-Agent Communication},
  author={Jason Lee and Kyunghyun Cho and Jason Weston and Douwe Kiela},
  archivePrefix={arXiv},
  year={2018},
  eprint={1710.06922},
}

@article{Chowdhury2020EscellES,
  title={Escell: Emergent Symbolic Cellular Language},
  author={Aritra Chowdhury and James R. Kubricht and Anup Sood and Peter H. Tu and Alberto Santamar\'ia-Pang},
  journal={2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)},
  year={2020},
  pages={1604-1607}
}

@article{Grossi2017EvolvedCS,
  title={Evolved communication strategies and emergent behaviour of multi-agents in pursuit domains},
  author={Gina Grossi and Brian J. Ross},
  journal={2017 IEEE Conference on Computational Intelligence and Games (CIG)},
  year={2017},
  pages={110-117}
}

@article{Sirota2019EvolvingRN,
  title={Evolving recurrent neural networks for emergent communication},
  author={Joshua Sirota and Vadim Bulitko and Matthew R. G. Brown and Sergio Poo Hernandez},
  journal={Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  year={2019}
}

@misc{Guo2021ExpressivityOE,
  title={Expressivity of Emergent Language is a Trade-off between Contextual Complexity and Unpredictability},
  author={Shangmin Guo and Yi Ren and Kory Wallace Mathewson and Simon Kirby and Stefano V. Albrecht and Kenny Smith},
  archivePrefix={arXiv},
  year={2021},
  eprint={2106.03982},
}

@unpublished{Ikram2021HexaJungleAM,
       booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR 2021), Embodied AI Workshop},
           month = {June},
           title = {HexaJungle: a MARL Simulator to Study the Emergence of Language},
            year = {2021},
             url = {https://openaccess.city.ac.uk/id/eprint/26284/},
        abstract = {Multi-agent reinforcement learning in mixed-motivesettings allows for the study of complex dynamics ofagent interactions. Embodied agents in partially ob-servable environments with the ability to communicatecan share information, agree on strategies, or even lieto each other.In order to study this, we propose a sim-ple environment where we can impose varying levels ofcooperation, communication and competition as pre-requisites to reach an optimal outcome. Welcome tothe jungle.},
          author = {Ikram, K. and Mondragon, E. and Alonso, E. and Garcia-Ortiz, M.}
}

@inproceedings{Cornudella2015HowIM,
  title={How Intrinsic Motivation can Speed Up Language Emergence},
  author={Miquel Cornudella and Paul Van Eecke and Remi van Trijp},
  booktitle={ECAL},
  year={2015}
}

@inproceedings{Tomlin2018IncrementalPA,
  title={Incremental Pragmatics and Emergent Communication},
  author={Nicholas Tomlin and Ellie Pavlick},
  year={2018}
}

@misc{Hazra2020InfiniteUO,
  title={Infinite use of finite means: Zero-Shot Generalization using Compositional Emergent Protocols},
  author={Rishi Hazra and Sonu Dixit and Sayambhu Sen},
  archivePrefix={arXiv},
  year={2020},
  eprint={2012.05011},
}

@inproceedings{Wang2021InfluencingTS,
  title={Influencing Towards Stable Multi-Agent Interactions},
  author={Woodrow Z. Wang and Andy Shih and Annie Xie and Dorsa Sadigh},
  booktitle={CoRL},
  year={2021}
}

@misc{Kharitonov2019InformationMI,
  title={Information Minimization In Emergent Languages},
  author={Eugene Kharitonov and Rahma Chaabouni and Diane Bouchacourt and Marco Baroni},
  archivePrefix={arXiv},
  year={2019},
  eprint={1905.13687},
}

@inproceedings{Eshghi2017InteractionalDA,
  title={Interactional dynamics and the emergence of language games},
  author={Arash Eshghi and Igor Shalyminov and Oliver Lemon},
  booktitle={FADLI@ESSLLI},
  year={2017}
}

@article{Korbak2021InteractionHA,
  title={Interaction history as a source of compositionality in emergent communication},
  author={Tomasz Korbak and Julian Zubek and {\L}ukasz Kuci{\'n}ski and Piotr Milos and Joanna R{\k{a}}czaszek-Leonardi},
  journal={Interaction Studies},
  year={2021}
}

@article{Patel2021InterpretationOE,
  title={Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents},
  author={Shivansh Patel and Saim Wani and Unnat Jain and Alexander G. Schwing and Svetlana Lazebnik and Manolis Savva and Angel Xuan Chang},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={15993-15943}
}

@inproceedings{Kempson2017LanguageAM,
  title={Language as Mechanisms for Interaction: Towards an Evolutionary Tale},
  author={Ruth Kempson and Eleni Gregoromichelaki and C. Howes},
  booktitle={TbiLLC},
  year={2017}
}

@inproceedings{Cornudella2015LanguageEI,
  title={Language Emergence in a Population of Artificial Agents Equipped with the Autotelic Principle},
  author={Miquel Cornudella and T. Poibeau},
  year={2015}
}

@article{Cambier2020LanguageEI,
  title={Language Evolution in Swarm Robotics: A Perspective},
  author={Nicolas Cambier and Roman Miletitch and Vincent Fr{\'e}mont and Marco Dorigo and Eliseo Ferrante and V. Trianni},
  journal={Frontiers in Robotics and AI},
  year={2020},
  volume={7}
}

@article{Johns2019LanguageMA,
  title={Language membership as a gradient emergent feature},
  author={Michael A. Johns and Michael T. Putnam},
  journal={Bilingualism: Language and Cognition},
  year={2019},
  volume={22},
  pages={701 - 702}
}

@article{Hilbert2020LargeScaleCI,
  title={Large-Scale Communication is More Complex and Unpredictable with Automated Bots},
  author={Martin Hilbert and David M. Darmon},
  journal={Journal of Communication},
  year={2020}
}

@inproceedings{Li2022LearningED,
  title={Learning Emergent Discrete Message Communication for Cooperative Reinforcement Learning},
  author={Sheng Li and Yutai Zhou and R. Allen and Mykel J. Kochenderfer},
  booktitle={ICRA},
  year={2022}
}

@inproceedings{Houser2020LearningLA,
  title={Learning Language: An Experiment},
  author={Daniel Houser and Yang Yang},
  year={2020}
}

@inproceedings{Zhang2019LearningTC,
  title={Learning to Communicate and Solve Visual Blocks-World Tasks},
  author={Qi Zhang and Richard L. Lewis and Satinder Singh and Edmund H. Durfee},
  booktitle={AAAI},
  year={2019}
}

@misc{Kaji2020LearningTC,
  title={Learning to cooperate: Emergent communication in multi-agent navigation},
  author={Ivana Kaji{\'c} and Eser Ayg{\"u}n and Doina Precup},
  archivePrefix={arXiv},
  year={2020},
  eprint={2004.01097},
}

@misc{Mihai2021LearningTD,
  title={Learning to Draw: Emergent Communication through Sketching},
  author={Daniela Mihai and Jonathon S. Hare},
  archivePrefix={arXiv},
  year={2021},
  eprint={2106.02067},
}

@misc{Taylor2021LearningTI,
  title={Learning to Improve Representations by Communicating About Perspectives},
  author={Julius Taylor and Eleni Nisioti and Cl{\'e}ment Moulin-Frier},
  archivePrefix={arXiv},
  year={2021},
  eprint={2109.09390},
}

@inproceedings{Kolb2019LearningTR,
  title={Learning to request guidance in emergent language},
  author={Benjamin Kolb and Leon Lang and Henning Bartsch and Arwin Gansekoele and Raymond Koopmanschap and Leonardo Romor and David Speck and Mathijs Mul and Elia Bruni},
  booktitle={EMNLP},
  year={2019}
}

@inproceedings{Wu2021Lecture2,
  title={Lecture 20 : Multi-Agent Deep RL \&amp; Emergent Communication},
  author={Cathy Wu and Athul Paul Jacob},
  year={2021}
}

@misc{Yao2022LinkingEA,
  title={Linking Emergent and Natural Languages via Corpus Transfer},
  author={Shunyu Yao and Mo Yu and Yang Zhang and Karthik Narasimhan and Joshua B. Tenenbaum and Chuang Gan},
  archivePrefix={arXiv},
  year={2022},
  eprint={2203.13344},
}

@misc{Mul2019MasteringEL,
  title={Mastering emergent language: learning to guide in simulated navigation},
  author={Mathijs Mul and Diane Bouchacourt and Elia Bruni},
  archivePrefix={arXiv},
  year={2019},
  eprint={1908.05135},
}

@misc{Bouchacourt2019MissTA,
  title={Miss Tools and Mr Fruit: Emergent Communication in Agents Learning about Object Affordances},
  author={Diane Bouchacourt and Marco Baroni},
  archivePrefix={arXiv},
  year={2019},
  eprint={1905.11871},
}

@misc{Boldt2022ModelingEL,
  title={Modeling Emergent Lexicon Formation with a Self-Reinforcing Stochastic Process},
  author={Brendon Boldt and David R. Mortensen},
  archivePrefix={arXiv},
  year={2022},
  eprint={2206.11146},
}

@inproceedings{Grupen2022MultiAgentCA,
  title={Multi-Agent Curricula and Emergent Implicit Signaling},
  author={Niko A. Grupen and Daniel D. Lee and Bart Selman},
  booktitle={AAMAS},
  year={2022}
}

@article{Simes2019MultiAgentDR,
  title={Multi-Agent Deep Reinforcement Learning with Emergent Communication},
  author={David Apolin{\'a}rio Sim{\~o}es and Nuno Lau and Lu{\'i}s Paulo Reis},
  journal={2019 International Joint Conference on Neural Networks (IJCNN)},
  year={2019},
  pages={1-8}
}

@article{Hagiwara2022MultiagentMC,
  title={Multiagent multimodal categorization for symbol emergence: emergent communication via interpersonal cross-modal inference},
  author={Yoshinobu Hagiwara and Kazuma Furukawa and Akira Taniguchi and Tadahiro Taniguchi},
  journal={Advanced Robotics},
  year={2022},
  volume={36},
  pages={239 - 260}
}

@inproceedings{Gupta2020NetworkedMR,
  title={Networked Multi-Agent Reinforcement Learning with Emergent Communication},
  author={Shubham Gupta and Rishi Hazra and Ambedkar Dukkipati},
  booktitle={AAMAS},
  year={2020}
}

@inproceedings{Liang2020OnEC,
  title={On Emergent Communication in Competitive Multi-Agent Teams},
  author={Paul Pu Liang and Jeffrey Chen and Ruslan Salakhutdinov and Louis-Philippe Morency and Satwik Kottur},
  booktitle={AAMAS},
  year={2020}
}

@article{Loreto2016OnTE,
  title={On the emergence of syntactic structures: quantifying and modelling duality of patterning},
  author={Vittorio Loreto and Pietro Gravino and Vito Domenico Pietro Servedio and Francesca Tria},
  journal={Topics in cognitive science},
  year={2016},
  volume={8 2},
  pages={
          469-80
        }
}

@inproceedings{Lowe2019OnTP,
  title={On the Pitfalls of Measuring Emergent Communication},
  author={Ryan Lowe and Jakob N. Foerster and Y-Lan Boureau and Joelle Pineau and Yann Dauphin},
  booktitle={AAMAS},
  year={2019}
}

@misc{Gupta2019OnVS,
  title={On Voting Strategies and Emergent Communication},
  author={Shubham Gupta and Ambedkar Dukkipati},
  archivePrefix={arXiv},
  year={2019},
  eprint={1902.06897},
}

@misc{Denamganai2020ReferentialGymAN,
  title={ReferentialGym: A Nomenclature and Framework for Language Emergence \&amp; Grounding in (Visual) Referential Games},
  author={Kevin Denamganai and James Alfred Walker},
  archivePrefix={arXiv},
  year={2020},
  eprint={2012.09486},
}

@article{Sabathiel2022SelfCommunicatingDR,
  title={Self-Communicating Deep Reinforcement Learning Agents Develop External Number Representations},
  author={Silvester Sabathiel and Trygve Solstad and Alberto Testolin and Flavio Petruzzellis},
  journal={Proceedings of the Northern Lights Deep Learning Workshop},
  year={2022}
}

@misc{Foguelman2021SimulationOE,
  title={Simulation of emergence in artificial societies: a practical model-based approach with the EB-DEVS formalism},
  author={Daniel Foguelman and Esteban Lanzarotti and E. Ferreyra and Rodrigo Castro},
  archivePrefix={arXiv},
  year={2021},
  eprint={2110.08170},
}

@inproceedings{Nakamura2015SimulationOT,
  title={Simulation of the Emergence of Language Groups Using the Iterated Learning Model on Social Networks},
  author={Makoto Nakamura and Ryuichi Matoba and Satoshi Tojo},
  year={2015}
}

@inproceedings{Ospina2019SociallyAB,
  title={Socially and Biologically Inspired Computing for Self-organizing Communications Networks},
  author={Juan Pablo Ospina and Joaqu{\'i}n F. S{\'a}nchez and Jorge Eduardo Ortiz Trivino and Carlos Andr{\'e}s Collazos Morales and Paola Ariza Colpas},
  booktitle={MLN},
  year={2019}
}

@article{Kantharaju2021SocialSO,
  title={Social Signals of Cohesion in Multi-party Interactions},
  author={Reshmashree Bangalore Kantharaju and Catherine Pelachaud},
  journal={Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents},
  year={2021}
}

@article{Hagiwara2019SymbolEA,
  title={Symbol Emergence as an Interpersonal Multimodal Categorization},
  author={Yoshinobu Hagiwara and Hiroyoshi Kobayashi and Akira Taniguchi and Tadahiro Taniguchi},
  journal={Frontiers in Robotics and AI},
  year={2019},
  volume={6}
}

@article{Taniguchi2019SymbolEI,
  title={Symbol Emergence in Cognitive Developmental Systems: A Survey},
  author={Tadahiro Taniguchi and Justus H. Piater and Florentin W{\"o}rg{\"o}tter and Emre Ugur and Matej Hoffmann and Lorenzo Jamone and Takayuki Nagai and Benjamin Rosman and Toshihiko Matsuka and Naoto Iwahashi and Erhan Oztop},
  journal={IEEE Transactions on Cognitive and Developmental Systems},
  year={2019},
  volume={11},
  pages={494-516}
}

@article{Taniguchi2016SymbolEI,
  title={Symbol emergence in robotics: a survey},
  author={Tadahiro Taniguchi and Takayuki Nagai and Tomoaki Nakamura and Naoto Iwahashi and Tetsuya Ogata and Hideki Asoh},
  journal={Advanced Robotics},
  year={2016},
  volume={30},
  pages={706 - 728}
}

@inproceedings{Ritt2018SymbolicAO,
  title={Symbolic Analysis of Machine Behaviour and the Emergence of the Machine Language},
  author={Roland Ritt and Paul O’Leary},
  booktitle={TPNC},
  year={2018}
}

@article{Oliveira2015SymbolicAI,
  title={Symbolic associations in neural network activations: Representations in the emergence of communication},
  author={Emerson Silva de Oliveira and Angelo C. Loula},
  journal={2015 International Joint Conference on Neural Networks (IJCNN)},
  year={2015},
  pages={1-8}
}

@inproceedings{Zeigler2018SYSTEMTF,
  title={SYSTEM THEORETIC FOUNDATIONS FOR EMERGENT BEHAVIOR MODELING: THE CASE OF EMERGENCE OF HUMAN LANGUAGE IN A RESOURCE-CONSTRAINED COMPLEX INTELLIGENT DYNAMICAL SYSTEM},
  author={Bernard P. Zeigler and Saurabh Mittal},
  year={2018}
}

@article{Gastaldi2021TheCO,
  title={The calculus of language: explicit representation of emergent linguistic structure through type-theoretical paradigms},
  author={Juan Luis Gastaldi and Lucie P Pellissier},
  journal={Interdisciplinary Science Reviews},
  year={2021},
  volume={46},
  pages={569 - 590}
}

@article{deCastroArrazola2019TheEO,
  title={The emergence of verse templates through iterated learning},
  author={Varuṇ deCastro-Arrazola and Simon Kirby},
  journal={Journal of Language Evolution},
  year={2019}
}

@article{Nowak2016TheEO,
  title={The emergence of word order and morphology in compositional languages via multigenerational signaling games},
  author={Iga Nowak and Giosu{\`e} Baggio},
  journal={Journal of Language Evolution},
  year={2016},
  volume={1},
  pages={137-150}
}

@article{BelEnguix2019TheIO,
  title={The Impact of Social Reputation in Language Evolution},
  author={Gemma Bel-Enguix},
  journal={Complexity Applications in Language and Communication Sciences},
  year={2019}
}

@inproceedings{Cornudella2016TheRO,
  title={The Role of Intrinsic Motivation in Artificial Language Emergence: a Case Study on Colour},
  author={Miquel Cornudella and T. Poibeau and Remi van Trijp},
  booktitle={COLING},
  year={2016}
}

@misc{SantamariaPang2020TowardsEL,
  title={Towards Emergent Language Symbolic Semantic Segmentation and Model Interpretability},
  author={Alberto Santamar\'ia-Pang and James R. Kubricht and Aritra Chowdhury and C. Bhushan and Peter H. Tu},
  archivePrefix={arXiv},
  year={2020},
  eprint={2007.09448},
}

@misc{Sowik2020TowardsGR,
  title={Towards Graph Representation Learning in Emergent Communication},
  author={Agnieszka Słowik and Abhinav Gupta and William L. Hamilton and Mateja Jamnik and S. Holden},
  archivePrefix={arXiv},
  year={2020},
  eprint={2001.09063},
}

@misc{Tucker2022TowardsHC,
  title={Towards Human-Agent Communication via the Information Bottleneck Principle},
  author={Mycal Tucker and Julie A. Shah and Roger Philip Levy and Noga Zaslavsky},
  archivePrefix={arXiv},
  year={2022},
  eprint={2207.00088},
}

@article{SantamariaPang2019TowardsSA,
  title={Towards Semantic Action Analysis via Emergent Language},
  author={Alberto Santamar\'ia-Pang and James R. Kubricht and Chinmaya Devaraj and Aritra Chowdhury and Peter H. Tu},
  journal={2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)},
  year={2019},
  pages={224-2244}
}

@article{SteinertThrelkeld2020TowardTE,
  title={Toward the Emergence of Nontrivial Compositionality},
  author={Shane Steinert-Threlkeld},
  journal={Philosophy of Science},
  year={2020},
  volume={87},
  pages={897 - 909}
}

@inproceedings{Burlak2018TrendsIE,
  title={Trends in evolution of signals' interpretation as precursors of the origin of human language},
  author={Svetlana Burlak},
  year={2018}
}

@article{RczaszekLeonardi2018UngroundingSI,
  title={Ungrounding symbols in language development: implications for modeling emergent symbolic communication in artificial systems},
  author={Joanna Rączaszek-Leonardi and Terrence W. Deacon},
  journal={2018 Joint IEEE 8th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
  year={2018},
  pages={232-237}
}

@inproceedings{Ohmer2021WhyAH,
  title={Why and how to study the impact of perception on language emergence in artificial agents},
  author={Xenia Ohmer and Michael Marino and Peter Koenig and Michael Franke},
  year={2021},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  url={https://escholarship.org/uc/item/6p82v6st},
  volume={43},
}

@article{Reboul2015WhyLR,
  title={Why language really is not a communication system: a cognitive view of language evolution},
  author={Anne Reboul},
  journal={Frontiers in Psychology},
  year={2015},
  volume={6}
}

@inproceedings{Gupta2020WinningAE,
  title={Winning an Election: On Emergent Strategic Communication in Multi-Agent Networks},
  author={Shubham Gupta and Ambedkar Dukkipati},
  booktitle={AAMAS},
  year={2020}
}

@misc{Chaabouni2019WordorderBI,
  title={Word-order Biases in Deep-agent Emergent Communication},
  author={Rahma Chaabouni and Eugene Kharitonov and Alessandro Lazaric and Emmanuel Dupoux and Marco Baroni},
  archivePrefix={arXiv},
  year={2019},
  eprint={1905.12330},
}

@phdthesis{Kgebck2018WordRF,
  title={Word Representations for Emergent Communication and Natural Language Processing},
  school={Chalmers University of Technology},
  author={Mikael K{\aa}geb{\"a}ck},
  year={2018},
  url={https://research.chalmers.se/en/publication/506086},
}

@misc{Hazra2021ZeroShotGU,
  title={Zero-Shot Generalization using Intrinsically Motivated Compositional Emergent Protocols},
  author={Rishi Hazra and Sonu Dixit and Sayambhu Sen},
  archivePrefix={arXiv},
  year={2021},
  eprint={2105.05069},
}

@misc{Wieczorek2023AFF,
 author = {Tobias J. Wieczorek and T. Tchumatchenko and Carlos Wert Carvajal and M. Eggl},
 booktitle = {arXiv.org},
 archivePrefix={arXiv},
 title = {A framework for the emergence and analysis of language in social learning agents},
 volume = {abs/2305.02632},
 year = {2023}
}

@Inproceedings{Ueda2022CAG,
 author = {Ryouichi Ueda and Taiga Ishii and Koki Washio and Yusuke Miyao},
 title = {Categorial Grammar Induction as a Compositionality Measure for Emergent Languages in Signaling Games},
 year = {2022}
}

@Article{Auersperger2022DefendingCI,
 author = {Michal Auersperger and Pavel Pecina},
 booktitle = {North American Chapter of the Association for Computational Linguistics},
 pages = {285-291},
 title = {Defending Compositionality in Emergent Languages},
 year = {2022}
}

@Inproceedings{Lipinski2022EMP,
 author = {Olaf Lipinski and A. Sobey and F. Cerutti and T. Norman},
 title = {Emergent Password Signalling in the Game of Werewolf},
 year = {2022}
}

@inproceedings{
Tucker2022GeneralizationAT,
title={Generalization and Translatability in Emergent Communication via Informational Constraints},
author={Mycal Tucker and Roger P. Levy and Julie Shah and Noga Zaslavsky},
booktitle={NeurIPS 2022 Workshop on Information-Theoretic Principles in Cognitive Systems},
year={2022},
url={https://openreview.net/forum?id=yf8suFtNZ5v}
}

@misc{Karten2022IntentGroundedCC,
 author = {Seth Karten and K. Sycara},
 title = {Intent-Grounded Compositional Communication through Mutual Information in Multi-Agent Teams},
 year = {2022},
 url={https://dcslgatech.github.io/iros22-multi-agent-workshop/contributed_papers/IROS22-DMMAS_paper_5730.pdf},
 note={in Decision Making in Multi-Agent Systems Workshop at IROS 2022},
}

@Article{Karten2022InterpretableLE,
 author = {Seth Karten and Mycal Tucker and Huao Li and Siva Kailas and Michael Lewis and K. Sycara},
 booktitle = {IEEE Transactions on Cognitive and Developmental Systems},
 journal = {IEEE Transactions on Cognitive and Developmental Systems},
 title = {Interpretable Learned Emergent Communication for Human-Agent Teams},
 year = {2022}
}

@Article{Eecke2023LanguageGM,
 author = {Paul Van Eecke and Katrien Beuls and Jérôme Botoko Ekila and Roxana Rădulescu},
 booktitle = {Journal of Language Evolution},
 journal = {Journal of Language Evolution},
 title = {Language games meet multi-agent reinforcement learning: A case study for the naming game},
 year = {2023}
}

@Article{Xiang2023MultiAgentRL,
 author = {Ping Xiang and Hangguan Shan and Zhou Su and Zhaoyang Zhang and Chen Chen and Er-Ping Li},
 booktitle = {IEEE Communications Letters},
 journal = {IEEE Communications Letters},
 pages = {195-199},
 title = {Multi-Agent Reinforcement Learning-Based Decentralized Spectrum Access in Vehicular Networks With Emergent Communication},
 volume = {27},
 year = {2023}
}

@misc{Boldt2022RecommendationsFS,
 author = {Brendon Boldt and David R. Mortensen},
 archivePrefix = {arXiv},
 title = {Recommendations for Systematic Research on Emergent Language},
 eprint = {2206.11302},
 year = {2022}
}

@Article{Bosc2022TheEO,
 author = {Tom Bosc and Pascal Vincent},
 booktitle = {Transactions of the Association for Computational Linguistics},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {1375-1391},
 title = {The Emergence of Argument Structure in Artificial Languages},
 volume = {10},
 year = {2022}
}

@inproceedings{Papadimitriou2020LearningMH,
    title = "{L}earning {M}usic {H}elps {Y}ou {R}ead: {U}sing Transfer to Study Linguistic Structure in Language Models",
    author = "Papadimitriou, Isabel  and
      Jurafsky, Dan",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.554/",
    doi = "10.18653/v1/2020.emnlp-main.554",
    pages = "6829--6839",
    abstract = "We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language. We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition."
}

@inproceedings{
yao2022linking,
title={Linking Emergent and Natural Languages via Corpus Transfer},
author={Shunyu Yao and Mo Yu and Yang Zhang and Karthik R Narasimhan and Joshua B. Tenenbaum and Chuang Gan},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=49A1Y6tRhaq}
}

@inproceedings{ortiz-suarez-etal-2020-monolingual,
    title = "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages",
    author = "Ortiz Su{\'a}rez, Pedro Javier  and
      Romary, Laurent  and
      Sagot, Beno{\^\i}t",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.156",
    doi = "10.18653/v1/2020.acl-main.156",
    pages = "1703--1714",
    abstract = "We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.",
}

@inproceedings{kharitonov-etal-2019-egg,
  IDS={egg,kharitonov2019egg,Kharitonov2019EGGAT},
    title = "{EGG}: a toolkit for research on Emergence of lan{G}uage in Games",
    author = "Kharitonov, Eugene  and
      Chaabouni, Rahma  and
      Bouchacourt, Diane  and
      Baroni, Marco",
    editor = "Pad{\'o}, Sebastian  and
      Huang, Ruihong",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-3010",
    doi = "10.18653/v1/D19-3010",
    pages = "55--60",
    abstract = "There is renewed interest in simulating language emergence among deep neural agents that communicate to jointly solve a task, spurred by the practical aim to develop language-enabled interactive AIs, as well as by theoretical questions about the evolution of human language. However, optimizing deep architectures connected by a discrete communication channel (such as that in which language emerges) is technically challenging. We introduce EGG, a toolkit that greatly simplifies the implementation of emergent-language communication games. EGG{'}s modular design provides a set of building blocks that the user can combine to create new games, easily navigating the optimization and architecture space. We hope that the tool will lower the technical barrier, and encourage researchers from various backgrounds to do original work in this exciting area.",
}

@article{mandelbrot1953informational,
  title={An informational theory of the statistical structure of language},
  author={Mandelbrot, Benoit and others},
  journal={Communication theory},
  volume={84},
  pages={486--502},
  year={1953},
  publisher={London}
}

@inproceedings{
evtimova2018emergent,
title={Emergent Communication in a Multi-Modal, Multi-Step Referential Game},
author={Katrina Evtimova and Andrew Drozdov and Douwe Kiela and Kyunghyun Cho},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJGZq6g0-},
}

@inproceedings{mu2021generalizations,
  IDS={mu2021emergent,Mu2021EmergentCO,mu2021general},
 author = {Mu, Jesse and Goodman, Noah},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {17994--18007},
 publisher = {Curran Associates, Inc.},
 title = {Emergent Communication of Generalizations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/9597353e41e6957b5e7aa79214fcb256-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{zoph-etal-2016-transfer,
    title = "Transfer Learning for Low-Resource Neural Machine Translation",
    author = "Zoph, Barret  and
      Yuret, Deniz  and
      May, Jonathan  and
      Knight, Kevin",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1163",
    doi = "10.18653/v1/D16-1163",
    pages = "1568--1575",
}

@misc{
perkins2022icy,
title={Icy: A benchmark for measuring compositional inductive bias of emergent communication models},
author={Hugh Perkins},
year={2022},
url={https://openreview.net/forum?id=S352vriz3G}
}

@inproceedings{
  guo2023emergent,
  title={Emergent Communication for Rules Reasoning},
  author={Yuxuan Guo and Yifan Hao and Rui Zhang and Enshuai Zhou and Zidong Du and Xishan Zhang and Xinkai Song and Yuanbo Wen and Yongwei Zhao and Xuehai Zhou and Jiaming Guo and Qi Yi and Shaohui Peng and Di Huang and Ruizhi Chen and Qi Guo and Yunji Chen},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=gx20B4ItIw}
}

@inproceedings{
guo2022expressivity,
title={Expressivity of Emergent Languages is a Trade-off between Contextual Complexity and Unpredictability},
author={Shangmin Guo and Yi Ren and Kory Wallace Mathewson and Simon Kirby and Stefano V Albrecht and Kenny Smith},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=WxuE_JWxjkW}
}

@inproceedings{li2019ease,
author = {Li, Fushan and Bowling, Michael},
title = {Ease-of-Teaching and Language Structure from Emergent Communication},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Artificial agents have been shown to learn to communicate when needed to complete a cooperative task. Some level of language structure (e.g., compositionality) has been found in the learned communication protocols. This observed structure is often the result of specific environmental pressures during training. By introducing new agents periodically to replace old ones, sequentially and within a population, we explore such a new pressure — ease of teaching — and show its impact on the structure of the resulting language.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1420},
numpages = {11}
}

@article{Russakovsky2014ImageNetLS,
  title={ImageNet Large Scale Visual Recognition Challenge},
  author={Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael S. Bernstein and Alexander C. Berg and Li Fei-Fei},
  journal={International Journal of Computer Vision},
  year={2014},
  volume={115},
  pages={211 - 252},
}

@inproceedings{Wang2018GLUEAM,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446/",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions."
}

@article{Marcus1993BuildingAL,
    title = "Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank",
    author = "Marcus, Mitchell P.  and
      Santorini, Beatrice  and
      Marcinkiewicz, Mary Ann",
    editor = "Hirschberg, Julia",
    journal = "Computational Linguistics",
    volume = "19",
    number = "2",
    year = "1993",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J93-2004/",
    pages = "313--330"
}

@InProceedings{bojar2014wmt,
  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale
{s}},
  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {12--58},
  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}
}

@inproceedings{downey-etal-2023-learning,
  IDS={downey2022learning},
    title = "Learning to translate by learning to communicate",
    author = "Downey, C.m.  and
      Zhou, Xuhui  and
      Liu, Zeyu  and
      Steinert-Threlkeld, Shane",
    editor = "Ataman, Duygu",
    booktitle = "Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.mrl-1.17",
    pages = "218--238",
}

@article{lacroix2019biology,
  author       = {Travis LaCroix},
  title        = {Biology and Compositionality: Empirical Considerations for Emergent-Communication
                  Protocols},
  journal      = {CoRR},
  volume       = {abs/1911.11668},
  year         = {2019},
  url          = {http://arxiv.org/abs/1911.11668},
  eprinttype    = {arXiv},
  eprint       = {1911.11668},
  timestamp    = {Tue, 03 Dec 2019 20:41:07 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1911-11668.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li-etal-2020-emergent,
  IDS={li2020emergent,Li2020EmergentCP},
    title = "Emergent Communication Pretraining for Few-Shot Machine Translation",
    author = "Li, Yaoyiran  and
      Ponti, Edoardo Maria  and
      Vuli{\'c}, Ivan  and
      Korhonen, Anna",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.416",
    doi = "10.18653/v1/2020.coling-main.416",
    pages = "4716--4731",
    abstract = "While state-of-the-art models that rely upon massively multilingual pretrained encoders achieve sample efficiency in downstream applications, they still require abundant amounts of unlabelled text. Nevertheless, most of the world{'}s languages lack such resources. Hence, we investigate a more radical form of unsupervised knowledge transfer in the absence of linguistic data. In particular, for the first time we pretrain neural networks via emergent communication from referential games. Our key assumption is that grounding communication on images{---}as a crude approximation of real-world environments{---}inductively biases the model towards learning natural languages. On the one hand, we show that this substantially benefits machine translation in few-shot settings. On the other hand, this also provides an extrinsic evaluation protocol to probe the properties of emergent languages ex vitro. Intuitively, the closer they are to natural languages, the higher the gains from pretraining on them should be. For instance, in this work we measure the influence of communication success and maximum sequence length on downstream performances. Finally, we introduce a customised adapter layer and annealing strategies for the regulariser of maximum-a-posteriori inference during fine-tuning. These turn out to be crucial to facilitate knowledge transfer and prevent catastrophic forgetting. Compared to a recurrent baseline, our method yields gains of 59.0{\%} 147.6{\%} in BLEU score with only 500 NMT training instances and 65.1{\%} 196.7{\%} with 1,000 NMT training instances across four language pairs. These proof-of-concept results reveal the potential of emergent communication pretraining for both natural language processing tasks in resource-poor settings and extrinsic evaluation of artificial languages.",
}

@INPROCEEDINGS{mu2023ec2,
  IDS={mu2023ec},
  author={Mu, Yao and Yao, Shunyu and Ding, Mingyu and Luo, Ping and Gan, Chuang},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={EC2: Emergent Communication for Embodied Control}, 
  year={2023},
  volume={},
  number={},
  pages={6704-6714},
  doi={10.1109/CVPR52729.2023.00648}}

@inproceedings{heusel2017fid,
 author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@inproceedings{artetxe-etal-2020-cross,
    title = "On the Cross-lingual Transferability of Monolingual Representations",
    author = "Artetxe, Mikel  and
      Ruder, Sebastian  and
      Yogatama, Dani",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.421",
    doi = "10.18653/v1/2020.acl-main.421",
    pages = "4623--4637",
    abstract = "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
}

@ONLINE{wikidump,
    author = "{Wikimedia Foundation}",
    title  = "Wikimedia Downloads",
    url    = "https://dumps.wikimedia.org"
}

@inproceedings{
ueda2023on,
title={On the Word Boundaries of Emergent Languages Based on Harris's Articulation Scheme},
author={Ryo Ueda and Taiga Ishii and Yusuke Miyao},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=b4t9_XASt6G}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{Wolf2019HuggingFacesTS,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6/",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}."
}

@inproceedings{pytorch,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{adam,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@techreport{WahCUB_200_2011,
	Title = {The Caltech-{UCSD} Birds-200-2011 Dataset},
	Author = {Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.},
	Year = {2011},
	Institution = {California Institute of Technology},
	Number = {CNS-TR-2011-001}
}

@inproceedings{sharma-etal-2018-conceptual,
    title = "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
    author = "Sharma, Piyush  and
      Ding, Nan  and
      Goodman, Sebastian  and
      Soricut, Radu",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1238",
    doi = "10.18653/v1/P18-1238",
    pages = "2556--2565",
    abstract = "We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",
}

@inproceedings{bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{chrf,
    title = "chr{F}: character n-gram {F}-score for automatic {MT} evaluation",
    author = "Popovi{\'c}, Maja",
    booktitle = "Proceedings of the Tenth Workshop on Statistical Machine Translation",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-3049",
    doi = "10.18653/v1/W15-3049",
    pages = "392--395",
}

@inproceedings{post-2018-call,
  title = "A Call for Clarity in Reporting {BLEU} Scores",
  author = "Post, Matt",
  booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
  month = oct,
  year = "2018",
  address = "Belgium, Brussels",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/W18-6319",
  pages = "186--191",
}

@inproceedings{xferbench,
    title = "{X}fer{B}ench: a Data-Driven Benchmark for Emergent Language",
    author = "Boldt, Brendon  and
      Mortensen, David R.",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.82",
    doi = "10.18653/v1/2024.naacl-long.82",
    pages = "1475--1489",
    abstract = "In this paper, we introduce a benchmark for evaluating the overall quality of emergent languages using data-driven methods. Specifically, we interpret the notion of the {``}quality{''} of an emergent language as its similarity to human language within a deep learning framework. We measure this by using the emergent language as pretraining data for a downstream NLP tasks in human language{---}the better the downstream performance, the better the emergent language. We implement this benchmark as an easy-to-use Python package that only requires a text file of utterances from the emergent language to be evaluated. Finally, we empirically test the benchmark{'}s validity using human, synthetic, and emergent language baselines.",
}

@article{boldt2022recommendations,
  title={Recommendations for systematic research on emergent language},
  author={Boldt, Brendon and Mortensen, David R.},
  archivePrefix={arXiv},
  eprint={2206.11302},
  year={2022}
}

@article{
boldt2024review,
title={A Review of the Applications of Deep Learning-Based Emergent Communication},
author={Brendon Boldt and David R. Mortensen},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=jesKcQxQ7j},
}

@misc{
boldt2022shaped,
title={Shaped Rewards Bias Emergent Language},
author={Brendon Boldt and Yonatan Bisk and David R Mortensen},
year={2022},
url={https://openreview.net/forum?id=057dxuWpfx}
}

@inproceedings{prabhumoye-etal-2021-case,
    title = "Case Study: Deontological Ethics in {NLP}",
    author = "Prabhumoye, Shrimai  and
      Boldt, Brendon  and
      Salakhutdinov, Ruslan  and
      Black, Alan W",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.297",
    doi = "10.18653/v1/2021.naacl-main.297",
    pages = "3784--3798",
    abstract = "Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.",
}

@incollection{greenberg1963universals,
  title={Some universals of grammar with particular reference to the order of meaningful elements},
  pages={58--90},
  booktitle={Universals of Language},
  publisher={The MIT Press},
  address={Cambridge, MA},
  author={Joseph H. Greenberg},
  editor={Joseph H. Greenberg},
  year=1963,
  url={https://archive.org/details/universalsoflang00unse}, 
}

@inproceedings{foerster2016learning,
  IDS={Foerster2016LearningTC},
author = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
title = {Learning to communicate with Deep multi-agent reinforcement learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2145–2153},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@article{werner_Dyer_1991,
    title={Evolution of Communication in Artificial Organisms},
    author={Werner, Gregory M. and Dyer, Michael G.},
    year={1991},
    pages={659 - 687},
    journal={Artificial Life II},
    editor={Langton, C. G. and Taylor, C. and Farmer, J. D., and Rasmussen, S.},
    publisher={Addison-Wesley}
}

@misc{baker2020emergenttoolusemultiagent,
      title={Emergent Tool Use From Multi-Agent Autocurricula}, 
      author={Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
      year={2020},
      eprint={1909.07528},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1909.07528}, 
}

@article{brighton2005,
title = {Language as an evolutionary system},
journal = {Physics of Life Reviews},
volume = {2},
number = {3},
pages = {177-226},
year = {2005},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2005.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1571064505000229},
author = {Henry Brighton and Kenny Smith and Simon Kirby},
keywords = {Language, Evolution, Artificial life, Culture, Adaptation, Replication},
abstract = {John Maynard Smith and Eörs Szathmáry argued that human language signified the eighth major transition in evolution: human language marked a new form of information transmission from one generation to another [Maynard Smith J, Szathmáry E. The major transitions in evolution. Oxford: Oxford Univ. Press; 1995]. According to this view language codes cultural information and as such forms the basis for the evolution of complexity in human culture. In this article we develop the theory that language also codes information in another sense: languages code information on their own structure. As a result, languages themselves provide information that influences their own survival. To understand the consequences of this theory we discuss recent computational models of linguistic evolution. Linguistic evolution is the process by which languages themselves evolve. This article draws together this recent work on linguistic evolution and highlights the significance of this process in understanding the evolution of linguistic complexity. Our conclusions are that: (1) the process of linguistic transmission constitutes the basis for an evolutionary system, and (2), that this evolutionary system is only superficially comparable to the process of biological evolution.}
}

@book{lewis1970ConventionAP,
	address = {Cambridge, MA, USA},
	author = {David Kellogg Lewis},
	editor = {},
	publisher = {Wiley-Blackwell},
	title = {Convention: A Philosophical Study},
	year = {1969}
}

@misc{lazaridou2020review,
  IDS={lazaridou2020emergentmultiagentcommunicationdeep,lazaridou2020emergent,Lazaridou2020EmergentMC},
      title={Emergent Multi-Agent Communication in the Deep Learning Era}, 
      author={Angeliki Lazaridou and Marco Baroni},
      year={2020},
      eprint={2006.02419},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.02419}, 
}

@book{kuhn,
	address = {Chicago, IL},
	author = {Thomas S. Kuhn},
	publisher = {University of Chicago Press},
	title = {The Structure of Scientific Revolutions},
	year = {1962}
}

@misc{boldt2023mathematicallymodelinglexiconentropy,
      IDS={boldt2023mathmodel,boldt2022mathematically},
      title={Mathematically Modeling the Lexicon Entropy of Emergent Language}, 
      author={Brendon Boldt and David R. Mortensen},
      year={2022},
      eprint={2211.15783},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.15783}, 
}

@article{bitter-lesson,
  title={The {B}itter {L}esson},
  year={2019},
  author={Richard Sutton},
  url={http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
} \cmt{cite}

@misc{elcc,
      title={{ELCC}: the {E}mergent {L}anguage {C}orpus {C}ollection}, 
      author={Brendon Boldt and David R. Mortensen},
      year={2024},
      eprint={2407.04158},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.04158}, 
}

@misc{weidinger2021ethicalsocialrisksharm,
    title={Ethical and social risks of harm from Language Models}, 
    author={Laura Weidinger and John Mellor and Maribeth Rauh and Conor Griffin and Jonathan Uesato and Po-Sen Huang and Myra Cheng and Mia Glaese and Borja Balle and Atoosa Kasirzadeh and Zac Kenton and Sasha Brown and Will Hawkins and Tom Stepleton and Courtney Biles and Abeba Birhane and Julia Haas and Laura Rimell and Lisa Anne Hendricks and William Isaac and Sean Legassick and Geoffrey Irving and Iason Gabriel},
    year={2021},
    eprint={2112.04359},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2112.04359}, 
}

@misc{carlini2021extractingtrainingdatalarge,
      title={Extracting Training Data from Large Language Models}, 
      author={Nicholas Carlini and Florian Tramer and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown and Dawn Song and Ulfar Erlingsson and Alina Oprea and Colin Raffel},
      year={2021},
      eprint={2012.07805},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2012.07805}, 
}

@article{terry2021pettingzoo,
  title={Petting{Z}oo: Gym for multi-agent reinforcement learning},
  author={Terry, J and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario and Hari, Ananth and Sullivan, Ryan and Santos, Luis S and Dieffendahl, Clemens and Horsch, Caroline and Perez-Vicente, Rodrigo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={15032--15043},
  year={2021}
}

@software{minari,
	author = {Younis, Omar G. and Perez-Vicente, Rodrigo and Balis, John U. and Dudley, Will and Davey, Alex and Terry, Jordan K},
	doi = {10.5281/zenodo.13767625},
	month = sep,
	publisher = {Zenodo},
	title = {Minari},
	url = {https://doi.org/10.5281/zenodo.13767625},
	version = {0.5.0},
	year = 2024,
	bdsk-url-1 = {https://doi.org/10.5281/zenodo.13767625}
}

@InProceedings{jaques2019social,
  title = 	 {Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning},
  author =       {Jaques, Natasha and Lazaridou, Angeliki and Hughes, Edward and Gulcehre, Caglar and Ortega, Pedro and Strouse, Dj and Leibo, Joel Z. and De Freitas, Nando},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3040--3049},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/jaques19a/jaques19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/jaques19a.html},
  abstract = 	 {We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents’ actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents’ behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. Consequently, the influence reward opens up a window of new opportunities for research in this area.}
}

@inproceedings{
lipinski2024speaking,
title={Speaking Your Language: Spatial Relationships in Interpretable Emergent Communication},
author={Olaf Lipinski and Adam Sobey and Federico Cerutti and Timothy J. Norman},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=vIP8IWmZlN}
}

@inproceedings{bouma2009npmi,
  title={{N}ormalized ({P}ointwise) {M}utual {I}nformation in collocation extraction},
  year=2009,
  author={Bouma, Gerlof},
  publisher={Gunter Narr Verlag},
  editors={Christian Chiarcos and Richard Eckart de Castilho and Manfred Stede},
  url={https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf},
  booktitle={Proceedings of the Biennial German Society for Computational Linguistics and Language Technology Conference},
}

@inproceedings{
ueda2022categorial,
title={Categorial Grammar Induction as a Compositionality Measure for Emergent Languages in Signaling Games},
author={Ryo Ueda and Taiga Ishii and Koki Washio and Yusuke Miyao},
booktitle={Emergent Communication Workshop at ICLR 2022},
year={2022},
url={https://openreview.net/forum?id=Sbgb7b0Q-5}
}

@inproceedings{chaabouni-etal-2019-word,
    title = "Word-order Biases in Deep-agent Emergent Communication",
    author = "Chaabouni, Rahma  and
      Kharitonov, Eugene  and
      Lazaric, Alessandro  and
      Dupoux, Emmanuel  and
      Baroni, Marco",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1509",
    doi = "10.18653/v1/P19-1509",
    pages = "5166--5175",
    abstract = "Sequence-processing neural networks led to remarkable progress on many NLP tasks. As a consequence, there has been increasing interest in understanding to what extent they process language as humans do. We aim here to uncover which biases such models display with respect to {``}natural{''} word-order constraints. We train models to communicate about paths in a simple gridworld, using miniature languages that reflect or violate various natural language trends, such as the tendency to avoid redundancy or to minimize long-distance dependencies. We study how the controlled characteristics of our miniature languages affect individual learning and their stability across multiple network generations. The results draw a mixed picture. On the one hand, neural networks show a strong tendency to avoid long-distance dependencies. On the other hand, there is no clear preference for the efficient, non-redundant encoding of information that is widely attested in natural language. We thus suggest inoculating a notion of {``}effort{''} into neural networks, as a possible way to make their linguistic behavior more human-like.",
}

@inproceedings{
conklin2023compositionality,
title={Compositionality with Variation Reliably Emerges in Neural Networks},
author={Henry Conklin and Kenny Smith},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=-Yzz6vlX7V-}
}

@inproceedings{jiang-etal-2019-tiger,
    title = "{TIGE}r: Text-to-Image Grounding for Image Caption Evaluation",
    author = "Jiang, Ming  and
      Huang, Qiuyuan  and
      Zhang, Lei  and
      Wang, Xin  and
      Zhang, Pengchuan  and
      Gan, Zhe  and
      Diesner, Jana  and
      Gao, Jianfeng",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1220/",
    doi = "10.18653/v1/D19-1220",
    pages = "2141--2152",
    abstract = "This paper presents a new metric called TIGEr for the automatic evaluation of image captioning systems. Popular metrics, such as BLEU and CIDEr, are based solely on text matching between reference captions and machine-generated captions, potentially leading to biased evaluations because references may not fully cover the image content and natural language is inherently ambiguous. Building upon a machine-learned text-image grounding model, TIGEr allows to evaluate caption quality not only based on how well a caption represents image content, but also on how well machine-generated captions match human-generated captions. Our empirical tests show that TIGEr has a higher consistency with human judgments than alternative existing metrics. We also comprehensively assess the metric`s effectiveness in caption evaluation by measuring the correlation between human judgments and metric scores."
}

@InProceedings{Yeh_2018_CVPR,
author = {Yeh, Raymond A. and Do, Minh N. and Schwing, Alexander G.},
title = {Unsupervised Textual Grounding: Linking Words to Image Concepts},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@article{Taniguchi17062016,
author = {Tadahiro Taniguchi and Takayuki Nagai and Tomoaki Nakamura and Naoto Iwahashi and Tetsuya Ogata and Hideki Asoh},
title = {Symbol emergence in robotics: a survey},
journal = {Advanced Robotics},
volume = {30},
number = {11-12},
pages = {706--728},
year = {2016},
publisher = {Taylor \& Francis},
doi = {10.1080/01691864.2016.1164622},
URL = {https://doi.org/10.1080/01691864.2016.1164622},
eprint = {https://doi.org/10.1080/01691864.2016.1164622}
}

@inproceedings{goldwater-etal-2006-contextual,
    title = "Contextual Dependencies in Unsupervised Word Segmentation",
    author = "Goldwater, Sharon  and
      Griffiths, Thomas L.  and
      Johnson, Mark",
    editor = "Calzolari, Nicoletta  and
      Cardie, Claire  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P06-1085/",
    doi = "10.3115/1220175.1220260",
    pages = "673--680"
}

@inproceedings{mochihashi-etal-2009-bayesian,
    title = "{B}ayesian Unsupervised Word Segmentation with Nested {P}itman-{Y}or Language Modeling",
    author = "Mochihashi, Daichi  and
      Yamada, Takeshi  and
      Ueda, Naonori",
    editor = "Su, Keh-Yih  and
      Su, Jian  and
      Wiebe, Janyce  and
      Li, Haizhou",
    booktitle = "Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP}",
    month = aug,
    year = "2009",
    address = "Suntec, Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P09-1012/",
    pages = "100--108"
}

@article{brent1999efficient,
author = {Brent, Michael R.},
title = {An Efficient, Probabilistically Sound Algorithm for Segmentation andWord Discovery},
year = {1999},
issue_date = {Feb. 1999},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {34},
number = {1–3},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1007541817488},
doi = {10.1023/A:1007541817488},
abstract = {This paper presents a model-based, unsupervised algorithm
for recovering word boundaries in a natural-language text from which
they have been deleted. The algorithm is derived from a probability
model of the source that generated the text. The fundamental
structure of the model is specified abstractly so that the detailed
component models of phonology, word-order, and word frequency can be
replaced in a modular fashion. The model yields a
language-independent, prior probability distribution on all possible
sequences of all possible words over a given alphabet, based on the
assumption that the input was generated by concatenating words from a
fixed but unknown lexicon. The model is unusual in that it treats
the generation of a complete corpus, regardless of length, as a
single event in the probability space. Accordingly, the algorithm
does not estimate a probability distribution on words; instead, it
attempts to calculate the prior probabilities of various word
sequences that could underlie the observed text. Experiments on
phonemic transcripts of spontaneous speech by parents to young
children suggest that our algorithm is more effective than other
proposed algorithms, at least when utterance boundaries are given and
the text includes a substantial number of short utterances.},
journal = {Mach. Learn.},
month = feb,
pages = {71–105},
numpages = {35},
keywords = {unsupervised learning, segmentation, probability models, minimum description length (MDL), language acquisition, bayesian grammar induction}
}

@inproceedings{kim-etal-2019-unsupervised,
    title = "Unsupervised Recurrent Neural Network Grammars",
    author = "Kim, Yoon  and
      Rush, Alexander  and
      Yu, Lei  and
      Kuncoro, Adhiguna  and
      Dyer, Chris  and
      Melis, G{\'a}bor",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1114/",
    doi = "10.18653/v1/N19-1114",
    pages = "1105--1117",
    abstract = "Recurrent neural network grammars (RNNG) are generative models of language which jointly model syntax and surface structure by incrementally generating a syntax tree and sentence in a top-down, left-to-right order. Supervised RNNGs achieve strong language modeling and parsing performance, but require an annotated corpus of parse trees. In this work, we experiment with unsupervised learning of RNNGs. Since directly marginalizing over the space of latent trees is intractable, we instead apply amortized variational inference. To maximize the evidence lower bound, we develop an inference network parameterized as a neural CRF constituency parser. On language modeling, unsupervised RNNGs perform as well their supervised counterparts on benchmarks in English and Chinese. On constituency grammar induction, they are competitive with recent neural language models that induce tree structures from words through attention mechanisms."
}

@inproceedings{kim-etal-2019-compound,
    title = "Compound Probabilistic Context-Free Grammars for Grammar Induction",
    author = "Kim, Yoon  and
      Dyer, Chris  and
      Rush, Alexander",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1228/",
    doi = "10.18653/v1/P19-1228",
    pages = "2369--2385",
    abstract = "We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models."
}

@inbook{Kirby_2002, place={Cambridge}, title={Learning, bottlenecks and the evolution of recursive syntax}, booktitle={Linguistic Evolution through Language Acquisition}, publisher={Cambridge University Press}, author={Kirby, Simon}, editor={Briscoe, TedEditor}, year={2002}, pages={173–204}}

@phdthesis{brighton2003thesis,
  author = "Brighton, Henry",
  year = 2003,
  title = "Simplicity as a driving force in linguistic evolution",
  school = "University of Edinburgh",
  address = "Edinburgh, Scotland, UK",
  url = {http://hdl.handle.net/1842/23810},
}

@inproceedings{boldt2025searching,
    title = "Searching for the Most Human-like Emergent Language",
    author = "Boldt, Brendon  and
      Mortensen, David R.",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.1188/",
    doi = "10.18653/v1/2025.emnlp-main.1188",
    pages = "23289--23307",
    ISBN = "979-8-89176-332-6",
    abstract = "In this paper, we design a signalling game-based emergent communication environment to generate state-of-the-art emergent languages in terms of similarity to human language. This is done with hyperparameter optimization, using XferBench as the objective function. XferBench quantifies the statistical similarity of emergent language to human language by measuring its suitability for deep transfer learning to human language. Additionally, we demonstrate the predictive power of entropy on the transfer learning performance of emergent language as well as corroborate previous results on the entropy-minimization properties of emergent communication systems. Finally, we report generalizations regarding what hyperparameters produce more realistic emergent languages, that is, ones which transfer better to human language."
}

@article{steels1997synthetic,
   author = "Steels, Luc",
   title = "The Synthetic Modeling of Language Origins", 
   journal= "Evolution of Communication",
   year = "1997",
   volume = "1",
   number = "1",
   pages = "1-34",
   doi = "https://doi.org/10.1075/eoc.1.1.02ste",
   url = "https://www.jbe-platform.com/content/journals/10.1075/eoc.1.1.02ste",
   publisher = "John Benjamins",
   issn = "1387-5337",
   type = "Journal Article",
   abstract = "This paper surveys work on the computational modeling of the origins and evolution of language. The main approaches are described and some example experiments from the domains of the evolution of communication, phonetics, lexicon formation, and syntax are discussed.",
  }

@article{kirby2002alife,
    author = {Kirby, Simon},
    title = {Natural Language From Artificial Life},
    journal = {Artificial Life},
    volume = {8},
    number = {2},
    pages = {185-215},
    year = {2002},
    month = {04},
    abstract = {This article aims to show that linguistics, in particular the study of the lexico-syntactic aspects of language, provides fertile ground for artificial life modeling. A survey of the models that have been developed over the last decade and a half is presented to demonstrate that ALife techniques have a lot to offer an explanatory theory of language. It is argued that this is because much of the structure of language is determined by the interaction of three complex adaptive systems: learning, culture, and biological evolution. Computational simulation, informed by theoretical linguistics, is an appropriate response to the challenge of explaining real linguistic data in terms of the processes that underpin human language.},
    issn = {1064-5462},
    doi = {10.1162/106454602320184248},
    url = {https://doi.org/10.1162/106454602320184248},
    eprint = {https://direct.mit.edu/artl/article-pdf/8/2/185/1661892/106454602320184248.pdf},
}

@article{maclennan,
author = {Bruce J. MacLennan and Gordon M. Burghardt},
title ={Synthetic Ethology and the Evolution of Cooperative Communication},

journal = {Adaptive Behavior},
volume = {2},
number = {2},
pages = {161-188},
year = {1993},
doi = {10.1177/105971239300200203},
URL = { https://doi.org/10.1177/105971239300200203 },
eprint = {https://doi.org/10.1177/105971239300200203}
,
    abstract = { Synthetic ethology is proposed as a means of conducting controlled experiments investigating the mechanisms and evolution of communication. After a discussion of the goals and methods of synthetic ethology, two series of experiments are described based on at least 5000 breeding cycles. The first demonstrates the evolution of cooperative communication in a population of simple machines. The average fitness of the population and the organization of its use of signals are compared under three conditions: communication suppressed, communication permitted, and communication permitted in the presence of learning. Where communication is permitted, the fitness increases approximately 26 times faster than when communication is suppressed; with communication and learning, the rate of fitness increase is nearly 100-fold The second series of experiments illustrates the evolution of a syntactically simple language, in which a pair of signals is required for effective communication. }
}

@article{smith2002cultural,
author = {Kenny Smith},
title = {The cultural evolution of communication in a population of neural networks},
journal = {Connection Science},
volume = {14},
number = {1},
pages = {65--84},
year = {2002},
publisher = {Taylor \& Francis},
doi = {10.1080/09540090210164306},
URL = { https://doi.org/10.1080/09540090210164306 },
eprint = { https://doi.org/10.1080/09540090210164306 }
}

@article{steels1999heads,
author = {Steels, Luc L.},
title = {The talking heads experiment},
year = {1999},
series = {Words and meanings},
volume = {Volume 1},
address = {Antwerpen},
publisher = {Laboratorium},
doi = {10.17169/FUDOCS_document_000000022455}
}

@incollection{batali1998grammar,
  author={Batali, John},
  address={Cambridge},
  booktitle={Approaches to the evolution of language: Social and Cognitive bases},
  title={Computational simulations of the emergence of grammar},
  year={1998},
  pages={405--426},
  publisher={Cambridge University Press},
}

@inproceedings{kirby1997learning,
  title={Learning, culture and evolution in the origin of linguistic constraints},
  author={Kirby, Simon and Hurford, James},
  booktitle={Fourth European conference on artificial life},
  pages={493--502},
  year={1997},
  organization={Cambridge (MS): MIT Press}
}

@article{Baroni_2020, title={Linguistic generalization and compositionality in modern artificial neural networks}, volume={375}, ISSN={0962-8436, 1471-2970}, DOI={10.1098/rstb.2019.0307}, note={arXiv: 1904.00157}, number={1791}, journal={Philosophical Transactions of the Royal Society B: Biological Sciences}, author={Baroni, Marco}, year={2020}, month={Feb}, pages={20190307} }

@inproceedings{narayanchen2019collaborative, address={Florence, Italy}, title={Collaborative Dialogue in Minecraft}, url={https://aclanthology.org/P19-1537}, DOI={10.18653/v1/P19-1537}, abstractNote={We wish to develop interactive agents that can communicate with humans to collaboratively solve tasks in grounded scenarios. Since computer games allow us to simulate such tasks without the need for physical robots, we define a Minecraft-based collaborative building task in which one player (A, the Architect) is shown a target structure and needs to instruct the other player (B, the Builder) to build this structure. Both players interact via a chat interface. A can observe B but cannot place blocks. We present the Minecraft Dialogue Corpus, a collection of 509 conversations and game logs. As a first step towards our goal of developing fully interactive agents for this task, we consider the subtask of Architect utterance generation, and show how challenging it is.}, booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, publisher={Association for Computational Linguistics}, author={Narayan-Chen, Anjali and Jayannavar, Prashant and Hockenmaier, Julia}, year={2019}, month={Jul}, pages={5405–5415} }

@inproceedings{rodriguez-luna-etal-2020-internal,
  IDS={2004.03868,rodriguez2020internal,Luna2020InternalAE},
    title = "Internal and external pressures on language emergence: least effort, object constancy and frequency",
    author = "Rodr{\'i}guez Luna, Diana  and
      Ponti, Edoardo Maria  and
      Hupkes, Dieuwke  and
      Bruni, Elia",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.397/",
    doi = "10.18653/v1/2020.findings-emnlp.397",
    pages = "4428--4437",
    abstract = "In previous work, artificial agents were shown to achieve almost perfect accuracy in referential games where they have to communicate to identify images. Nevertheless, the resulting communication protocols rarely display salient features of natural languages, such as compositionality. In this paper, we propose some realistic sources of pressure on communication that avert this outcome. More specifically, we formalise the principle of least effort through an auxiliary objective. Moreover, we explore several game variants, inspired by the principle of object constancy, in which we alter the frequency, position, and luminosity of the objects in the images. We perform an extensive analysis on their effect through compositionality metrics, diagnostic classifiers, and zero-shot evaluation. Our findings reveal that the proposed sources of pressure result in emerging languages with less redundancy, more focus on high-level conceptual information, and better abilities of generalisation. Overall, our contributions reduce the gap between emergent and natural languages."
}

@inproceedings{10.5555/3504035.3504218,
  IDS={mordatch2018emergence,Mordatch2018EmergenceOG,mordatch2018grounded,mordatch2017emergence},
author = {Mordatch, Igor and Abbeel, Pieter},
title = {Emergence of grounded compositional language in multi-agent populations},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of nonverbal communication such as pointing and guiding when language communication is unavailable.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {183},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{portelance-etal-2021-emergence,
  IDS={2109.06232,portelance2021the,Portelance2021TheEO},
    title = "The Emergence of the Shape Bias Results from Communicative Efficiency",
    author = "Portelance, Eva  and
      Frank, Michael C.  and
      Jurafsky, Dan  and
      Sordoni, Alessandro  and
      Laroche, Romain",
    editor = "Bisazza, Arianna  and
      Abend, Omri",
    booktitle = "Proceedings of the 25th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.conll-1.48/",
    doi = "10.18653/v1/2021.conll-1.48",
    pages = "607--623",
    abstract = "By the age of two, children tend to assume that new word categories are based on objects' shape, rather than their color or texture; this assumption is called the shape bias. They are thought to learn this bias by observing that their caregiver{'}s language is biased towards shape based categories. This presents a chicken and egg problem: if the shape bias must be present in the language in order for children to learn it, how did it arise in language in the first place? In this paper, we propose that communicative efficiency explains both how the shape bias emerged and why it persists across generations. We model this process with neural emergent language agents that learn to communicate about raw pixelated images. First, we show that the shape bias emerges as a result of efficient communication strategies employed by agents. Second, we show that pressure brought on by communicative need is also necessary for it to persist across generations; simply having a shape bias in an agent{'}s input language is insufficient. These results suggest that, over and above the operation of other learning strategies, the shape bias in human learners may emerge and be sustained by communicative pressures."
}

@inproceedings{rita-etal-2020-lazimpa,
    title = "``{L}az{I}mpa'': Lazy and Impatient neural agents learn to communicate efficiently",
    author = "Rita, Mathieu  and
      Chaabouni, Rahma  and
      Dupoux, Emmanuel",
    editor = "Fern{\'a}ndez, Raquel  and
      Linzen, Tal",
    booktitle = "Proceedings of the 24th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.conll-1.26/",
    doi = "10.18653/v1/2020.conll-1.26",
    pages = "335--343",
    abstract = "Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, ``LazImpa'', where the speaker is made increasingly lazy, i.e., avoids long messages, and the listener impatient, i.e., seeks to guess the intended content as soon as possible.",
  IDS={2010.01878,rita2020lazimpa,Rita2020LazImpaLA},
}

@inproceedings{
rita2022on,
title={On the role of population heterogeneity in emergent communication},
author={Mathieu Rita and Florian Strub and Jean-Bastien Grill and Olivier Pietquin and Emmanuel Dupoux},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=5Qkd7-bZfI},
  IDS={2204.12982,Rita2022OnTR},
}

@misc{openai2019dota2largescale,
      title={Dota 2 with Large Scale Deep Reinforcement Learning}, 
      author={OpenAI and : and Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemysław Dębiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Chris Hesse and Rafal Józefowicz and Scott Gray and Catherine Olsson and Jakub Pachocki and Michael Petrov and Henrique P. d. O. Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and Ilya Sutskever and Jie Tang and Filip Wolski and Susan Zhang},
      year={2019},
      eprint={1912.06680},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.06680}, 
}
