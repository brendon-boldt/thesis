%%%%%%%%%%%%%%%%%%%
\chapter*{Prologue}\label{ch:prologue}
%%%%%%%%%%%%%%%%%%%
The year is 2018.
In the last couple years,
  AlphaGo defeated Lee Sedol, reigning Go world champion \citep{bbc-alphago}.
  OpenAI has been making for itself as a reinforcement learning lab with OpenAI Gym \citep{openaigym} and OpenAI Five \citep{openai2019dota2largescale}.
  New deep algorithms are being introduced left and right:
    Deep Q-Network, Advantage Actor Critic, Deep Deterministic Policy Gradient, Trust Region Policy Optimization, Proximal Policy Optimization
    \citep{li2018rlhistory}.
And now, AlphaZero \citep{silver2017masteringchessshogiselfplay} has made its predecessor, AlphaGo, obsolete by discarding human-generated training data and doubling down on the power of deep reinforcement learning.
It is in this milieu that I begin to develop the ideas that would grow into this thesis.
Richard Sutton's ``Bitter Lesson'' is only a year way from this, all but declaring application-specific human expertise dead and done for at the hands of scaling compute, parameters, and data.

Expert-designed systems reigned in games like chess, starting with Deep Blue's 1997 victory over Garry Kasparov \citep{deepblue}, but AlphaZero, replacing domain knowledge with deep learning-driven search, dealt a mighty blow to the paradigm's dominance.
It is here we can draw an analogy with the subject matter of this thesis:
The origins of human language and its evolution had for decades been studied through expert-designed simulations based on theoretically-motivated models (i.e., \emph{emergent language}) \citep{kirby2002alife}, but perhaps deep reinforcement learning could revolutionize the study of language evolution like it had many a time before.
Just as AlphaZero had re-derived many chess strategies based only the rules of the game (as well as found some new ones), maybe similar techniques could rederive the communication strategies which characterize human language.
Toward this hope, papers such as \citet{foerster2016learning,lazaridou2016multiagent,havrylov2017emergence}
  took the first steps towards harnessing deep RL for emergent language.

One way to look at scientific exploration through simulation is searching for the ``interesting'' interface between order and chaos much like the boundary region of the Mandelbrot set.
Too much order? Uninteresting.
The simulation is so tightly defined that results are obvious from the starting conditions.
Too much chaos? Uninteresting.
The simulation so unconstrained so as to exhibit no discernible patterns or just collapse to a trivial outcome.
While the classical approaches to emergent language start with order and introduce chaos to find the interesting, deep learning-based emergent language begins in the chaos: design the environment and let multi-agent reinforcement learning work its magic, hopefully the order is there.

\vfill
\begin{center}
But if the order is not there, what are we to do? \\
\bigskip
Read this thesis.
\end{center}
\vfill





%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%

Large language models are good at mimicking human language.
Some might even say they are good at \emph{using} human language, but this is either imprecise or inaccurate:
  LLMs' production of text is based the statistical likelihood of meaningless (so far as they are concerned) tokens, fine-tuned to humans' preferences.
This is in contrast to humans' use---and even more so their acquisition---of language which is laden with meaning derived from the rich internal, physical, and social context which permeates language use.
The end result is that LLMs' approximation of human language fails at a number of tasks but, more significantly, is limited in providing insights into the nature of human language itself.

\emph{Emergent language} (also known as \emph{emergent communication}\footnote{I \emph{will} use these interchangeably.}) is an alternative paradigm to developing language-capable models that does not train on human language data but rather invents a communication system \emph{de novo}.
In its most basic form, emergent language comprises a simulation using neural network-based agents which are trained to cooperatively complete some task in a virtual environment.
These agents are equipped with a communication channel of discrete tokens with no \emph{a priori} meaning---the meaning of the messages is established through the optimization process encouraging communication which is advantageous to completing the task, more analogous to humans' use of communication.
% The communication protocol, then, is the ``emergent language'' since the meaningful system of communication as arisen organically from the environment and task.

Emergent language differs from the more ``traditional'' approach to modeling language that LLMs use in that it does not try to mimic patterns in human language but instead tries to rederive such patterns from functional pressures analogous to those hypothesized to have guided human language's own evolution.
Since this process language emerging is far more analogous to how human language develops and is learned, it has a much greater potential to contribute to the scientific understanding of human language.
Furthermore, certain practical tasks might lie beyond the reach of the mimicry approach LLMs employ due to the often superficial ``understanding'' they have of language; these problems, too, can be addressed by emergent language which models not only the surface features of language but also its semantics and social context.
Finally, generating language data through emergent communication avoids many of the ethical issues that crop up with LLMs' dependence on human language data from amplifying toxicity from the Web to freely (ab)using copyrighted and personal content \citep{weidinger2021ethicalsocialrisksharm,carlini2021extractingtrainingdatalarge}.

Here is the problem: despite its potential, the field of emergent communication has not made measurable progress toward its more revolutionary applications to scientific understanding or practical problems.
This thesis, then, introduces resources and algorithms for emergent language that enable measurable progress in the field, enabling practitioners to tackle its most significant applications through cumulative contributions.
It accomplishes this first by reviewing the particular ways in which emergent communication can solve practical applications and improve scientific understanding.
This is followed by introducing emergent language data resources which permit empirical evaluation across a variety of emergent languages.
These resources are then used to develop environment-agnostic analytical algorithms centering on the key applications of emergent communication.
Namely, this thesis introduces
  (1) a deep transfer learning-based evaluation metric for emergent communication to measure the practical applicability of emergent language
  and (2) algorithms for discovering morphology of emergent languages as a foundation for further linguistic analysis.
Such tools help to discern order among the often chaotic output of these simulations.

\section{Background}

\paragraph{Agent-based models of language evolution}
The origin of human language has been a perennial question of human inquiry from ancient mythology to contemporary linguistics.
While much of the history of this question is relevant to this work, I will limit this discussion to the most salient approach: namely that of computer simulations investigating the origin and evolution of human language.
This intersection between linguistics and computer science gained momentum in the 1990's with the advances in computational power enabling the use of \emph{agent-based modeling} for simulations of communication systems \citep{steels1997synthetic}.
Agent based models are simulations which model systems at the level of \emph{individuals} and their interactions in an environment;
  such models allow us to see how ``global properties emerge by local interactions'' in order to better understand complex systems like language \citep{steels1997synthetic}.

Employing agent-based models to study the origin and evolution of human language is what is termed \emph{emergent language} (or \emph{emergent communication}) insofar as the phenomenon of language (or at least some communication system) emerges from the features of the agents and their environment.
Such techniques have been applied to a number of closely related topics including \citep{kirby2002alife}:
  how genetically-defined communication systems (like animal communication) might spontaneously emerge \citep{maclennan},
  how learned systems (like human language) are transmitted \citep{smith2002cultural},
  how semantic and syntax could emerge in the context of language \citep{steels1999heads,batali1998grammar},
  and how language and language \emph{acquisition} could co-evolve \citep{kirby1997learning}.

% \cmt{Should I give a one-to-two sentence summary of the paradigm of CEL, or is that what I just did?}
% \cmt{Maybe highlight the discovery of IL as an effective explanation.}
% \cmt{Discuss how CEL is particularly suited for testing particular theories.}


\paragraph{Deep learning vs.\@ classical approaches}
The work mentioned above I term \emph{classical} emergent language (CEL) insofar as it uses more historically typical modeling techniques: namely, the behavior of the agents and the environment is largely hardcoded into the implementation as part of the design of the simulation.
The learning behaviors employed by the agents are relatively simple and constrained, drawing from classical AI and machine learning techniques (e.g., symbolic methods, Bayesian learning).
In short, CEL builds computer models to test formal models.

Deep learning-based emergent language (DEL)---the body of literature in which this thesis is most directly situated---it is the rebellious child of CEL and the deep reinforcement learning boom of the late 2010's (cf.\@ \hyperref[ch:prologue]{the Prologue}).
It applies the techniques of deep neural networks and multi-agent reinforcement learning to the same questions presented by CEL.\footnote{In theory, DEL is concerned with the same questions, but in practice, the bulk of investigation is limited to the emergence of semantics and syntax, as mentioned above.}
I say rebellious here because, rather than being a harmonious union of the two,
  (1) DEL diverges from deep reinforcement learning by being more concerned with the byproducts of the learning process (viz.\@ the communication protocols) than with effectively solving the particular tasks (communicative success),
  and (2) DEL diverges from CEL by a relative absence of grounding in the broader linguistics literature, often starting nearly from scratch in the formulation of hypotheses and experimental approaches.
    Furthermore, implementing any such formal models from CEL in DEL is made difficult by the complexity of deep learning approaches.

Nevertheless, the intention of applying deep learning to emergent language is founded in improving the realism of the agent-based models which are key to the classical approach.
In fact,
  just as Simon Kirby argues for agent-based modeling\footnote{Referred to ``artificial life'' in the cited work.} approaches for studying the evolution of language beyond ``the classical alternative---analytic mathematical modeling---[which] may require the kinds of idealization that will necessitate the removal of the very network of interactions that give rise to the target of explanation'' \citep{kirby2002alife},
  DEL takes the next step on the path of removing such idealizations still present in CEL to further extend the range of observable phenomena.
Specifically, deep learning exhibits two features towards this end:
First, it has a greater representational capacity insofar as the complexity and nuance of phenomena it can represent exceeds what is possible with classical approaches (cf.\@ the ubiquity of deep learning in contemporary computer vision and natural language processing).
Second, it also offers the prospect of introducing fewer inductive biases into the model of language evolution (via its generally-applicable learning processes), leading to fewer assumptions being built into the simulation.
Finally, the shift to deep learning methods also opens up further possible applications to engineering challenges such as creating realistic synthetic language data at scale or developing robust multi-agent communication protocols.
% ``In complex dynamical systems, verbal theorizing often leads to incorrect predictions because our intuitions about the links between local interactions and global behavior are notoriously unreliable. Furthermore, the classical alternative---analytic mathematical modeling---may require the kinds of idealization that will necessitate the removal of the very network of interactions that give rise to the target of explanation.'' \cmt{cite Kirby}


\paragraph{Deep learning-based emergent language}
Turning our attention now fully to deep learning-based emergent language,
  this subfield has its genesis in 2016 with seminal papers such as
  ``Learning to communicate with deep multi-agent reinforcement learning'' \citep{foerster2016learning}
  and ``Multi-Agent Cooperation and the Emergence of (Natural) Language'' \citep{lazaridou2016multiagent}.
These were some of the first papers to applying deep multi-agent reinforcement learning to developing discrete token-based communication systems \emph{de novo} in a way analogous to human language.
With deep learning, agents could observe pixel-based images with convolutional neural networks and produce the messages with recurrent neural networks---two techniques which had made significant advances towards matching human performance in image and language processing.
% While prior work applied mathematical models \citep{brighton2005} and classical machine learning methods \citep{werner_Dyer_1991}, the introduction of deep learning opened up the possibility of a far more robust notion of the results of the simulations being \emph{emergent}.
% That is, with mathematical models the range of results is tightly constrained by the design of the model and ``emergent'' phenomena are either relatively simple or encoded into the model itself.\footnote{Although Conway's Game of Life is notable exception to this.}
% Deep reinforcement learning, on the other hand, has demonstrated vivid example of complex behaviors emerging in environments with simple rules such as DeepMind's AlphaZero \citep{silver2017masteringchessshogiselfplay} or OpenAI's multi-agent hide-and-seek \citep{baker2020emergenttoolusemultiagent}.

The prototypical emergent communication experiment consists two or more deep neural network-based agents situated in some kind of environment or game where they must cooperate in order to succeed.
The agents are equipped with a communication channel consisting of discrete tokens with no \emph{a priori} meaning; it is only through the reinforcement learning-based optimization that messages passed between agents begin to take on meaning.
The resulting behavior, most especially the communication protocol, is typically the object of analysis, addressing question such as:
  Did an effective communication protocol emerge at all?
  What structural features characterize it?
  Do these features align at all to human language?
  What can we infer about language formation more generally from the above?

In practice, much of the literature has focused on the signaling game and the emergence of compositionality in communication
  \citep{havrylov2017sequence,mordatch2018grounded,chaabouni2022emergent}.
The signaling game itself was introduced in the context of game theory by David Lewis \citep{lewis1970ConventionAP};
  it is one of the simplest possible environments for emergent communication contributing, in large part, to its popularity.
The game consists of only two agents: a sender and a receiver.
The sender makes on observation (e.g., an orange circle) and sends a message to the receiver who must, based on the message alone, determine the nature of the observation (e.g., it was an orange circle, not a blue circle).
A visualization of the signaling game is provided in \cref{fig:signaling-game}.
The question of compositionality arises when we look at how the communication protocol encodes compound meanings like a red square: A compositional protocol would encode ``red'' and ``circle'' with their own words which could be reused to express meanings like a red circle or a blue square.
On the other hand, holistic communication sometimes emerges where a unique word refers to red square, bearing no relation to the word(s) for red circle.
Compositional communication, generally, is seen as more desirable both for practical reasons (more efficient encoding of information) as well as for its resemblance to how humans tend to encode meaning in language.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.53\textwidth}
    \centering
    \setlength\fboxsep{0pt}
    \input{src/figures/robot}
    \vspace{1cm}
    \caption{%
      $T_1$: The sender (left) observes an object.
      $T_2$: The sender passes a message to the receiver (right).
      $T_3$: The receiver chooses from a handful of candidate objects.
    }
    \unskip\label{fig:boxface}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \input{src/figures/signaling-chart}
    \caption{Illustration of the technical architecture of the signaling game.}
    \unskip\label{fig:signaling-chart}
  \end{subfigure}
  \caption{%
    An illustration of the discrimination variant of the signaling game, one of the simplest and most common environments in emergent communication research.
  }
  \unskip\label{fig:signaling-game}
\end{figure}


Other environments do appear in the literature such as navigation tasks or dialogue-based games
  \citep{unger2020GeneralizingEC,brandizzi2022rlupus}.
In addition to compositionality, other phenomena have been the subject of investigation such as pragmatics, transfer learning, and the information theoretic properties of emergent language
  \citep{kang2020incorporatingpragmaticreasoningcommunication,yao2022linking,tucker2021discrete}.
% While there are too many papers to summarize comprehensively here, I estimate that there are on the order of $200$ papers directly related to emergent communication.\footnote{Figure based on finding ${\sim}150$ papers during a comprehensive literature review in 2023.}
For a general review of the emergent communication literature, I recommend \citet{lazaridou2020review}.

\section{Motivation}
Early on in the history of deep learning-based emergent language, papers such as ``Natural Language Does Not Emerge `Naturally' in Multi-Agent Dialog'' \citep{kottur2017natural} and \citet{bouchacourt2018how} already argued the difficulty of finding informative patterns in emergent languages, and such papers would prove to be harbinger of the decade to come:
  there would be little measurable progress toward the farther-reaching applications of the field.
Many research papers end up employing \emph{ad hoc} analyses tailored to the particular phenomenon of interest without a clear way to unify the findings into a broader understanding.
While normal science (as \citet{kuhn} terms it) often proceeds by small, additive research contributions, emergent communication has not developed research paradigm where the small contributions can truly add together.

One can read much of the literature on emergent communication, learn of many different trends that appear in particular environments, and still largely have little idea why emergent languages look the way they do nor what they might look like in a new environment.
Furthermore, despite the potential for groundbreaking applications in natural language processing and linguistics, emergent communication has not produced any broadly-applicable contributions in either (\cref{ch:review}).
In contrast, classical emergent language research has proved fruitful, yielding results such as the iterated learning model which are more easily reincorporated into the broader field of linguistics \citep{kirby2014iterated}.
Thus, the problems mentioned above are most pertinent to the deep learning approach to emergent language.

I intend for this thesis, then, to make foundational contributions addressing the unique structural issues in deep learning-based emergent language research that has resulted in this.
In a large part, I argue that this requires making the patterns and structures present in emergent languages (if they are there) more easily observable.
This is analogous to the broader project of interpretability in deep learning specified for emergent language---although it proves to play a more central role in emergent language insofar as interpretation is fundamental to the field.
The methods of classical emergent language, for the most part, assume that the basic structures in the emergent languages (e.g., words) are already evident,
  meanting that the project of finding structure in deep emergent languages is, in a way, trying to recover some of what was lost in the classical-to-deep transition and better connect the two sides of the broader field.

% If the same old, tried and true paradigm of machine learning research simply worked for emergent language, we would have seen tangible progress by now, but we have not, so one can assume that emergent language is a dead end, is missing a key technological prerequisite (e.g., computing resources, better reinforcement learning algorithms), or needs specially tailored methodological improvements.
% The work documented here addresses the last of these.


\section{Approach}

Thus, this thesis introduces principled (\cref{sec:intro-review}), environment-agnostic (\cref{sec:intro-elcc}) analytical algorithms to support measurable progress towards both the engineering-centric (\cref{sec:intro-xferbench}) and scientific (\cref{sec:intro-morph}) goals of emergent language.

% \phantom{}\cmt{This whole paragraph needs to be rewritten.}
% The overall intent of the thesis is to create quantitative evaluative methods which work across a wide variety of emergent communication environments in order to support a research and development workflow more like that of the rest of deep learning-based research.
% Methods should yield \emph{quantitative} metrics to permit direct comparison of different systems, statistical analysis, and more automated methods of exploring emergent communication environments.
% These methods must also be \emph{evaluative} \cmt{Only XB is evaluative, so is this still fine for CSAR contributions?} since we a \cmt{?} number that we can optimize for---a goal---not simply a number describing one aspect of a system that requires further interpretation.
% Finally, this thesis looks to the rest of deep learning research, especially in regards to structures like benchmarks and evaluation metrics, for inspiration because these factors are critical for its own success. \cmt{Maybe remove this; it is only loosely true and comes off as milquetoast.}
% While the long-term development of emergent communication methods needs far more than just borrowing methods from deep learning, developing general-purpose evaluative tools is critical in unifying the research efforts of the field such that they can begin to progress in a tangible way.

\iffalse Keep this for now.
\paragraph{An aside}
While the fundamental lack of progress has, from an early point, been the main focus of my doctoral research, I had to make a decision between two ways of addressing this problem: the ``scientific'' way and the ``engineering'' way.
The scientific approach would entail developing a theory of emergent communication which would enable progress by first creating a concrete, unified theory that research could contribute two, and second, by illuminating the most promising directions based on the predictions of the field.
Such an approach was addressed by my paper ``Mathematically Modeling the Lexicon Entropy of Emergent Language'' \citep{boldt2023mathmodel} which was a sort of proof-of-concept for this approach, attempting to establish an empirically verifiable formal model of certain behaviors in emergent communication experiments.
Of the two approaches, this one certainly had (and has) more appeal to me, but I ultimately decided against it largely because I judged that it was not feasible.
In essence, the current paradigm of emergent communication research follows this approach (even if the scientific method is hardly employed explicitly), that is, trying to find general principles which can be empirically verified.
I think this paradigm fails in large part because emergent language is\dots\emph{emergent}!
No amount of adding up little parts is going to tell you ahead of time what is going to happen in the large-scale, so the idea would go.
If this is true, and my intuition pushes me in this direction, then emergent communication, \emph{qua} emergent phenomenon, is a matter of ``go big or go home'': scale up or pack up.\footnote{This sounds like a bitter lesson \citep{bitter-lesson}.}

This leads us to the engineering approach which might be summed up as: how to scale up intelligently.
This is the approach this thesis follows.
In contrast to the scientific approach, the engineering approach focuses on creating structures which allow research to try a wide variety of approaches and efficiently sift through the results to find the next handhold in the ascent.
Naturally, some degree of theoretical understanding is necessary to explore intelligently, but the threshold of understanding for guiding exploration is lower than what might typically be associated with scientific understanding or prediction.
Finally, from a practical standpoint, the engineering approach proved more tractable for writing a thesis: the intermediate papers more easily speak for themselves and making an immediate impact is far more feasible.
\fi




% \paragraph{How to read this thesis}%
% \phantom{}\cmt{This will probably be removed now.}
% While each of the chapters can be read on its own, they are arranged in a logical (and chronological) progression.
% The primary of contribution of this thesis is to establish a coherent, multi-faceted foundation for measurable progress in deep learning-based emergent language.
% And this contributions requires the whole document to present.
% 
% There are four major divisions of the content of this thesis.
% First, \cref{ch:review} presents a comprehensive review of the goals and applications of emergent language research.
%   While every single goal listed is not directly touched upon in this thesis, this chapter is answers the question of why particular later contributions were chosen and how they are impactful.
% Second, \cref{ch:elcc} introduces a data resource which will be used in subsequent chapters.
% %   This work also helps to showcase some of the variety present in the emergent language simulations as well as makes it possible to experiment with emergent language corpora without having to install and run the environment they are derived from.
% Third, \cref{ch:xferbench,ch:hpo} employ data-driven, deep learning methods to understand how emergent languages look to neural networks vis-\`a-vis human languages..
%   % This is specifically analyzing emergent language through the eyes of machine learning models, that is, looking large-scale patterns discovered from data.
%   % \cmt{More here?}
% Lastly, \cref{ch:morphemes,ch:phrasebook} introduce methods for detecting linguistic structures (including morphemes and syntax) in emergent language corpora.
%   % While they provide a handful of interesting insights on their own, the techniques introduced are intended to be generally applicable to emergent languages so as to make morphological analysis easily accessible for future work.
% \cmt{All public code related to this thesis is available on GitHub (\url{https://github.com/brendon-boldt/}) under free software licenses.}

% Below, I will discuss how each of the divisions fits into the broader contribution of the thesis.
% For more concrete summaries as to what the chapters entail, please see the abstracts in each chapter.


\subsection{Finding a lodestar}%
\label{sec:intro-review}
\Cref{ch:review} provides a comprehensive review on the goals and applications of emergent language research.
This chapter is not a general review but review of the \emph{goals} of this field of research because it is imperative for any field of research to understand where it is going and why.
While many of the goals of deep learning-based emergent language overlap with those of classical emergent language, others extend to the machine learning world.
Beyond the general clarity that articulating directions provides, this chapter performs two specific functions for this thesis.
First, by articulating the goals and, in many cases, the minimal progress towards achieving them, it motivates the paradigmatic changes which this thesis proposes in later chapters.
Second, the articulated goals form the very basis of the analytical methods which are introduced in later chapters.
Using emergent language to pretrain language models?
  Evaluate them against XferBench (\cref{ch:xferbench}).
Trying to make claims about the morphology of emergent languages?
  Induce a morpheme inventory with CSAR (\cref{ch:morphemes}).
Thus this chapter provides both the motivation and scaffold for the \emph{principled} analytical algorithms introduced the later chapters.

\subsection{Towards unity in a diverse landscape}%
\label{sec:intro-elcc}

\Cref{ch:elcc} introduces the Emergent Language Corpus Collection (ELCC), a diverse collection of emergent language corpora derived from free and open source implementations of emergent language environments.
As stated above, emergent language research is often organized into little islands working with specific environments to test specific phenomena with considerably less work spanning even a small subset of the wide variety of emergent language environments.
Introducing a collection of diverse emergent language corpora in a unified format (along with ready-to-run versions of the corresponding codebases) makes a significant step towards more comparative research yielding more \emph{environment-agnostic} contributions.
Furthermore, making the corpora available in a simple JSON-based format (and obviating running the environments in the first place) lowers the barrier of entry to studying emergent languages in general, especially for those coming from a non-technical background.
The promise of analysis across diverse languages is borne out within this very thesis as ELCC is employed both in XferBench (\cref{ch:xferbench}) and CSAR (\cref{ch:morphemes}).


% \Cref{ch:elcc} \cmt{fix removing rich corpora} introduce an important data resource to emergent communication research: the Emergent Language Corpus Collection (ELCC), a collection of emergent language corpora with semantic annotations of utterances derived from a variety of free and open source emergent communication implementations.
% Each of these corpora is accompanied by metadata describing statistical properties of the corpus, taxonomic properties of the environment it came from, and a turnkey shell script for reproducing the corpus (or developing a new one).
% This collection of corpora is made a public such that it can be both easily used and contributed to by the broader research community.

% In its own right, this collection is a significant contribution to emergent communication as it increases the accessibility of emergent language data both for researchers who might be able to generate the emergent languages themselves and those looking to compare a wide variety of emergent languages.
% More importantly for the thesis, though, having a robust collection of emergent language corpora is necessary test, contextualize, and motivate the results of the following chapters.
% The transfer learning-based metric discussed below takes emergent language corpora as input, and to demonstrate its utility in comparing various approaches to emergent communication, it needs to be applied to a wide variety of emergent languages.
% For the methods of detecting linguistic structure, not only is the variety of emergent languages important, but the semantic annotations are a critical part of discovering the latent structure that might be present in the emergent languages.
% Thus, the collection of emergent language corpora forms the foundation for the rest of the research in this thesis while also demonstrating how emergent communication research can be made more accessible.

\subsection{Seeing language like a neural network}%
\label{sec:intro-xferbench}

\Cref{ch:xferbench} introduces XferBench, an evaluation metric for emergent language corpora based on deep transfer learning.
The intuition behind XferBench is that when a model is pretrained on emergent language data, its downstream performance on human language-based natural language processing tasks (e.g., language modeling, machine translation) correlates with its similarity to human language, as far as a deep neural network is concerned.
XferBench is \emph{environment-agnostic} insofar as it only requires an emergent language corpus as input, meaning that it can be applied to most any environment, and compare differing environments on single quantitative scale.
XferBench is \emph{principled} insofar as it is directly measuring how well emergent languages function in place of human languages in deep learning settings, a major component of the \emph{engineering} goals of emergent language.
Furthermore, the type of realism which XferBench is measuring is complementary to the \emph{scientific} goals of emergent language which stand to benefit from more realistic simulations of linguistic phenomena.
\Cref{ch:hpo} demonstrates XferBench in action by using it as an objective function for hyperparameter search over the hyperparameters of an emergent language environment, effectively finding measurably more human-like emergent languages with minimal manual input.

% \cmt{Talk about the fact somewhere that we need these analytical algorithms because we cannot inspect emergent languages manually or presuppose structures in them.}

% XferBench exemplifies the goal of the thesis insofar as it establishes an evaluative method for easily and effectively comparing emergent languages on a level playing field.
% The transfer learning-based approach captures notion of how ``good'' an emergent language (corpus) is from the perspective of machine learning.
% This is meant in two ways:
%   First, the emergent languages are analyzed according to the methods of machine learning, discovering regularly occurring patterns with data-driven methods and a low inductive bias.
%   Second, transfer learning method closely mirrors many of the practical applications of emergent communication which consist of using emergent language data is pretraining or evaluation data for natural language processing models.
% While it is not as simple to establish a ranking of ``better'' and ``worse'' with the largely open ended task of designing an emergent communication environment, XferBench still provides a useful notion of what directions are having a tangible effect on the complexity of the emergent languages those environments develop.


\subsection{Discovering order in the chaos}%
\label{sec:intro-morph}

While the previously mentioned chapters employ black-box techniques to measure engineering-centric progress in emergent language, \cref{ch:morphemes,ch:phrasebook} introduce good, old-fashioned algorithms (no neural networks) for uncovering morphosyntactic structures in emergent languages.
This is motivated by the fact that the \emph{scientific} goals of emergent language require understanding the patterns and regularities present in emergent language despite the largely cryptic output produced by the simulations in question.
Specifically, \cref{ch:morphemes} introduces CSAR, an algorithm for inducing morphemes from emergent language corpora annotated with the individual utterances meanings.
This approach is motivated by the fact that simply identifying the minimal units of meaning (i.e., morphemes) in emergent languages is non-trivial and critical to understanding most higher-level linguistic structures (e.g., morphology, syntax, pragmatics).
Furthermore, CSAR makes very few prior assumptions about what kind of structure is present in the emergent languages analyzed, making it suitable to handle the wide variety of patterns which could possibly emerge.
\cref{ch:phrasebook} puts the morphemes CSAR induces to the test by measuring their effectiveness for synthesizing and analyzing emergent language utterances produced by the neural network-based agents.
The patterns observed in synthesizing and analyzing emergent language at the morpheme-level present one of the most \emph{principled} investigations of the syntax of deep learning-based emergent languages to date.

% \cmt{Check this change in reference to phrasebook}
% \Cref{ch:morphemes,ch:phrasebook} introduce algorithms for detecting linguistic structures in any emergent language corpora that possesses in annotations as described above in ELCC.\@
% \Cref{ch:morphemes} specifically looks at detecting ``morphemes'' in the sense of atomic units of form with a distinct meaning.
% The output is a list of token sequences which correspond with particular meanings in the environment.
% Not only does this enable a host of interesting analyses, but it also lays the groundwork for \cmt{fix} which introduces an algorithm to detect structure among these morphemes, making a first step towards identifying the syntax of emergent languages.
% 
% The original intent for the linguistics-focused component of this thesis was to develop a benchmark similar in intent to XferBench but looking at how close the linguistic features of emergent language were to those of human language in areas such as syntax, social variation, and discourse.
% Yet upon planning the concrete details of such a metric, it was apparent that it was not clear \emph{if} such features as syntax or discourse existed in any meaningful way, let alone there being a way to detect them.
% Thus, in order to help decide where to begin we visualized and informal hierarchy of linguistic phenomena in \cref{fig:linguistic-dag}; the direction of the hierarchy is determined by what phenomena presuppose the existence of more basic phenomena.
% What was immediately apparent is that while it easy to point to concrete notions of semantics and tokens (the atomic components of a message/utterance in emergent communication), even even the existence and nature of the immediate combination of these---morphemes---was not established (and hardly explored).
% 
% \begin{figure}
%   \centering
%   \includegraphics[width=0.85\linewidth]{assets/linguistic-dag}
%   \caption{%
%     Hierarchy of linguistic concepts.
%     $X\rightarrow Y$ can be read as ``the definition of $X$ presupposes $Y$ being defined'' or roughly ``$X$ depends on $Y$''.
%     The only concepts whose existence is established in emergent language are \emph{semantics} and \emph{token}.}
%   \unskip\label{fig:linguistic-dag}
% \end{figure}
% 
% Thus, I decided to pursue establishing a method to show the existence of and identify morphemes and syntax---the backbone of the hierarchy depicted in \cref{fig:linguistic-dag}, which is likely the most that can be done toward developing a metric of linguistic similarity in the scope of this thesis.
% Nevertheless, the proposed algorithms still fit well within this thesis' theme of pioneering accessible general purpose in methods in emergent communication that permit the direct comparison of emergent languages across a wide of environment and implementations.
