\input{src/preamble}
\begin{document}


\input{src/title}


\newpage
\thispagestyle{plain}
\section*{Abstract}
Emergent communication is the field of research which studies how language-like communication systems evolve from scratch in agent-based simulations.
The most recent incarnation of this topic, starting in 2016, has focused on leveraging recent advancements in deep neural network, reinforcement learning, and natural language processing.
Emergent communication, as a method, has significant potential applications from powering unprecedentedly detailed simulations of how human invent, acquire, and use language to providing an alternative source of language data to the Internet-gulping large language models.
Despite this potential, the field has yet make any significant progress towards these applications largely because it lacks any methodological resources to unify research efforts within the field;
  that is, research findings are often ``one-off'', lacking any way of making general claims or comparing itself directly to other approaches.
This thesis advances the field of emergent communication by developing the resources that are necessary for forging a unified research program that is critical the advancement of any field of science or engineering.

The first contribution is to the conceptual foundations of emergent communication in the form of a comprehensive account of potential applications of emergent communication across natural language processing, multi-agent reinforcement learning, linguistics, and beyond.
\cmt{Include explicit summary of TMLR paper in introduction somewhere?}
This conceptual foundation is paired with the more mundane but entirely vital data resource of a collection of emergent communication corpora collected from a variety of free and open source implementations of simulations from the literature.
These two elements in turn enable the introduction of \emph{evaluation} metrics---that is, quantitative measures of emergent communication which quantify how good, in some general sense, a language generated by an emergent communication simulation is.
In particular, we quantify ``how good an emergent language is'' by measuring its similarity to human language.
This is done in two complementary paradigms: a data-driven, machine learning paradigm and a theoretically-motivated linguistic paradigm.
The machine learning-based metric uses deep transfer learning to assess how similar an emergent language is to human language by using the emergent language as pretraining data for a downstream human language-based task.
The linguistics-based metrics algorithmically identify the presence of universal features of human language in emergent languages.
Finally, this thesis explores the relationship between the machine learning-based and linguistics-based metrics, so as to determine what types of emergent communication simulations produce the most human language-like emergent languages in a general sense.


\newpage
\tableofcontents*

\input{src/introduction}

\chapter{Building a Library of Emergent Languages}
%\lipsum[1-20]{}

\section{ELCC: the Emergent Language Corpora Collection \note{under review}}


\section{Adding semantics annotations to corpora \note{proposed}}
\unskip\label{sec:rich-corpora}

\chapter{Evaluation of Emergent Languages with Deep Transfer Learning}
%\lipsum[1-20]{}

\input{chapters/xferbench/src/main}

\chapter{Explaining Emergent Languages' Effectiveness for Deep Transfer Learning \note{in progress}}
\unskip\label{ch:xferbench-analysis}
%\lipsum[1-5]{}



\chapter{Finding Linguistic Universals in Emergent Languages \note{proposed}}
\unskip\label{ch:universals}
%\lipsum[1-20]{}

What universals do we want to search for?
I think we should choose things that are not trivial.
Then, what would be the most important?
Let's create a graph of linguistic concepts!

\begin{figure}
  \centering
  \includegraphics[width=0.95\linewidth]{assets/linguistic-dag}
  % \begin{verbatim}
  % digraph G {
  %     word -> morpheme;
  %     morpheme -> semantics;
  %     pos -> syntax;
  %     headedness -> pos;
  %     morpheme -> token;
  %     socvar -> syntax;
  %     entropy -> token;
  %     syntax -> word;
  %     lexdyn -> word;
  %     socvar -> lexdyn;
  %     sentence -> clause;
  %     recursivity -> clause;
  %     clause -> syntax;
  %     agreement -> pos;
  %     opcl_class -> pos;
  % }
  % \end{verbatim}
  \caption{Hierarchy of linguistic concepts.}
\end{figure}

Let's assume for a minute that one of the universals that we will need to detect is syntax.
What we will need to do is create a definition of syntax in some generative sense: a language has syntax if its strings can be generated by such and such a process.
(Is there are a more ``discriminative'' take on this process?)
We'll need this generative take because we'll need to generate synthetic data that we can test our ``discriminative'' algorithm on given only surface forms (or surface forms + semantics).
In addition to this generative definition, we'll need to introduce ideas of fuzziness either through non-strict adherence to the rules or by rules which inherently have some fuzz in them.
Thus, we will have a definition of syntax which is applicable to EL since it doesn't make structural assumptions about human language as well as a process to generate synthetic data with varying levels/complexities/adherences to syntax which can for the basis of our detection algorithm.
The idea here is that these synthetic languages will---by definition---show the full range of grammars (not necessarily very possible grammar).
On the one hand, this feels like a bad approach because of course we can't cover every sense of grammar, but it seems like the most rigorous we can get.
Maybe instead of saying ``we're testing for syntax full-stop'' we can say we are testing for ``$\alpha$-syntax'' which we define as being such and such: it isn't fully syntax but it is a reduced version that is easier to get to, setting a lower bar for emergent languages.



\chapter{Meta-analysis of Linguistic Universals and Transfer Learning Performance \note{proposed}}
\unskip\label{ch:meta-analysis}

\section{The correlation between the two}

\section{Creating the best emergent communication system}


\bibliographystyle{plainnat}
\bibliography{src/main,chapters/xferbench/src/main}
% \bibliography{chapters/xferbench/src/main}

\appendix

\chapter{Appendix: XferBench}
\input{chapters/xferbench/src/appendix}

 \typeout{INFO: \arabic{comment} comments.}
\end{document}
