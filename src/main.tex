\input{src/preamble}
\begin{document}


\input{src/title}


\newpage
\thispagestyle{plain}
\section*{Abstract}

\newpage
\tableofcontents*

\input{src/introduction}

\chapter{Building a Library of Emergent Languages}
%\lipsum[1-20]{}

\section{ELCC: the Emergent Language Corpora Collection \note{under review}}


\section{Adding semantics annotations to corpora \note{proposed}}
\unskip\label{sec:rich-corpora}

\chapter{Evaluation of Emergent Languages with Deep Transfer Learning}
%\lipsum[1-20]{}

\input{chapters/xferbench/src/main}

\chapter{Explaining Emergent Languages' Effectiveness for Deep Transfer Learning \note{in progress}}
\unskip\label{ch:xferbench-analysis}
%\lipsum[1-5]{}



\chapter{Finding Linguistic Universals in Emergent Languages \note{proposed}}
\unskip\label{ch:universals}
%\lipsum[1-20]{}

What universals do we want to search for?
I think we should choose things that are not trivial.
Then, what would be the most important?
Let's create a graph of linguistic concepts!

\begin{figure}
  \centering
  \includegraphics[width=0.95\linewidth]{assets/linguistic-dag}
  % \begin{verbatim}
  % digraph G {
  %     word -> morpheme;
  %     morpheme -> semantics;
  %     pos -> syntax;
  %     headedness -> pos;
  %     morpheme -> token;
  %     socvar -> syntax;
  %     entropy -> token;
  %     syntax -> word;
  %     lexdyn -> word;
  %     socvar -> lexdyn;
  %     sentence -> clause;
  %     recursivity -> clause;
  %     clause -> syntax;
  %     agreement -> pos;
  %     opcl_class -> pos;
  % }
  % \end{verbatim}
  \caption{Hierarchy of linguistic concepts.}
\end{figure}

Let's assume for a minute that one of the universals that we will need to detect is syntax.
What we will need to do is create a definition of syntax in some generative sense: a language has syntax if its strings can be generated by such and such a process.
(Is there are a more ``discriminative'' take on this process?)
We'll need this generative take because we'll need to generate synthetic data that we can test our ``discriminative'' algorithm on given only surface forms (or surface forms + semantics).
In addition to this generative definition, we'll need to introduce ideas of fuzziness either through non-strict adherence to the rules or by rules which inherently have some fuzz in them.
Thus, we will have a definition of syntax which is applicable to EL since it doesn't make structural assumptions about human language as well as a process to generate synthetic data with varying levels/complexities/adherences to syntax which can for the basis of our detection algorithm.
The idea here is that these synthetic languages will---by definition---show the full range of grammars (not necessarily very possible grammar).
On the one hand, this feels like a bad approach because of course we can't cover every sense of grammar, but it seems like the most rigorous we can get.
Maybe instead of saying ``we're testing for syntax full-stop'' we can say we are testing for ``$\alpha$-syntax'' which we define as being such and such: it isn't fully syntax but it is a reduced version that is easier to get to, setting a lower bar for emergent languages.



\chapter{Meta-analysis of Linguistic Universals and Transfer Learning Performance \note{proposed}}
\unskip\label{ch:meta-analysis}

\section{The correlation between the two}

\section{Creating the best emergent communication system}


\bibliographystyle{plainnat}
\bibliography{src/main,chapters/xferbench/src/main}
% \bibliography{chapters/xferbench/src/main}

\appendix

\chapter{XferBench}
\input{chapters/xferbench/src/appendix}

 \typeout{INFO: \arabic{comment} comments.}
\end{document}




